mm
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

# Function to load and preprocess the dataset
def load_and_preprocess_data(filepath):
    try:
        df = pd.read_csv(filepath)
        # Convert 'DATE' column to datetime format, handle errors by coercing invalid formats to NaT (Not a Time)
        df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')
        
        # Drop rows with any missing values
        df.dropna(inplace=True)
        
        return df
    except FileNotFoundError:
        print(f"File '{filepath}' not found.")
        return None
    except pd.errors.EmptyDataError:
        print("File is empty.")
        return None
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

# Generate full recommendation ranks for all sectors and accounts
def generate_full_ranks(interaction_matrix, sector_similarity_matrix, sectors):
    sector_ranks = pd.DataFrame(index=interaction_matrix.index, columns=sectors)
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[account].values.reshape(1, -1)
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)
        
        # Ranking sectors based on similarity scores
        sorted_indices = np.argsort(-user_sector_similarity).flatten()
        sector_ranks.loc[account] = [np.where(sorted_indices == i)[0][0] + 1 for i in range(len(sectors))]
    
    return sector_ranks

def main():
    df = load_and_preprocess_data('main6.csv')
    if df is None:
        return

    unique_accounts = df['ACCT'].unique()
    train_accounts, test_accounts = train_test_split(unique_accounts, test_size=0.2, random_state=42)

    train_df = df[df['ACCT'].isin(train_accounts)]
    test_df = df[df['ACCT'].isin(test_accounts)]

    interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
    scaler = MinMaxScaler()
    interaction_counts_normalized_train = scaler.fit_transform(interaction_counts_train)

    combined_interaction_train = interaction_counts_normalized_train
    sectors = interaction_counts_train.columns.tolist()

    sector_similarity = cosine_similarity(combined_interaction_train.T)
    
    combined_interaction_df_train = pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, columns=sectors)
    all_sector_ranks = generate_full_ranks(combined_interaction_df_train, sector_similarity, sectors)

    # Exporting the results
    try:
        with pd.ExcelWriter('sector_recommendations_and_matrices.xlsx', engine='openpyxl') as writer:
            all_sector_ranks.to_excel(writer, sheet_name='Sector-wise Recommendation Ranks')
        print("Exported all data and recommendation ranks to 'sector_recommendations_and_matrices.xlsx'.")
    except Exception as e:
        print(f"Failed to export data: {e}")

if __name__ == "__main__":
    main()











import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

# Load and preprocess the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])

# Identify unique accounts and split into training and testing sets
unique_accounts = df['ACCT'].unique()
train_accounts, test_accounts = train_test_split(unique_accounts, test_size=0.2, random_state=42)

# Filter the dataset for training and testing
train_df = df[df['ACCT'].isin(train_accounts)]
test_df = df[df['ACCT'].isin(test_accounts)]

# Pivot to get interaction matrices and normalize
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
scaler = MinMaxScaler()
interaction_counts_normalized_train = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized_train = scaler.fit_transform(interaction_amounts_train)

# Combine and calculate sector similarity
combined_interaction_train = (interaction_counts_normalized_train + interaction_amounts_normalized_train) / 2
sectors = interaction_counts_train.columns
sector_similarity = cosine_similarity(combined_interaction_train.T)

# Define a function to calculate ranks for all sectors for each customer
def calculate_sector_ranks(interaction_matrix, sector_similarity_matrix):
    sector_ranks = pd.DataFrame(index=interaction_matrix.index, columns=interaction_matrix.columns)
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[account].values.reshape(1, -1)
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        sorted_indices = np.argsort(-user_sector_similarity)
        ranks = np.empty_like(sorted_indices)
        ranks[sorted_indices] = np.arange(len(user_sector_similarity))
        sector_ranks.loc[account] = ranks + 1  # +1 to start ranking from 1 instead of 0
    return sector_ranks

# Calculate sector ranks for each customer
combined_interaction_df_train = pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, columns=sectors)
sector_ranks_df = calculate_sector_ranks(combined_interaction_df_train, sector_similarity)

# Exporting sector ranks
sector_ranks_df.to_csv('sector_ranks_per_customer.csv')

print("Exported sector ranks for each customer to 'sector_ranks_per_customer.csv'.")








import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

# Load and preprocess the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])

# Identify unique accounts
unique_accounts = df['ACCT'].unique()

# Split unique accounts into training and testing sets
train_accounts, test_accounts = train_test_split(unique_accounts, test_size=0.2, random_state=42)

# Filter the original dataset to create training and testing datasets based on the split accounts
train_df = df[df['ACCT'].isin(train_accounts)]
test_df = df[df['ACCT'].isin(test_accounts)]

# Creating and normalizing interaction matrices for counts and amounts
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
scaler = MinMaxScaler()
interaction_counts_normalized_train = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized_train = scaler.fit_transform(interaction_amounts_train)

# Hot encoding sectors for training data
hot_encoded_sectors_train = (interaction_counts_train > 0).astype(int)

# Combine normalized matrices with equal weight for each factor
combined_interaction_train = (interaction_counts_normalized_train + interaction_amounts_normalized_train) / 2
sectors = interaction_counts_train.columns

# Calculate the sector-to-sector similarity matrix from the combined interaction matrix
sector_similarity = cosine_similarity(combined_interaction_train.T)

# Generate recommendations and rank for all customers
def generate_recommendations_and_ranks(interaction_matrix, sector_similarity_matrix, sectors, top_n=5):
    recommendations = {}
    ranks = {}
    sector_names = sectors.tolist()
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[account].values.reshape(1, -1)
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommended_sectors = [sector_names[i] for i in top_sector_indices]
        recommendations[account] = recommended_sectors
        ranks[account] = np.sort(-user_sector_similarity)[:top_n] * -1
    return recommendations, ranks

combined_interaction_df_train = pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, columns=sectors)
all_recommendations, all_ranks = generate_recommendations_and_ranks(combined_interaction_df_train, sector_similarity, sectors)

# Hot encoding sectors for testing data
interaction_counts_test = test_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
hot_encoded_sectors_test = (interaction_counts_test > 0).astype(int)

# Exporting results, sector-wise analysis, and additional outputs
with pd.ExcelWriter('enhanced_sector_recommendations_and_matrices.xlsx', engine='openpyxl') as writer:
    # Original outputs
    # [Export code for Recommendations, Interaction Counts, etc., as before]

    # Additional outputs
    hot_encoded_sectors_train.to_excel(writer, sheet_name='Hot Encoded Sectors Train')
    hot_encoded_sectors_test.to_excel(writer, sheet_name='Hot Encoded Sectors Test')
    
    # Sector-wise recommendation rank for each customer
    pd.DataFrame.from_dict(all_ranks, orient='index', columns=[f'Rank {i+1}' for i in range(5)]).to_excel(writer, sheet_name='Recommendation Ranks')

print("Exported all data, recommendations, sector-wise analysis, and additional outputs to 'enhanced_sector_recommendations_and_matrices.xlsx'.")









import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
import openpyxl  # Ensure openpyxl is installed for Excel export

# Load the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])  # Convert 'DATE' to datetime if needed

# Time-based split for training and testing
split_date = pd.Timestamp('2023-01-01')  # Adjust based on your dataset
train_df = df[df['DATE'] < split_date]
test_df = df[df['DATE'] >= split_date]

# Creating Interaction Matrices for Training Data (Counts and Amounts)
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Normalize the interaction matrices
scaler = MinMaxScaler()
interaction_counts_normalized = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized = scaler.fit_transform(interaction_amounts_train)

# Combine normalized matrices with equal weight for demonstration purposes
combined_interaction = (interaction_counts_normalized + interaction_amounts_normalized) / 2

# Calculate sector-to-sector similarity matrix from the combined interaction matrix
sector_similarity = cosine_similarity(combined_interaction.T)
sector_similarity_df = pd.DataFrame(sector_similarity, index=interaction_counts_train.columns, columns=interaction_counts_train.columns)

# Function to generate recommendations for all customers based on sector similarity
def generate_recommendations_for_all(interaction_matrix, sector_similarity_matrix, sectors, top_n=5):
    recommendations = {}
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[[account]].values
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommended_sectors = sectors[top_sector_indices].tolist()
        recommendations[account] = recommended_sectors
    return recommendations

# Generate recommendations for all customers
interaction_matrix_df = pd.DataFrame(combined_interaction, index=interaction_counts_train.index, columns=interaction_counts_train.columns)
sectors = interaction_counts_train.columns
all_recommendations = generate_recommendations_for_all(interaction_matrix_df, sector_similarity, sectors)

# Convert recommendations to DataFrame for export
recommendations_df = pd.DataFrame.from_dict(all_recommendations, orient='index', columns=[f'Top {i+1}' for i in range(5)])

# Prepare summary of actual amounts spent by each customer in each sector during the test period
actual_spending_test_period = test_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Initialize a DataFrame to store the comparison of actual spending in recommended sectors
comparison_df = pd.DataFrame(columns=['ACCT', 'Recommended Sectors', 'Actual Spending in Recommended Sectors'])

for acct, recommended_sectors in all_recommendations.items():
    if acct in actual_spending_test_period.index:
        actual_spending = actual_spending_test_period.loc[acct, recommended_sectors].sum()
    else:
        actual_spending = 0
    comparison_df = comparison_df.append({
        'ACCT': acct,
        'Recommended Sectors': ', '.join(recommended_sectors),
        'Actual Spending in Recommended Sectors': actual_spending
    }, ignore_index=True)

# Export to Excel with separate sheets
with pd.ExcelWriter('recommendations_and_matrices.xlsx', engine='openpyxl') as writer:
    recommendations_df.to_excel(writer, sheet_name='Customer Recommendations')
    interaction_counts_train.to_excel(writer, sheet_name='Interaction Counts Train')
    interaction_amounts_train.to_excel(writer, sheet_name='Interaction Amounts Train')
    sector_similarity_df.to_excel(writer, sheet_name='Sector Similarity')
    comparison_df.to_excel(writer, sheet_name='Spending Comparison')

print("Exported all data to 'recommendations_and_matrices.xlsx'.")
