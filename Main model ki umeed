import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
from scipy.sparse import csr_matrix

# Load the dataset
df_demo = pd.read_csv('main6.csv')

# Ensure 'DATE' is in datetime format and create 'Day_Type'
df_demo['DATE'] = pd.to_datetime(df_demo['DATE'])
df_demo['Day_Type'] = df_demo['DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Creating Interaction Matrices for Counts and Amounts
interaction_matrix_counts = df_demo.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_matrix_amounts = df_demo.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Normalize the matrices using MinMaxScaler
scaler = MinMaxScaler()
interaction_matrix_counts_scaled = scaler.fit_transform(interaction_matrix_counts)
interaction_matrix_amounts_scaled = scaler.fit_transform(interaction_matrix_amounts)

# Convert normalized matrices to sparse format for efficiency
interaction_matrix_counts_sparse = csr_matrix(interaction_matrix_counts_scaled)
interaction_matrix_amounts_sparse = csr_matrix(interaction_matrix_amounts_scaled)

# Combine the scaled matrices to create a unified interaction matrix
combined_matrix_sparse = (interaction_matrix_counts_sparse + interaction_matrix_amounts_sparse) / 2

# Calculate similarity matrix (Item-Based Collaborative Filtering)
# Note: Ensure dense_output is set to False for large datasets
similarity_matrix = cosine_similarity(combined_matrix_sparse.transpose(), dense_output=False)

# Enhanced recommendation function considering both interaction counts and amounts
def recommend_sectors_enhanced(account_id, interaction_matrix_counts, interaction_matrix_amounts, similarity_matrix, sectors, n_recommendations=5):
    if account_id not in interaction_matrix_counts.index:
        return "Account not found."
    
    account_idx = interaction_matrix_counts.index.get_loc(account_id)
    
    scores = similarity_matrix[account_idx].toarray().flatten()
    
    top_indices = scores.argsort()[-n_recommendations-1:-1][::-1]
    
    weight_counts = 0.6
    weight_amounts = 0.4
    aggregated_counts = interaction_matrix_counts.iloc[top_indices].sum(axis=0) * weight_counts
    aggregated_amounts = interaction_matrix_amounts.iloc[top_indices].sum(axis=0) * weight_amounts
    aggregated_preferences = (aggregated_counts + aggregated_amounts).sort_values(ascending=False)
    
    top_sectors = aggregated_preferences.index[:n_recommendations].tolist()
    
    return top_sectors

# Example usage
account_id_example = 'ACCT_1'  # Replace with an actual account ID from your dataset
sectors = interaction_matrix_counts.columns
recommended_sectors = recommend_sectors_enhanced(account_id_example, interaction_matrix_counts, interaction_matrix_amounts, similarity_matrix, sectors, n_recommendations=5)
print(f"Enhanced Recommended Sectors for {account_id_example}: {recommended_sectors}")









import pandas as pd
from scipy.sparse import lil_matrix, csr_matrix
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import MaxAbsScaler

class CollaborativeFilteringModel:
    def __init__(self):
        self.nearest_neighbors = NearestNeighbors(metric='cosine', algorithm='auto')
        self.customer_to_idx = {}
        self.idx_to_customer = {}
        self.sector_to_idx = {}
        self.idx_to_sector = {}

    def fit(self, transactions):
        # Assign indices for customers and sectors
        self._assign_indices(transactions)

        # Create and normalize interaction matrix
        counts_matrix, amounts_matrix = self._create_interaction_matrices(transactions)
        self.combined_matrix = self._normalize_and_combine_matrices(counts_matrix, amounts_matrix)

        # Fit NearestNeighbors model
        self.nearest_neighbors.fit(self.combined_matrix)

    def _assign_indices(self, transactions):
        self.customer_to_idx = {customer: i for i, customer in enumerate(transactions['ACCT'].unique())}
        self.idx_to_customer = {i: customer for customer, i in self.customer_to_idx.items()}
        self.sector_to_idx = {sector: i for i, sector in enumerate(transactions['Sector'].unique())}
        self.idx_to_sector = {i: sector for sector, i in self.sector_to_idx.items()}

    def _create_interaction_matrices(self, transactions):
        shape = (len(self.customer_to_idx), len(self.sector_to_idx))
        counts_matrix = lil_matrix(shape, dtype=int)
        amounts_matrix = lil_matrix(shape, dtype=float)

        for _, row in transactions.iterrows():
            c_idx = self.customer_to_idx[row['ACCT']]
            s_idx = self.sector_to_idx[row['Sector']]
            counts_matrix[c_idx, s_idx] += 1
            amounts_matrix[c_idx, s_idx] += row['TRAN_AMT']

        return counts_matrix.tocsr(), amounts_matrix.tocsr()

    def _normalize_and_combine_matrices(self, counts_matrix, amounts_matrix):
        scaler = MaxAbsScaler(with_mean=False)
        normalized_counts = scaler.fit_transform(counts_matrix)
        normalized_amounts = scaler.fit_transform(amounts_matrix)
        # Combine normalized matrices. You might adjust this logic based on your specific needs.
        return normalized_counts + normalized_amounts

    def predict_top_sectors(self, customer_id, top_n=5):
        if customer_id not in self.customer_to_idx:
            return []

        customer_idx = self.customer_to_idx[customer_id]
        _, indices = self.nearest_neighbors.kneighbors(self.combined_matrix[customer_idx:customer_idx+1], n_neighbors=top_n + 1)
        
        sector_votes = {}
        for idx_array in indices:
            for idx in idx_array[1:]:  # Skip the first one (itself)
                sectors = self.combined_matrix[idx].nonzero()[1]
                for sector_idx in sectors:
                    sector = self.idx_to_sector[sector_idx]
                    sector_votes[sector] = sector_votes.get(sector, 0) + 1

        sorted_sectors = sorted(sector_votes, key=sector_votes.get, reverse=True)[:top_n]
        return sorted_sectors

# Example usage remains the same










import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import MaxAbsScaler

class CollaborativeFilteringModel:
    def __init__(self):
        self.customer_to_idx = {}
        self.idx_to_customer = {}
        self.sector_to_idx = {}
        self.idx_to_sector = {}
        self.nn_model = NearestNeighbors(metric='cosine', algorithm='auto')

    def fit(self, transactions):
        # Assign unique indices to customers and sectors
        self._assign_indices(transactions)
        
        # Build interaction matrices for counts and amounts
        counts_matrix, amounts_matrix = self._build_interaction_matrices(transactions)
        
        # Combine the matrices (here we use simple addition, but other methods like weighted sum can be applied)
        combined_matrix = counts_matrix + amounts_matrix
        
        # Fit the Nearest Neighbors model
        self.nn_model.fit(combined_matrix)

    def _assign_indices(self, transactions):
        unique_customers = transactions['ACCT'].unique()
        unique_sectors = transactions['Sector'].unique()
        
        self.customer_to_idx = {customer: idx for idx, customer in enumerate(unique_customers)}
        self.idx_to_customer = {idx: customer for customer, idx in self.customer_to_idx.items()}
        
        self.sector_to_idx = {sector: idx for idx, sector in enumerate(unique_sectors)}
        self.idx_to_sector = {idx: sector for sector, idx in self.sector_to_idx.items()}

    def _build_interaction_matrices(self, transactions):
        # Initialize sparse matrices
        shape = (len(self.customer_to_idx), len(self.sector_to_idx))
        counts_matrix = csr_matrix(shape, dtype=int)
        amounts_matrix = csr_matrix(shape, dtype=float)
        
        # Populate matrices
        for _, row in transactions.iterrows():
            customer_idx = self.customer_to_idx[row['ACCT']]
            sector_idx = self.sector_to_idx[row['Sector']]
            counts_matrix[customer_idx, sector_idx] += 1
            amounts_matrix[customer_idx, sector_idx] += row['TRAN_AMT']
        
        # Normalize matrices to ensure equal weighting
        scaler = MaxAbsScaler()
        counts_matrix = csr_matrix(scaler.fit_transform(counts_matrix))
        amounts_matrix = csr_matrix(scaler.fit_transform(amounts_matrix))
        
        return counts_matrix, amounts_matrix

    def predict_top_sectors(self, customer_id, top_n=5):
        if customer_id not in self.customer_to_idx:
            return []
        
        customer_idx = self.customer_to_idx[customer_id]
        distances, indices = self.nn_model.kneighbors([self.nn_model._fit_X[customer_idx]], n_neighbors=top_n + 1)
        
        # Aggregate sectors from the neighbors
        sector_scores = {}
        for neighbor_idx in indices.flatten()[1:]:
            sectors = self.nn_model._fit_X[neighbor_idx].nonzero()[1]
            for sector_idx in sectors:
                sector = self.idx_to_sector[sector_idx]
                sector_scores[sector] = sector_scores.get(sector, 0) + 1
        
        # Return the top N sectors
        top_sectors = sorted(sector_scores, key=sector_scores.get, reverse=True)[:top_n]
        return top_sectors

# Example Usage
transactions = pd.read_csv("path/to/your/transaction_data.csv")
model = CollaborativeFilteringModel()
model.fit(transactions)

customer_id = 'example_customer_id'  # Replace with an actual customer ID from your dataset
top_sectors = model.predict_top_sectors(customer_id, top_n=5)
print(f"Top sectors for customer {customer_id}: {top_sectors}")










import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np

class SectorRankingModel:
    def __init__(self):
        self.norm_counts = None
        self.norm_amounts = None
        self.customer_index_mapping = {}  # Maps ACCT ID to numerical index
        self.index_customer_mapping = {}  # Maps numerical index back to ACCT ID
        self.sectors = None

    def fit(self, transactions):
        transactions['Day_Type'] = transactions['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')
        
        # Create pivot tables
        pivot_counts = self.create_pivot_tables(transactions, 'count')
        pivot_amounts = self.create_pivot_tables(transactions, 'sum')
        
        # Normalize data
        self.norm_counts = self.normalize_data(pivot_counts)
        self.norm_amounts = self.normalize_data(pivot_amounts)
        
        # Create mappings for ACCT IDs
        self.customer_index_mapping = {acct: idx for idx, acct in enumerate(pivot_counts.index)}
        self.index_customer_mapping = {idx: acct for acct, idx in self.customer_index_mapping.items()}
        self.sectors = pivot_counts.columns

    def create_pivot_tables(self, dataframe, aggfunc):
        return dataframe.pivot_table(
            index='ACCT',
            columns='Sector',
            values='TRAN_AMT',
            aggfunc=aggfunc,
            fill_value=0
        )
    
    def normalize_data(self, data):
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data)
        return pd.DataFrame(scaled_data, index=data.index, columns=data.columns)
    
    def predict(self, customer_id, n_top=5):
        if customer_id not in self.customer_index_mapping:
            return []
        customer_index = self.customer_index_mapping[customer_id]
        similarity_matrix = cosine_similarity(self.norm_counts + self.norm_amounts)
        
        weighted_scores = np.dot(similarity_matrix[customer_index], (self.norm_counts + self.norm_amounts) / 2)
        top_sector_indices = weighted_scores.argsort()[::-1][:n_top]
        
        top_sectors = [self.sectors[i] for i in top_sector_indices]
        return top_sectors

    @staticmethod
    def evaluate_precision_at_k(model, test_transactions, k=5):
        test_transactions['Day_Type'] = test_transactions['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')
        test_pivot = model.create_pivot_tables(test_transactions, 'count')
        
        hits = 0
        total = 0
        
        for customer_id in test_pivot.index:
            if customer_id not in model.customer_index_mapping:
                continue  # Skip customers not in the training set
            actual_top_sectors = test_pivot.loc[customer_id].sort_values(ascending=False).head(k).index
            predicted_top_sectors = model.predict(customer_id, n_top=k)
            
            # Count how many of the predicted top sectors are in the actual top sectors
            hits += len(set(predicted_top_sectors) & set(actual_top_sectors))
            total += k
        
        precision_at_k = hits / total if total > 0 else 0
        return precision_at_k
