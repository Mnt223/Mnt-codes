import requests
from bs4 import BeautifulSoup
import time

def extract_stores(url, store_list_class, store_class, store_name_tag):
    """Fetches and extracts store names from a given mall website."""

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'
    }

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            store_list_element = soup.find('div', class_=store_list_class)
            stores = []

            if store_list_element:
                shop_elements = store_list_element.find_all('div', class_=store_class)
                for shop_element in shop_elements:
                    shop_name = shop_element.find(store_name_tag).text.strip()
                    stores.append(shop_name)

            return stores

    except requests.exceptions.RequestException as e:
        print(f"Error scraping website: {e}")
        time.sleep(5)  # Introduce a delay before retrying 
        return None  # Return None on error

# Example Usage (Replace with the correct website and selectors)
url = "https://bengaluru.lulumall.in/"  
store_list_class = "your-store-list-class" 
store_class = "your-shop-class"
store_name_tag = "h3"

stores = extract_stores(url, store_list_class, store_class, store_name_tag)

if stores:
    print(stores)
else:
    print("Store extraction failed. Check your selectors or website availability.")












import requests
from bs4 import BeautifulSoup

url = "https://www.lucknow.lulumall.in/"  # Update if the store list is on a different page
html_content = requests.get(url).text  # Get HTML as text

soup = BeautifulSoup(html_content, 'html.parser')

# Assuming the HTML snippet is within a larger element:
store_list_element = soup.find('div', class_='mentor-widget-container') 

if store_list_element:
    categories = store_list_element.find_all('li', class_='elementor-icon-list-item')

    for category in categories:
        category_link = category.find('a')['href']
        category_name = category.find('span', class_='elementor-icon-list-text').text.strip()
        print(f"Category: {category_name}")
        print(f"Link: {category_link}")
        print("----")











import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import OneHotEncoder

# Function to calculate cosine similarity
def calculate_cosine_similarity(data):
  similarity_matrix = cosine_similarity(data)
  return similarity_matrix

# Function to calculate similarity based on age and occupation
def calculate_profile_similarity(customer1_data, customer2_data):
  age_similarity = 1 - abs(customer1_data['Age'] - customer2_data['Age']) / 50 
  occupation_similarity = np.dot(customer1_data['Occupation_Encoded'], customer2_data['Occupation_Encoded'].T)
  return age_similarity * 0.3 + occupation_similarity * 0.7 

# Function to find the top 5 sectors based on similarity
def find_top_sectors(data, customer_index, n=5):
  # Collaborative similarity
  similarities = calculate_cosine_similarity(data.values)
  similarities_series = pd.Series(similarities[customer_index], index=data.index)

  # Profile similarity
  profile_similarities = []
  for i in range(len(data)):
      profile_similarities.append(calculate_profile_similarity(data.iloc[customer_index], data.iloc[i]))
  profile_similarities_series = pd.Series(profile_similarities, index=data.index)

  # Combine similarities
  combined_similarities = 0.6 * similarities_series + 0.4 * profile_similarities_series 

  # Find top sectors
  sorted_similarities = combined_similarities.sort_values(ascending=False)
  top_sectors = []
  for similar_customer in sorted_similarities.index[:n]:
    top_sector = data.loc[similar_customer].idxmax()
    if top_sector not in top_sectors:
      top_sectors.append(top_sector)
      if len(top_sectors) == n:
        break
  return top_sectors

# Load data
df = pd.read_csv("IF_MAIN6.csv")

# Preprocessing
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Pivot tables for sector frequencies
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_frequency_weekday = df[df['Day_Type'] == 'Weekday'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_frequency_weekend = df[df['Day_Type'] == 'Weekend'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)

# Encoding occupations
occupation_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False) 
df['Occupation_Encoded'] = occupation_encoder.fit_transform(df[['Occupation']])

# Initialize results DataFrame
results_df = pd.DataFrame(columns=['ACCT', 'Overall Top Sectors', 'Weekday Top Sectors', 'Weekend Top Sectors'])

# Iterate over each customer and get recommendations
for customer_id in sector_frequency.index:
  customer_index = list(sector_frequency.index).index(customer_id)
  overall_top_sectors = find_top_sectors(sector_frequency, customer_index)
  weekday_top_sectors = find_top_sectors(sector_frequency_weekday, customer_index)
  weekend_top_sectors = find_top_sectors(sector_frequency_weekend, customer_index)

  # Append results
  results_df = results_df.append({
      'ACCT': customer_id,
      'Overall Top Sectors': overall_top_sectors,
      'Weekday Top Sectors': weekday_top_sectors,
      'Weekend Top Sectors': weekend_top_sectors
  }, ignore_index=True)

# Save the results to a CSV file
results_df.to_csv("Customer_Top_Sectors.csv", index=False)
print("File Saved: Customer_Top_Sectors.csv")








# Import necessary libraries
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Function to calculate cosine similarity
def calculate_cosine_similarity(data):
    similarity_matrix = cosine_similarity(data)
    return similarity_matrix

# Function to find the top 5 sectors based on cosine similarity
def find_top_sectors(data, customer_index, n=5):
    similarities = calculate_cosine_similarity(data.values)
    similarities_series = pd.Series(similarities[customer_index], index=data.index)
    sorted_similarities = similarities_series.sort_values(ascending=False)
    top_sectors = []
    for similar_customer in sorted_similarities.index[:n]:
        top_sector = data.loc[similar_customer].idxmax()
        if top_sector not in top_sectors:
            top_sectors.append(top_sector)
        if len(top_sectors) == n:
            break
    return top_sectors

# Load data
df = pd.read_csv("IF_MAIN6.csv")

# Preprocessing
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Pivot tables for sector frequencies
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_frequency_weekday = df[df['Day_Type'] == 'Weekday'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_frequency_weekend = df[df['Day_Type'] == 'Weekend'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)

# Initialize an empty DataFrame to store results
results_df = pd.DataFrame(columns=['ACCT', 'Overall Top Sectors', 'Weekday Top Sectors', 'Weekend Top Sectors'])

# Iterate over each customer
for customer_id in sector_frequency.index:
    customer_index = list(sector_frequency.index).index(customer_id)
    overall_top_sectors = find_top_sectors(sector_frequency, customer_index)
    weekday_top_sectors = find_top_sectors(sector_frequency_weekday, customer_index)
    weekend_top_sectors = find_top_sectors(sector_frequency_weekend, customer_index)
    
    # Append the results to the DataFrame
    results_df = results_df.append({
        'ACCT': customer_id,
        'Overall Top Sectors': overall_top_sectors,
        'Weekday Top Sectors': weekday_top_sectors,
        'Weekend Top Sectors': weekend_top_sectors
    }, ignore_index=True)

# Save the results to a CSV file
results_df.to_csv("Customer_Top_Sectors.csv", index=False)

print("File saved: Customer_Top_Sectors.csv")
