import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from scipy.sparse import csr_matrix

# Load dataset
df = pd.read_csv('main.csv')
df['Date'] = pd.to_datetime(df['Date'])

# Aggregate transaction data
agg_amount = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum').fillna(0)
agg_freq = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count').fillna(0)
customer_age = df.groupby('ACCT')['AGE_NUM'].first()

# Scaling
scaler = RobustScaler()
agg_amount_scaled = scaler.fit_transform(agg_amount)
agg_freq_scaled = scaler.fit_transform(agg_freq)
customer_age_scaled = scaler.fit_transform(customer_age.values.reshape(-1, 1))

# Convert to sparse matrices
agg_amount_sparse = csr_matrix(agg_amount_scaled)
agg_freq_sparse = csr_matrix(agg_freq_scaled)

# Popularity-Based Recommendations
def get_popular_sectors(top_n=5):
    sector_popularity_amount = pd.DataFrame(agg_amount.sum(axis=0), columns=['popularity']).sort_values(by='popularity', ascending=False)
    return sector_popularity_amount.head(top_n).index.tolist()

# Memory-Based CF
neighbors_model = NearestNeighbors(n_neighbors=5, metric='cosine')
neighbors_model.fit(agg_amount_sparse)

def get_memory_based_recommendations(acct_index, top_n=5):
    distances, indices = neighbors_model.kneighbors(agg_amount_sparse[acct_index], n_neighbors=top_n)
    sectors = agg_amount.columns[agg_amount.iloc[indices.flatten()].values.argmax(axis=1)]
    return list(dict.fromkeys(sectors))[:top_n]  # Unique sectors

# Model-Based CF
sectors = agg_amount.columns
models = {}
for sector in sectors:
    X = agg_amount_scaled.drop(sector, axis=1)
    y = agg_amount_scaled[sector]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    models[sector] = model

def get_model_based_recommendations(acct_index, top_n=5):
    acct_data = agg_amount_scaled[acct_index:acct_index+1]
    predictions = {sector: model.predict(acct_data.drop(sector, axis=1))[0] for sector, model in models.items()}
    top_sectors = sorted(predictions, key=predictions.get, reverse=True)[:top_n]
    return top_sectors

# Weighted Hybrid Recommendations
def weighted_hybrid_recommendations(acct_index, top_n=5):
    scores = {}
    popular_sectors = get_popular_sectors(top_n)
    memory_based_sectors = get_memory_based_recommendations(acct_index, top_n)
    model_based_sectors = get_model_based_recommendations(acct_index, top_n)
    
    for sector in popular_sectors:
        scores[sector] = scores.get(sector, 0) + 1
    for sector in memory_based_sectors:
        scores[sector] = scores.get(sector, 0) + 2
    for sector in model_based_sectors:
        scores[sector] = scores.get(sector, 0) + 3
    
    top_sectors = sorted(scores, key=scores.get, reverse=True)[:top_n]
    return top_sectors

# Evaluation (Precision and Recall)
def evaluate_recommendations(true_sectors, recommended_sectors):
    true_set = set(true_sectors)
    recommended_set = set(recommended_sectors)
    intersection = true_set.intersection(recommended_set)
    precision = len(intersection) / len(recommended_set) if recommended_set else 0
    recall = len(intersection) / len(true_set) if true_set else 0
    return precision, recall

# Example Usage
acct_index = 0  # Replace with actual index from your dataset
true_sectors = ['Sector1', 'Sector3']  # Replace with actual sectors for evaluation
recommended_sectors = weighted_hybrid_recommendations(acct_index, top_n=5)
precision, recall = evaluate_recommendations(true_sectors, recommended_sectors)
print(f"Recommended Sectors: {recommended_sectors}")
print(f"Precision: {precision}, Recall: {recall}")










import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('main.csv')

# Data Preparation
# Convert sectors to one-hot encoding and multiply by transaction amount for weighted interactions
df_one_hot = pd.get_dummies(df['Sector'])
df_weighted = df_one_hot.multiply(df['TRAN_AMT'], axis="index")

# Aggregate this data by ACCT to get a profile for each customer's interactions with sectors
customer_profiles = df_weighted.groupby(df['ACCT']).sum()

# Standardize the customer profiles
scaler = StandardScaler()
customer_profiles_scaled = scaler.fit_transform(customer_profiles)
customer_profiles_scaled = pd.DataFrame(customer_profiles_scaled, index=customer_profiles.index, columns=customer_profiles.columns)

# 1. Popularity-Based Model
sector_popularity = customer_profiles.sum().sort_values(ascending=False)

# 2. Memory-Based Collaborative Filtering (CF)
# Calculate cosine similarity among users
user_similarity = cosine_similarity(customer_profiles_scaled)
user_similarity_df = pd.DataFrame(user_similarity, index=customer_profiles.index, columns=customer_profiles.index)

# 3. Model-Based Collaborative Filtering (CF)
# Using Random Forest as an example to predict customer interaction for a specific sector based on their profile
# Let's choose a target sector for demonstration, e.g., 'Entertainment'
target_sector = 'Entertainment'
X = customer_profiles_scaled.drop(target_sector, axis=1)
y = customer_profiles_scaled[target_sector]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

predictions = model.predict(X_test)

mse = mean_squared_error(y_test, predictions)
print(f'MSE for predicting {target_sector}: {mse}')

# Note: This code focuses on demonstrating the integration of all three models.
# In practice, further steps are needed for a complete recommendation system, such as:
# - Transforming predictions from model-based CF into actionable recommendations
# - Combining recommendations from all three models into a final recommendation list per user


# Calculate the popularity based on total transaction amount
sector_popularity_amount = agg_amount_scaled.sum().sort_values(ascending=False)

# Calculate the popularity based on transaction frequency
sector_popularity_freq = agg_freq_scaled.sum().sort_values(ascending=False)

print("Top 5 Popular Sectors by Amount:")
print(sector_popularity_amount.head(5))
print("\nTop 5 Popular Sectors by Frequency:")
print(sector_popularity_freq.head(5))


from sklearn.model_selection import train_test_split

# Placeholder dictionaries to store models and their metrics
models_amount = {}
models_freq = {}
mse_scores_amount = {}
mse_scores_freq = {}

sectors = agg_amount_scaled.columns

for sector in sectors:
    print(f"Training models for sector: {sector}")
    
    # Prepare features and labels for amount and frequency
    X_amount = agg_amount_scaled.drop(sector, axis=1)
    y_amount = agg_amount_scaled[sector]
    
    X_freq = agg_freq_scaled.drop(sector + '_freq', axis=1, errors='ignore')  # Using errors='ignore' in case '_freq' suffix was not added
    y_freq = agg_freq_scaled.get(sector + '_freq', pd.Series(index=X_freq.index, data=0))  # Default to 0 if not present
    
    # Splitting the dataset (demonstration purposes, consider cross-validation in practice)
    X_train_amount, X_test_amount, y_train_amount, y_test_amount = train_test_split(X_amount, y_amount, test_size=0.2, random_state=42)
    X_train_freq, X_test_freq, y_train_freq, y_test_freq = train_test_split(X_freq, y_freq, test_size=0.2, random_state=42)
    
    # Model training for transaction amount
    model_amount = RandomForestRegressor(n_estimators=100, random_state=42)
    model_amount.fit(X_train_amount, y_train_amount)
    predictions_amount = model_amount.predict(X_test_amount)
    mse_amount = mean_squared_error(y_test_amount, predictions_amount)
    
    # Model training for transaction frequency
    model_freq = RandomForestRegressor(n_estimators=100, random_state=42)
    model_freq.fit(X_train_freq, y_train_freq)
    predictions_freq = model_freq.predict(X_test_freq)
    mse_freq = mean_squared_error(y_test_freq, predictions_freq)
    
    # Store models and their metrics
    models_amount[sector] = model_amount
    mse_scores_amount[sector] = mse_amount
    models_freq[sector] = model_freq
    mse_scores_freq[sector] = mse_freq
    
    print(f"Finished {sector}: MSE Amount - {mse_amount}, MSE Frequency - {mse_freq}")
# - Evaluating and tuning the models based on performance metrics relevant to recommendation systems


# Calculate cosine similarity based on transaction amounts
cosine_sim_amount = cosine_similarity(agg_amount_scaled)

# Calculate cosine similarity based on transaction frequencies
cosine_sim_freq = cosine_similarity(agg_freq_scaled)

# Average the similarity scores from amount and frequency for an overall similarity score
cosine_sim_overall = (cosine_sim_amount + cosine_sim_freq) / 2

# Convert to DataFrame for easier handling
cosine_sim_overall_df = pd.DataFrame(cosine_sim_overall, index=agg_amount_scaled.index, columns=agg_amount_scaled.index)

# Display similarity scores for the first customer as an example
print("Overall Similarity Scores for the First Customer:")
print(cosine_sim_overall_df.iloc[0].sort_values(ascending=False).head())
