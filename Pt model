import pandas as pd

def rank_and_encode_transactions(data_file, output_rank_file, output_encoded_file):
  """
  Ranks transactions per sector and generates hot encoding for sectors with amounts.

  Args:
      data_file: Path to the CSV data file.
      output_rank_file: Path to save the ranked transactions CSV file.
      output_encoded_file: Path to save the hot-encoded sectors CSV file.
  """

  # Read data
  data = pd.read_csv(data_file)

  # Rank transactions per sector
  sector_totals = data.groupby('Sector')['TRAN_AMT'].sum().reset_index()
  sector_totals = sector_totals.sort_values(by='TRAN_AMT', ascending=False)
  sector_totals.to_csv(output_rank_file, index=False)

  # Hot encoding for sectors with amounts
  transactions_with_amount = data[data['TRAN_AMT'] > 0]
  sector_dummies = pd.get_dummies(transactions_with_amount['Sector'], prefix='Sector')
  sector_dummies.to_csv(output_encoded_file, index=False)

# Example usage:
data_file = 'Main.csv'  # Replace with your data file path
output_rank_file = 'ranked_transactions.csv'
output_encoded_file = 'sector_encoding.csv'

rank_and_encode_transactions(data_file, output_rank_file, output_encoded_file)

print("Transaction rankings and sector encoding completed. Check the output CSV files.")







new
import os
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.preprocessing import RobustScaler
import numpy as np
import implicit

# Environment setup for OpenBLAS
os.environ['OPENBLAS_NUM_THREADS'] = '1'

# Prepare output directory
output_dir = 'final_model_outputs'
os.makedirs(output_dir, exist_ok=True)

# Load the dataset
data = pd.read_csv('Main.csv')
data['Date'] = pd.to_datetime(data['Date'])
data.sort_values(by='Date', inplace=True)

# Temporal split
split_date = data['Date'].quantile(0.8, interpolation='nearest')
train_data = data[data['Date'] <= split_date]
test_data = data[data['Date'] > split_date]

# Interaction matrix for training data
def prepare_interaction_matrix(df):
    counts = df.groupby(['ACCT', 'Sector']).size().unstack(fill_value=0)
    amounts = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
    combined = np.log1p(counts) + np.log1p(amounts)
    scaler = RobustScaler()
    combined_scaled = scaler.fit_transform(combined)
    return csr_matrix(combined_scaled), counts.index.tolist(), counts.columns.tolist()

train_matrix, train_accts, sectors = prepare_interaction_matrix(train_data)

# Initialize and train the ALS model
model = implicit.als.AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20, use_gpu=False)
model.fit(train_matrix.T)  # Item-User matrix

# Popular sectors for fallback recommendations
def get_fallback_recommendations(df, n=5):
    popular_sectors = df.groupby('Sector')['TRAN_AMT'].sum().nlargest(n).index.tolist()
    return popular_sectors

fallback_rec = get_fallback_recommendations(train_data)

# Generate and export recommendations
def export_recommendations(model, train_matrix, train_accts, sectors, fallback_rec, output_path):
    all_recs = {}
    for acct in set(train_accts + test_data['ACCT'].unique().tolist()):
        if acct in train_accts:
            user_idx = train_accts.index(acct)
            recs = [sectors[i] for i, _ in model.recommend(user_idx, train_matrix, N=5)]
        else:
            recs = fallback_rec
        all_recs[acct] = recs
    rec_df = pd.DataFrame.from_dict(all_recs, orient='index', columns=[f'Rec_{i+1}' for i in range(5)])
    rec_df.to_csv(output_path)

export_recommendations(model, train_matrix, train_accts, sectors, fallback_rec, os.path.join(output_dir, 'all_user_recommendations.csv'))

print("Recommendations generated for all users. Please check the 'final_model_outputs' directory.")





final code 
import os
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.preprocessing import RobustScaler
import numpy as np
import implicit

# Set environment variable for OpenBLAS to use a single thread
os.environ['OPENBLAS_NUM_THREADS'] = '1'

# Prepare the output directory
output_dir = 'complete_model_outputs'
os.makedirs(output_dir, exist_ok=True)

# Load and prepare the data
data = pd.read_csv('Main.csv')
data['Date'] = pd.to_datetime(data['Date'])
data.sort_values(by='Date', inplace=True)

# Temporal split
split_date = data['Date'].quantile(0.8, interpolation='nearest')
train_data = data[data['Date'] <= split_date]
test_data = data[data['Date'] > split_date]

# Function to prepare interaction matrices
def prepare_interaction_matrix(df, index=None):
    counts = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='size', fill_value=0)
    amounts = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
    if index is not None:
        counts = counts.reindex(index, fill_value=0)
        amounts = amounts.reindex(index, fill_value=0)
    scaler = RobustScaler()
    combined = scaler.fit_transform(np.log1p(counts) + np.log1p(amounts))
    return csr_matrix(combined), counts.index, counts.columns

train_matrix, train_index, sectors = prepare_interaction_matrix(train_data)
test_matrix, _, _ = prepare_interaction_matrix(test_data, train_index)

# Initialize and train the ALS model
model = implicit.als.AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20, use_gpu=False)
model.fit(train_matrix.T * 40)  # Item-User format

# Function to calculate Precision@K
def calculate_precision_at_k(model, train_matrix, test_matrix, k=5):
    hits = 0
    total = 0
    for user_id in range(train_matrix.shape[0]):
        test_interactions = test_matrix[user_id].indices
        if len(test_interactions) == 0:
            continue
        recommendations = [rec[0] for rec in model.recommend(user_id, train_matrix, N=k)]
        hits += len(set(recommendations) & set(test_interactions))
        total += len(test_interactions)
    return hits / (total * k) if total else 0

precision = calculate_precision_at_k(model, train_matrix, test_matrix, k=5)
print(f"Precision@5: {precision}")

# Exporting the results
def export_results(matrix, index, columns, filename):
    df = pd.DataFrame(matrix.toarray(), index=index, columns=columns)
    df.to_csv(os.path.join(output_dir, filename))

export_results(train_matrix, train_index, sectors, 'train_matrix.csv')
export_results(test_matrix, train_index, sectors, 'test_matrix.csv')

# Generating and exporting recommendations for all users
def export_recommendations(model, matrix, user_ids, filename):
    recs = {user_id: [sectors[rec[0]] for rec in model.recommend(idx, matrix, N=5)] for idx, user_id in enumerate(user_ids)}
    rec_df = pd.DataFrame.from_dict(recs, orient='index')
    rec_df.to_csv(os.path.join(output_dir, filename))

export_recommendations(model, train_matrix, train_index, 'user_recommendations.csv')

print("Completed all tasks. Check the 'complete_model_outputs' directory for results.")





2nd code
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.preprocessing import RobustScaler
import numpy as np
import implicit
import os

# Create output directory
output_dir = 'complete_model_outputs'
os.makedirs(output_dir, exist_ok=True)

# Load data
data = pd.read_csv('Main.csv')
data['Date'] = pd.to_datetime(data['Date'])
data.sort_values(by='Date', inplace=True)

# Temporal split
split_date = data['Date'].quantile(0.8)
train_data = data[data['Date'] <= split_date]
test_data = data[data['Date'] > split_date]

# Export train and test data
train_data.to_csv(os.path.join(output_dir, 'train_data.csv'), index=False)
test_data.to_csv(os.path.join(output_dir, 'test_data.csv'), index=False)

# Prepare interaction matrices for counts and amounts
def prepare_matrices(df):
    interaction_counts = df.groupby(['ACCT', 'Sector']).size().unstack(fill_value=0)
    interaction_amounts = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
    return interaction_counts, interaction_amounts

train_counts, train_amounts = prepare_matrices(train_data)
test_counts, test_amounts = prepare_matrices(test_data)

# Scale and combine counts and amounts
scaler_counts = RobustScaler()
scaler_amounts = RobustScaler()
counts_scaled = scaler_counts.fit_transform(np.log1p(train_counts)) # Log transform to handle skewness in counts
amounts_scaled = scaler_amounts.fit_transform(np.log1p(train_amounts)) # Log transform to handle skewness in amounts

# Combine counts and amounts into a single interaction strength matrix
combined_strength = (counts_scaled + amounts_scaled) / 2
sparse_matrix = csr_matrix(combined_strength)

# Train the ALS model
model = implicit.als.AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20, use_gpu=False)
model.fit(sparse_matrix.T * 40) # Transpose for item-user format

# Export combined matrix for analysis
pd.DataFrame(combined_strength, index=train_counts.index, columns=train_counts.columns).to_csv(os.path.join(output_dir, 'combined_strength.csv'))

# Function to calculate and save Precision@K
def calculate_precision_at_k(model, train_sparse, test_counts, k=5):
    hits = 0
    total = 0
    for acct, row in test_counts.iterrows():
        if acct not in train_counts.index:
            continue
        user_id = train_counts.index.get_loc(acct)
        test_items = set(row[row > 0].index)
        if not test_items:
            continue
        recommended, _ = zip(*model.recommend(user_id, train_sparse, N=k))
        hits += len(set(recommended) & test_items)
        total += k
    precision_at_k = hits / total if total > 0 else 0
    return precision_at_k

# Evaluate the model
precision = calculate_precision_at_k(model, sparse_matrix, test_counts, k=5)
print(f"Precision@K: {precision}")
with open(os.path.join(output_dir, 'precision_at_k.txt'), 'w') as f:
    f.write(f"Precision@K: {precision}")

# Generate and export recommendations for each user
def export_recommendations(model, sparse_matrix, acct_index, sectors, filename):
    recommendations = {}
    for user_id, acct in enumerate(acct_index):
        recommended_items = model.recommend(user_id, sparse_matrix, N=5)
        recommendations[acct] = [sectors[item_id] for item_id, _ in recommended_items]
    recommendations_df = pd.DataFrame.from_dict(recommendations, orient='index')
    recommendations_df.to_csv(filename)

export_recommendations(model, sparse_matrix, train_counts.index.tolist(), train_counts.columns.tolist(), os.path.join(output_dir, 'user_recommendations.csv'))








1st code 
import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.preprocessing import RobustScaler
import implicit
import os

# Ensure the output directory exists
output_dir = 'full_model_evaluation_outputs'
os.makedirs(output_dir, exist_ok=True)

# Load the data
data = pd.read_csv('Main.csv')
data['Date'] = pd.to_datetime(data['Date'])
data.sort_values('Date', inplace=True)

# Temporal split for train and test datasets
split_date = data['Date'].quantile(0.8)
train_data = data[data['Date'] <= split_date]
test_data = data[data['Date'] > split_date]

# Function to create interaction matrix
def create_interaction_matrix(df, acct_col, sector_col, value_col):
    interactions = df.pivot_table(index=acct_col, columns=sector_col, values=value_col, aggfunc='size', fill_value=0)
    return interactions

# Create interaction matrices for counts and amounts
interaction_counts_train = create_interaction_matrix(train_data, 'ACCT', 'Sector', 'TRAN_AMT')
interaction_counts_test = create_interaction_matrix(test_data, 'ACCT', 'Sector', 'TRAN_AMT')

# Apply scaling and combine
scaler = RobustScaler()
counts_scaled = scaler.fit_transform(interaction_counts_train)
sparse_matrix_train = csr_matrix(counts_scaled)

# Initialize and fit the model
model = implicit.als.AlternatingLeastSquares(factors=50, regularization=0.01, iterations=20, use_gpu=False)
model.fit(sparse_matrix_train.T)  # Transpose matrix to fit item-user format

# Generating and saving recommendations for each user
def generate_and_save_recommendations(model, sparse_data, filename):
    num_users = sparse_data.shape[0]
    recommendations = []
    
    for user_id in range(num_users):
        recs = model.recommend(user_id, sparse_data, N=5)
        recommendations.append([rec[0] for rec in recs])
    
    rec_df = pd.DataFrame(recommendations, columns=[f'Rec_{i+1}' for i in range(5)])
    rec_df.to_csv(os.path.join(output_dir, filename), index_label='UserIndex')

generate_and_save_recommendations(model, sparse_matrix_train, 'recommendations.csv')

# Precision@K calculation
def calculate_precision_at_k(model, train_sparse, test_sparse, k=5):
    hits = 0
    total = 0
    
    for user_id in range(train_sparse.shape[0]):
        if user_id >= test_sparse.shape[0]:
            continue  # User not present in test data
        test_interactions = test_sparse[user_id].indices
        if len(test_interactions) == 0:
            continue  # Skip users with no interactions in test set
            
        recommendations = [rec[0] for rec in model.recommend(user_id, train_sparse, N=k)]
        hits += len(set(recommendations) & set(test_interactions))
        total += k
    
    return hits / total if total > 0 else 0

# Preparing test interaction matrix similarly
counts_scaled_test = scaler.transform(interaction_counts_test)
sparse_matrix_test = csr_matrix(counts_scaled_test)

# Compute and print Precision@K
precision_at_k = calculate_precision_at_k(model, sparse_matrix_train, sparse_matrix_test, k=5)
print(f"Precision@K for the model: {precision_at_k}")

# Save train and test matrices for reference
pd.DataFrame(sparse_matrix_train.toarray()).to_csv(os.path.join(output_dir, 'train_matrix.csv'))
pd.DataFrame(sparse_matrix_test.toarray()).to_csv(os.path.join(output_dir, 'test_matrix.csv'))








import pandas as pd
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors

# Load the dataset
df = pd.read_csv('main.csv')
df['Date'] = pd.to_datetime(df['Date'])

# Aggregate transaction data
agg_amount = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum').fillna(0)
agg_freq = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count').fillna(0)

# Scaling
scaler_amount = RobustScaler()
scaler_freq = RobustScaler()
agg_amount_scaled = pd.DataFrame(scaler_amount.fit_transform(agg_amount), columns=agg_amount.columns, index=agg_amount.index)
agg_freq_scaled = pd.DataFrame(scaler_freq.fit_transform(agg_freq), columns=agg_freq.columns, index=agg_freq.index)

# Nearest Neighbors for Memory-Based CF
neighbors_model_amount = NearestNeighbors(n_neighbors=5, metric='cosine')
neighbors_model_freq = NearestNeighbors(n_neighbors=5, metric='cosine')
agg_amount_sparse = csr_matrix(agg_amount_scaled)
agg_freq_sparse = csr_matrix(agg_freq_scaled)
neighbors_model_amount.fit(agg_amount_sparse)
neighbors_model_freq.fit(agg_freq_sparse)

# Function to find nearest neighbors and recommend sectors based on those neighbors
def get_neighbors_recommendations(acct_index, top_n=5, use_freq=False):
    neighbors_model = neighbors_model_freq if use_freq else neighbors_model_amount
    agg_data_sparse = agg_freq_sparse if use_freq else agg_amount_sparse
    distances, indices = neighbors_model.kneighbors(agg_data_sparse[acct_index:acct_index+1], n_neighbors=top_n)
    sectors = agg_amount.columns[agg_amount.iloc[indices[0]].sum().nlargest(top_n).index].tolist()
    return sectors

# Model-Based Collaborative Filtering for both Amount and Frequency
models_amount = {}
models_freq = {}

for sector in agg_amount.columns:
    # Amount model
    X_amount = agg_amount_scaled.drop(columns=[sector])
    y_amount = agg_amount_scaled[sector]
    model_amount = RandomForestRegressor(n_estimators=100, random_state=42)
    model_amount.fit(X_amount, y_amount)
    models_amount[sector] = model_amount
    
    # Frequency model
    X_freq = agg_freq_scaled.drop(columns=[sector])
    y_freq = agg_freq_scaled[sector]
    model_freq = RandomForestRegressor(n_estimators=100, random_state=42)
    model_freq.fit(X_freq, y_freq)
    models_freq[sector] = model_freq

# Weighted Hybrid Recommendations
def weighted_hybrid_recommendations(acct_index, top_n=5):
    sectors_amount = get_neighbors_recommendations(acct_index, top_n=top_n, use_freq=False)
    sectors_freq = get_neighbors_recommendations(acct_index, top_n=top_n, use_freq=True)
    combined_sectors = list(dict.fromkeys(sectors_amount + sectors_freq))[:top_n]
    return combined_sectors

# Placeholder function to simulate true sectors (replace with your actual data source)
def get_true_sectors(acct_index):
    return ['Sector1', 'Sector3']  # Replace with actual sectors

# Evaluation
def evaluate_recommendations(true_sectors, recommended_sectors):
    true_set = set(true_sectors)
    recommended_set = set(recommended_sectors)
    precision = len(true_set & recommended_set) / len(recommended_set) if recommended_set else 0
    recall = len(true_set & recommended_set) / len(true_set) if true_set else 0
    return precision, recall

# Generate recommendations and evaluations for each customer
results = []
for acct_index in range(len(agg_amount_scaled)):
    recommended_sectors = weighted_hybrid_recommendations(acct_index, top_n=5)
    true_sectors = get_true_sectors(acct_index)  # Implement your method to get the actual sectors
    precision, recall = evaluate_recommendations(true_sectors, recommended_sectors)
    results.append({
        'ACCT': agg_amount.index[acct_index],
        'Recommended Sectors': recommended_sectors,
        'Precision': precision,
        'Recall': recall
    })

# Export results to CSV
results_df = pd.DataFrame(results)
results_df.to_csv('recommendation_results.csv', index=False)
print("Recommendation results exported to 'recommendation_results.csv'.")





import pandas as pd
import numpy as np
from sklearn.preprocessing import RobustScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from scipy.sparse import csr_matrix

# Load dataset
df = pd.read_csv('main.csv')
df['Date'] = pd.to_datetime(df['Date'])

# Aggregate transaction data
agg_amount = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum').fillna(0)
agg_freq = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count').fillna(0)
customer_age = df.groupby('ACCT')['AGE_NUM'].first()

# Scaling
scaler = RobustScaler()
agg_amount_scaled = scaler.fit_transform(agg_amount)
agg_freq_scaled = scaler.fit_transform(agg_freq)
customer_age_scaled = scaler.fit_transform(customer_age.values.reshape(-1, 1))

# Convert to sparse matrices
agg_amount_sparse = csr_matrix(agg_amount_scaled)
agg_freq_sparse = csr_matrix(agg_freq_scaled)

# Popularity-Based Recommendations
def get_popular_sectors(top_n=5):
    sector_popularity_amount = pd.DataFrame(agg_amount.sum(axis=0), columns=['popularity']).sort_values(by='popularity', ascending=False)
    return sector_popularity_amount.head(top_n).index.tolist()

# Memory-Based CF
neighbors_model = NearestNeighbors(n_neighbors=5, metric='cosine')
neighbors_model.fit(agg_amount_sparse)

def get_memory_based_recommendations(acct_index, top_n=5):
    distances, indices = neighbors_model.kneighbors(agg_amount_sparse[acct_index], n_neighbors=top_n)
    sectors = agg_amount.columns[agg_amount.iloc[indices.flatten()].values.argmax(axis=1)]
    return list(dict.fromkeys(sectors))[:top_n]  # Unique sectors

# Model-Based CF
sectors = agg_amount.columns
models = {}
for sector in sectors:
    X = agg_amount_scaled.drop(sector, axis=1)
    y = agg_amount_scaled[sector]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    models[sector] = model

def get_model_based_recommendations(acct_index, top_n=5):
    acct_data = agg_amount_scaled[acct_index:acct_index+1]
    predictions = {sector: model.predict(acct_data.drop(sector, axis=1))[0] for sector, model in models.items()}
    top_sectors = sorted(predictions, key=predictions.get, reverse=True)[:top_n]
    return top_sectors

# Weighted Hybrid Recommendations
def weighted_hybrid_recommendations(acct_index, top_n=5):
    scores = {}
    popular_sectors = get_popular_sectors(top_n)
    memory_based_sectors = get_memory_based_recommendations(acct_index, top_n)
    model_based_sectors = get_model_based_recommendations(acct_index, top_n)
    
    for sector in popular_sectors:
        scores[sector] = scores.get(sector, 0) + 1
    for sector in memory_based_sectors:
        scores[sector] = scores.get(sector, 0) + 2
    for sector in model_based_sectors:
        scores[sector] = scores.get(sector, 0) + 3
    
    top_sectors = sorted(scores, key=scores.get, reverse=True)[:top_n]
    return top_sectors

# Evaluation (Precision and Recall)
def evaluate_recommendations(true_sectors, recommended_sectors):
    true_set = set(true_sectors)
    recommended_set = set(recommended_sectors)
    intersection = true_set.intersection(recommended_set)
    precision = len(intersection) / len(recommended_set) if recommended_set else 0
    recall = len(intersection) / len(true_set) if true_set else 0
    return precision, recall

# Example Usage
acct_index = 0  # Replace with actual index from your dataset
true_sectors = ['Sector1', 'Sector3']  # Replace with actual sectors for evaluation
recommended_sectors = weighted_hybrid_recommendations(acct_index, top_n=5)
precision, recall = evaluate_recommendations(true_sectors, recommended_sectors)
print(f"Recommended Sectors: {recommended_sectors}")
print(f"Precision: {precision}, Recall: {recall}")










import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('main.csv')

# Data Preparation
# Convert sectors to one-hot encoding and multiply by transaction amount for weighted interactions
df_one_hot = pd.get_dummies(df['Sector'])
df_weighted = df_one_hot.multiply(df['TRAN_AMT'], axis="index")

# Aggregate this data by ACCT to get a profile for each customer's interactions with sectors
customer_profiles = df_weighted.groupby(df['ACCT']).sum()

# Standardize the customer profiles
scaler = StandardScaler()
customer_profiles_scaled = scaler.fit_transform(customer_profiles)
customer_profiles_scaled = pd.DataFrame(customer_profiles_scaled, index=customer_profiles.index, columns=customer_profiles.columns)

# 1. Popularity-Based Model
sector_popularity = customer_profiles.sum().sort_values(ascending=False)

# 2. Memory-Based Collaborative Filtering (CF)
# Calculate cosine similarity among users
user_similarity = cosine_similarity(customer_profiles_scaled)
user_similarity_df = pd.DataFrame(user_similarity, index=customer_profiles.index, columns=customer_profiles.index)

# 3. Model-Based Collaborative Filtering (CF)
# Using Random Forest as an example to predict customer interaction for a specific sector based on their profile
# Let's choose a target sector for demonstration, e.g., 'Entertainment'
target_sector = 'Entertainment'
X = customer_profiles_scaled.drop(target_sector, axis=1)
y = customer_profiles_scaled[target_sector]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

predictions = model.predict(X_test)

mse = mean_squared_error(y_test, predictions)
print(f'MSE for predicting {target_sector}: {mse}')

# Note: This code focuses on demonstrating the integration of all three models.
# In practice, further steps are needed for a complete recommendation system, such as:
# - Transforming predictions from model-based CF into actionable recommendations
# - Combining recommendations from all three models into a final recommendation list per user


# Calculate the popularity based on total transaction amount
sector_popularity_amount = agg_amount_scaled.sum().sort_values(ascending=False)

# Calculate the popularity based on transaction frequency
sector_popularity_freq = agg_freq_scaled.sum().sort_values(ascending=False)

print("Top 5 Popular Sectors by Amount:")
print(sector_popularity_amount.head(5))
print("\nTop 5 Popular Sectors by Frequency:")
print(sector_popularity_freq.head(5))


from sklearn.model_selection import train_test_split

# Placeholder dictionaries to store models and their metrics
models_amount = {}
models_freq = {}
mse_scores_amount = {}
mse_scores_freq = {}

sectors = agg_amount_scaled.columns

for sector in sectors:
    print(f"Training models for sector: {sector}")
    
    # Prepare features and labels for amount and frequency
    X_amount = agg_amount_scaled.drop(sector, axis=1)
    y_amount = agg_amount_scaled[sector]
    
    X_freq = agg_freq_scaled.drop(sector + '_freq', axis=1, errors='ignore')  # Using errors='ignore' in case '_freq' suffix was not added
    y_freq = agg_freq_scaled.get(sector + '_freq', pd.Series(index=X_freq.index, data=0))  # Default to 0 if not present
    
    # Splitting the dataset (demonstration purposes, consider cross-validation in practice)
    X_train_amount, X_test_amount, y_train_amount, y_test_amount = train_test_split(X_amount, y_amount, test_size=0.2, random_state=42)
    X_train_freq, X_test_freq, y_train_freq, y_test_freq = train_test_split(X_freq, y_freq, test_size=0.2, random_state=42)
    
    # Model training for transaction amount
    model_amount = RandomForestRegressor(n_estimators=100, random_state=42)
    model_amount.fit(X_train_amount, y_train_amount)
    predictions_amount = model_amount.predict(X_test_amount)
    mse_amount = mean_squared_error(y_test_amount, predictions_amount)
    
    # Model training for transaction frequency
    model_freq = RandomForestRegressor(n_estimators=100, random_state=42)
    model_freq.fit(X_train_freq, y_train_freq)
    predictions_freq = model_freq.predict(X_test_freq)
    mse_freq = mean_squared_error(y_test_freq, predictions_freq)
    
    # Store models and their metrics
    models_amount[sector] = model_amount
    mse_scores_amount[sector] = mse_amount
    models_freq[sector] = model_freq
    mse_scores_freq[sector] = mse_freq
    
    print(f"Finished {sector}: MSE Amount - {mse_amount}, MSE Frequency - {mse_freq}")
# - Evaluating and tuning the models based on performance metrics relevant to recommendation systems


# Calculate cosine similarity based on transaction amounts
cosine_sim_amount = cosine_similarity(agg_amount_scaled)

# Calculate cosine similarity based on transaction frequencies
cosine_sim_freq = cosine_similarity(agg_freq_scaled)

# Average the similarity scores from amount and frequency for an overall similarity score
cosine_sim_overall = (cosine_sim_amount + cosine_sim_freq) / 2

# Convert to DataFrame for easier handling
cosine_sim_overall_df = pd.DataFrame(cosine_sim_overall, index=agg_amount_scaled.index, columns=agg_amount_scaled.index)

# Display similarity scores for the first customer as an example
print("Overall Similarity Scores for the First Customer:")
print(cosine_sim_overall_df.iloc[0].sort_values(ascending=False).head())
