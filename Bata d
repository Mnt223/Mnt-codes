import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

def load_and_preprocess_data(filepath):
    df = pd.read_csv(filepath)
    df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')
    df.dropna(inplace=True)
    return df

def generate_interaction_matrix(df, sectors):
    # Ensure all sectors are present, even if no transactions occurred for some
    interaction_matrix = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
    for sector in sectors:
        if sector not in interaction_matrix.columns:
            interaction_matrix[sector] = 0
    return interaction_matrix[sorted(sectors)]

def scale_interaction_matrix(interaction_matrix):
    scaler = MinMaxScaler()
    scaled_matrix = scaler.fit_transform(interaction_matrix)
    return pd.DataFrame(scaled_matrix, index=interaction_matrix.index, columns=interaction_matrix.columns)

def calculate_cosine_similarity(matrix):
    return cosine_similarity(matrix.T)

def generate_hot_encoding(df, sectors):
    # Adjust to ensure all defined sectors are included
    hot_encoded = pd.get_dummies(df[['ACCT', 'Sector']], columns=['Sector'], prefix='', prefix_sep='').groupby('ACCT').max()
    for sector in sectors:
        if sector not in hot_encoded.columns:
            hot_encoded[sector] = 0
    return hot_encoded[sorted(sectors)]

def get_recommendation_ranks(test_interaction_matrix, sector_similarity):
    scores = np.dot(test_interaction_matrix, sector_similarity)
    ranks = (-scores).argsort(axis=1).argsort(axis=1) + 1
    return pd.DataFrame(ranks, index=test_interaction_matrix.index, columns=test_interaction_matrix.columns)

def generate_transaction_ranks(df, sectors):
    interaction_matrix = generate_interaction_matrix(df, sectors)
    transaction_ranks = interaction_matrix.rank(axis=1, method='max', ascending=False).astype(int)
    return transaction_ranks

def main():
    df = load_and_preprocess_data('main6.csv')
    split_date = pd.to_datetime("2023-01-01")
    sectors = sorted(df['Sector'].unique())
    train_df, test_df = df[df['DATE'] < split_date], df[df['DATE'] >= split_date]

    train_interaction_matrix = generate_interaction_matrix(train_df, sectors)
    test_interaction_matrix = generate_interaction_matrix(test_df, sectors)
    
    train_scaled = scale_interaction_matrix(train_interaction_matrix)
    test_scaled = scale_interaction_matrix(test_interaction_matrix)
    
    sector_similarity = calculate_cosine_similarity(train_scaled)
    
    train_hot_encoding = generate_hot_encoding(train_df, sectors)
    test_hot_encoding = generate_hot_encoding(test_df, sectors)
    test_recommendations = get_recommendation_ranks(test_scaled, sector_similarity)
    test_transaction_ranks = generate_transaction_ranks(test_df, sectors)

    with pd.ExcelWriter('analysis_outputs.xlsx', engine='openpyxl') as writer:
        train_hot_encoding.to_excel(writer, sheet_name='Train Hot Encoding')
        test_hot_encoding.to_excel(writer, sheet_name='Test Hot Encoding')
        test_recommendations.to_excel(writer, sheet_name='Test Recommendations')
        test_transaction_ranks.to_excel(writer, sheet_name='Test Transaction Ranks')

    print("All requested outputs have been exported to 'analysis_outputs.xlsx'.")

if __name__ == "__main__":
    main()




tt
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

def load_and_preprocess_data(filepath):
    df = pd.read_csv(filepath)
    df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')
    df.dropna(inplace=True)
    return df

def generate_interaction_matrix(df):
    interaction_matrix = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
    return interaction_matrix

def scale_interaction_matrix(interaction_matrix):
    scaler = MinMaxScaler()
    scaled_matrix = scaler.fit_transform(interaction_matrix)
    return pd.DataFrame(scaled_matrix, index=interaction_matrix.index, columns=interaction_matrix.columns)

def calculate_cosine_similarity(matrix):
    return cosine_similarity(matrix.T)

def generate_hot_encoding(df):
    hot_encoded = pd.get_dummies(df[['ACCT', 'Sector']], columns=['Sector'], prefix='', prefix_sep='').groupby('ACCT').max()
    return hot_encoded

def get_recommendation_ranks(test_interaction_matrix, sector_similarity):
    scores = np.dot(test_interaction_matrix, sector_similarity)
    ranks = (-scores).argsort(axis=1).argsort(axis=1) + 1
    return pd.DataFrame(ranks, index=test_interaction_matrix.index, columns=test_interaction_matrix.columns)

def generate_transaction_ranks(df):
    interaction_matrix = generate_interaction_matrix(df)
    transaction_ranks = interaction_matrix.rank(axis=1, method='max', ascending=False).astype(int)
    return transaction_ranks

def main():
    df = load_and_preprocess_data('main6.csv')
    split_date = pd.to_datetime("2023-01-01")
    sectors = sorted(df['Sector'].unique())
    train_df, test_df = df[df['DATE'] < split_date], df[df['DATE'] >= split_date]

    train_interaction_matrix = generate_interaction_matrix(train_df)
    test_interaction_matrix = generate_interaction_matrix(test_df)

    train_scaled = scale_interaction_matrix(train_interaction_matrix)
    test_scaled = scale_interaction_matrix(test_interaction_matrix)

    sector_similarity = calculate_cosine_similarity(train_scaled)
    
    train_hot_encoding = generate_hot_encoding(train_df)
    test_hot_encoding = generate_hot_encoding(test_df)
    test_recommendations = get_recommendation_ranks(test_scaled, sector_similarity)
    test_transaction_ranks = generate_transaction_ranks(test_df)

    with pd.ExcelWriter('analysis_outputs.xlsx', engine='openpyxl') as writer:
        train_hot_encoding.to_excel(writer, sheet_name='Train Hot Encoding')
        test_hot_encoding.to_excel(writer, sheet_name='Test Hot Encoding')
        test_recommendations.to_excel(writer, sheet_name='Test Recommendations')
        test_transaction_ranks.to_excel(writer, sheet_name='Test Transaction Ranks')

    print("All requested outputs have been exported to 'analysis_outputs.xlsx'.")

if __name__ == "__main__":
    main()







import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

def load_and_preprocess_data(filepath):
    try:
        df = pd.read_csv(filepath)
        df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')
        df.dropna(inplace=True)
        return df
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

def generate_interaction_matrix(df, value='TRAN_AMT', aggfunc='sum'):
    return df.pivot_table(index='ACCT', columns='Sector', values=value, aggfunc=aggfunc, fill_value=0)

def scale_interaction_matrix(interaction_matrix, axis=0):
    scaler = MinMaxScaler()
    scaled_matrix = scaler.fit_transform(interaction_matrix) if axis == 0 else scaler.fit_transform(interaction_matrix.T).T
    return pd.DataFrame(scaled_matrix, index=interaction_matrix.index, columns=interaction_matrix.columns)

def calculate_cosine_similarity(matrix):
    return cosine_similarity(matrix), cosine_similarity(matrix.T)

def generate_hot_encoding(df, columns=None):
    hot_encoded = pd.get_dummies(df, columns=columns, prefix='', prefix_sep='')
    hot_encoded = hot_encoded.groupby('ACCT').max()
    return hot_encoded

def recommend_sectors(account_sector_matrix, sector_similarity):
    scores = np.dot(account_sector_matrix, sector_similarity)
    ranks = (-scores).argsort(axis=1).argsort(axis=1) + 1
    return pd.DataFrame(ranks, index=account_sector_matrix.index, columns=account_sector_matrix.columns)

def calculate_hit_rate(recommended_ranks, actual_interactions):
    hits = sum((recommended_ranks.iloc[:, :5] <= actual_interactions).any(axis=1))
    total = len(recommended_ranks)
    return hits / total if total > 0 else 0

def main():
    df = load_and_preprocess_data('main6.csv')
    split_date = pd.to_datetime("2023-01-01")
    train_df, test_df = df[df['DATE'] < split_date], df[df['DATE'] >= split_date]

    train_matrix = generate_interaction_matrix(train_df)
    test_matrix = generate_interaction_matrix(test_df)

    # Scale along the customer axis for both training and testing datasets
    train_scaled = scale_interaction_matrix(train_matrix, axis=0)
    test_scaled = scale_interaction_matrix(test_matrix, axis=0)

    # Calculate cosine similarity based on scaled training data
    sector_similarity, _ = calculate_cosine_similarity(train_scaled)

    # Hot encoding not directly used in this refactor but could be included for exploratory data analysis
    train_hot_encoding = generate_hot_encoding(train_df, ['Sector'])
    test_hot_encoding = generate_hot_encoding(test_df, ['Sector'])

    test_recommendations = recommend_sectors(test_scaled, sector_similarity)

    hit_rate = calculate_hit_rate(test_recommendations, test_hot_encoding)
    print(f"Hit Rate: {hit_rate:.2f}")

    with pd.ExcelWriter('analysis_outputs.xlsx', engine='openpyxl') as writer:
        train_hot_encoding.to_excel(writer, sheet_name='Train Hot Encoding')
        test_hot_encoding.to_excel(writer, sheet_name='Test Hot Encoding')
        test_recommendations.to_excel(writer, sheet_name='Test Recommendations')

    print("Exported to 'analysis_outputs.xlsx'.")

if __name__ == "__main__":
    main()











import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
import openpyxl  # Ensure openpyxl is installed for Excel export

# Load the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])

# Splitting the dataset into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Creating and normalizing interaction matrices
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

scaler = MinMaxScaler()
interaction_counts_normalized = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized = scaler.fit_transform(interaction_amounts_train)

combined_interaction = (interaction_counts_normalized + interaction_amounts_normalized) / 2
sectors = interaction_counts_train.columns.tolist()

# Calculate the sector-to-sector similarity matrix
sector_similarity = cosine_similarity(combined_interaction.T)

# Generate recommendations
def generate_recommendations(interaction_matrix, sector_similarity, sectors, top_n=5):
    recommendations = {}
    for account, interactions in interaction_matrix.iterrows():
        user_sector_similarity = np.dot(interactions.values, sector_similarity)
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommendations[account] = [sectors[i] for i in top_sector_indices]
    return recommendations

recommendations = generate_recommendations(pd.DataFrame(combined_interaction, index=interaction_counts_train.index, columns=sectors), sector_similarity, sectors)

# Custom functions for calculating precision, recall, coverage, and NDCG are assumed to be defined here as shown in previous guidance

# Calculating metrics
precision_at_5, recall_at_5 = calculate_precision_recall(recommendations, test_df, k=5)
coverage = calculate_coverage(recommendations, sectors)
ndcg_at_5 = calculate_ndcg(recommendations, test_df, k=5)

# Exporting the results
with pd.ExcelWriter('recommendation_results.xlsx', engine='openpyxl') as writer:
    # Recommendations
    pd.DataFrame.from_dict(recommendations, orient='index').to_excel(writer, sheet_name='Recommendations')
    # Metrics
    metrics_df = pd.DataFrame({
        'Metric': ['Precision@5', 'Recall@5', 'Coverage', 'NDCG@5'],
        'Value': [precision_at_5, recall_at_5, coverage, ndcg_at_5]
    })
    metrics_df.to_excel(writer, sheet_name='Evaluation Metrics')
    # Additional exports like interaction matrices can be added here

print("Analysis and recommendations exported to recommendation_results.xlsx")








import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

# Load and preprocess the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])

# Splitting dataset into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Export train and test datasets
train_df.to_csv('train_data.csv', index=False)
test_df.to_csv('test_data.csv', index=False)

# Creating and normalizing interaction matrices for counts and amounts
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
scaler = MinMaxScaler()
interaction_counts_normalized_train = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized_train = scaler.fit_transform(interaction_amounts_train)

# Combine normalized matrices with equal weight for each factor
combined_interaction_train = (interaction_counts_normalized_train + interaction_amounts_normalized_train) / 2
sectors = interaction_counts_train.columns

# Calculate the sector-to-sector similarity matrix from the combined interaction matrix
sector_similarity = cosine_similarity(combined_interaction_train.T)

# Generate recommendations for all customers
def generate_recommendations_for_all(interaction_matrix, sector_similarity_matrix, sectors, top_n=5):
    recommendations = {}
    sector_names = sectors.tolist()  # Convert sector names to a list for direct indexing
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[account].values.reshape(1, -1)
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommended_sectors = [sector_names[i] for i in top_sector_indices]
        recommendations[account] = recommended_sectors
    return recommendations

combined_interaction_df_train = pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, columns=sectors)
all_recommendations = generate_recommendations_for_all(combined_interaction_df_train, sector_similarity, sectors)

# Sector-wise analysis for recommendation accuracy
def sector_wise_analysis(recommendations, test_transactions, sectors):
    sector_analysis = {sector: {'total_recommendations': 0, 'successful_matches': 0} for sector in sectors}
    
    for account, recommended_sectors in recommendations.items():
        actual_sectors = test_transactions[test_transactions['ACCT'] == account]['Sector'].tolist()
        for recommended_sector in recommended_sectors:
            sector_analysis[recommended_sector]['total_recommendations'] += 1
            if recommended_sector in actual_sectors:
                sector_analysis[recommended_sector]['successful_matches'] += 1

    return sector_analysis

sector_analysis_results = sector_wise_analysis(all_recommendations, test_df, sectors)

# Exporting results and sector-wise analysis
with pd.ExcelWriter('sector_recommendations_and_matrices.xlsx', engine='openpyxl') as writer:
    pd.DataFrame.from_dict(all_recommendations, orient='index', 
                           columns=[f'Recommendation {i+1}' for i in range(5)]).to_excel(writer, sheet_name='Recommendations')
    pd.DataFrame(interaction_counts_normalized_train, index=interaction_counts_train.index, 
                 columns=sectors).to_excel(writer, sheet_name='Interaction Counts')
    pd.DataFrame(interaction_amounts_normalized_train, index=interaction_amounts_train.index, 
                 columns=sectors).to_excel(writer, sheet_name='Interaction Amounts')
    pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, 
                 columns=sectors).to_excel(writer, sheet_name='Combined Interaction')
    pd.DataFrame(sector_similarity, index=sectors, columns=sectors).to_excel(writer, sheet_name='Sector Similarity')
    # Adding sector-wise analysis sheet
    sector_analysis_df = pd.DataFrame(sector_analysis_results).transpose()
    sector_analysis_df.to_excel(writer, sheet_name='Sector-wise Analysis')

print("Exported all data and recommendations, including sector-wise analysis, to 'sector_recommendations_and_matrices.xlsx'.")








import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

# Load and preprocess the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])

# Splitting dataset into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Export train and test datasets
train_df.to_csv('train_data.csv', index=False)
test_df.to_csv('test_data.csv', index=False)

# Creating and normalizing interaction matrices for counts and amounts
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
scaler = MinMaxScaler()
interaction_counts_normalized_train = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized_train = scaler.fit_transform(interaction_amounts_train)

# Combine normalized matrices with equal weight for each factor
combined_interaction_train = (interaction_counts_normalized_train + interaction_amounts_normalized_train) / 2
sectors = interaction_counts_train.columns

# Calculate the sector-to-sector similarity matrix from the combined interaction matrix
sector_similarity = cosine_similarity(combined_interaction_train.T)

# Generate recommendations for all customers
def generate_recommendations_for_all(interaction_matrix, sector_similarity_matrix, sectors, top_n=5):
    recommendations = {}
    sector_names = sectors.tolist()  # Convert sector names to a list for direct indexing
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[account].values.reshape(1, -1)
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommended_sectors = [sector_names[i] for i in top_sector_indices]
        recommendations[account] = recommended_sectors
    return recommendations

combined_interaction_df_train = pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, columns=sectors)
all_recommendations = generate_recommendations_for_all(combined_interaction_df_train, sector_similarity, sectors)

# Define the custom rank-based accuracy function
def custom_rank_based_accuracy(recommendations, test_transactions):
    scores = []
    for account, recommended_sectors in recommendations.items():
        actual_sectors = test_transactions[test_transactions['ACCT'] == account]['Sector'].unique()
        score = sum(1 for sector in recommended_sectors if sector in actual_sectors) / len(recommended_sectors) if recommended_sectors else 0
        scores.append(score)
    average_score = np.mean(scores) if scores else 0
    return average_score

# Calculate the custom rank-based accuracy
accuracy_score = custom_rank_based_accuracy(all_recommendations, test_df)
print(f"Custom Rank-Based Accuracy Score: {accuracy_score:.4f}")

# Exporting results
with pd.ExcelWriter('sector_recommendations_and_matrices.xlsx', engine='openpyxl') as writer:
    pd.DataFrame.from_dict(all_recommendations, orient='index', columns=[f'Recommendation {i+1}' for i in range(5)]).to_excel(writer, sheet_name='Recommendations')
    pd.DataFrame(interaction_counts_normalized_train, index=interaction_counts_train.index, columns=sectors).to_excel(writer, sheet_name='Interaction Counts')
    pd.DataFrame(interaction_amounts_normalized_train, index=interaction_amounts_train.index, columns=sectors).to_excel(writer, sheet_name='Interaction Amounts')
    pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, columns=sectors).to_excel(writer, sheet_name='Combined Interaction')
    pd.DataFrame(sector_similarity, index=sectors, columns=sectors).to_excel(writer, sheet_name='Sector Similarity')

print("Exported all data and recommendations to 'sector_recommendations_and_matrices.xlsx'.")











def calculate_accuracy(recommendations, test_data):
    correct_predictions = 0
    for account, recommended_sectors in recommendations.items():
        # Extract the sectors where the customer actually made transactions in the test period
        actual_sectors = test_data.loc[test_data['ACCT'] == account, 'Sector'].unique()
        # Check if any recommended sector matches the actual sectors
        if any(sector in actual_sectors for sector in recommended_sectors):
            correct_predictions += 1
    # Calculate accuracy as the ratio of correct predictions to total predictions
    accuracy = correct_predictions / len(recommendations) if recommendations else 0
    return accuracy

# Calculate the accuracy for the recommendations based on the test data
accuracy_score = calculate_accuracy(all_recommendations, test_df)
print(f"Accuracy Score: {accuracy_score:.4f}")







import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
import openpyxl  # Ensure openpyxl is installed for Excel export

# Load the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])  # Convert 'DATE' to datetime if needed

# Time-based split for training and testing
split_date = pd.Timestamp('2023-01-01')  # Adjust based on your dataset
train_df = df[df['DATE'] < split_date]
test_df = df[df['DATE'] >= split_date]

# Creating Interaction Matrices for Training Data (Counts and Amounts)
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Normalize the interaction matrices
scaler = MinMaxScaler()
interaction_counts_normalized = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized = scaler.fit_transform(interaction_amounts_train)

# Combine normalized matrices with equal weight for demonstration purposes
combined_interaction = (interaction_counts_normalized + interaction_amounts_normalized) / 2

# Calculate sector-to-sector similarity matrix from the combined interaction matrix
sector_similarity = cosine_similarity(combined_interaction.T)
sector_similarity_df = pd.DataFrame(sector_similarity, index=interaction_counts_train.columns, columns=interaction_counts_train.columns)

# Function to generate recommendations for all customers based on sector similarity
def generate_recommendations_for_all(interaction_matrix, sector_similarity_matrix, sectors, top_n=5):
    recommendations = {}
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[[account]].values
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommended_sectors = sectors[top_sector_indices].tolist()
        recommendations[account] = recommended_sectors
    return recommendations

# Generate recommendations for all customers
interaction_matrix_df = pd.DataFrame(combined_interaction, index=interaction_counts_train.index, columns=interaction_counts_train.columns)
sectors = interaction_counts_train.columns
all_recommendations = generate_recommendations_for_all(interaction_matrix_df, sector_similarity, sectors)

# Convert recommendations to DataFrame for export
recommendations_df = pd.DataFrame.from_dict(all_recommendations, orient='index', columns=[f'Top {i+1}' for i in range(5)])

# Prepare summary of actual amounts spent by each customer in each sector during the test period
actual_spending_test_period = test_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Initialize a DataFrame to store the comparison of actual spending in recommended sectors
comparison_df = pd.DataFrame(columns=['ACCT', 'Recommended Sectors', 'Actual Spending in Recommended Sectors'])

for acct, recommended_sectors in all_recommendations.items():
    if acct in actual_spending_test_period.index:
        actual_spending = actual_spending_test_period.loc[acct, recommended_sectors].sum()
    else:
        actual_spending = 0
    comparison_df = comparison_df.append({
        'ACCT': acct,
        'Recommended Sectors': ', '.join(recommended_sectors),
        'Actual Spending in Recommended Sectors': actual_spending
    }, ignore_index=True)

# Export to Excel with separate sheets
with pd.ExcelWriter('recommendations_and_matrices.xlsx', engine='openpyxl') as writer:
    recommendations_df.to_excel(writer, sheet_name='Customer Recommendations')
    interaction_counts_train.to_excel(writer, sheet_name='Interaction Counts Train')
    interaction_amounts_train.to_excel(writer, sheet_name='Interaction Amounts Train')
    sector_similarity_df.to_excel(writer, sheet_name='Sector Similarity')
    comparison_df.to_excel(writer, sheet_name='Spending Comparison')

print("Exported all data to 'recommendations_and_matrices.xlsx'.")




import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
import openpyxl  # Ensure openpyxl is installed for Excel export

# Load the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])  # Convert 'DATE' to datetime format

# Time-based split for training and testing
split_date = pd.Timestamp('2023-01-01')  # Adjust based on your dataset
train_df = df[df['DATE'] < split_date]
test_df = df[df['DATE'] >= split_date]

# Creating Interaction Matrices for Training Data (Counts and Amounts)
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Normalize the interaction matrices
scaler = MinMaxScaler()
interaction_counts_normalized = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized = scaler.fit_transform(interaction_amounts_train)

# Combine normalized matrices with equal weight for demonstration purposes
combined_interaction = (interaction_counts_normalized + interaction_amounts_normalized) / 2

# Calculate sector-to-sector similarity matrix from the combined interaction matrix
sector_similarity = cosine_similarity(combined_interaction.T)
sector_similarity_df = pd.DataFrame(sector_similarity, index=interaction_counts_train.columns, columns=interaction_counts_train.columns)

# Function to generate recommendations for all customers based on sector similarity
def generate_recommendations_for_all(interaction_matrix, sector_similarity_matrix, sectors, top_n=5):
    recommendations = {}
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[[account]].values
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommended_sectors = sectors[top_sector_indices].tolist()
        recommendations[account] = recommended_sectors
    return recommendations

# Generate recommendations for all customers
interaction_matrix_df = pd.DataFrame(combined_interaction, index=interaction_counts_train.index, columns=interaction_counts_train.columns)
all_recommendations = generate_recommendations_for_all(interaction_matrix_df, sector_similarity, interaction_counts_train.columns)

# Prepare summary of actual amounts spent by each customer in each sector during the test period
actual_spending_test_period = test_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Initialize a DataFrame to store the comparison of actual spending in recommended sectors
comparison_df = pd.DataFrame(columns=['ACCT', 'Recommended Sectors', 'Actual Spending in Recommended Sectors'])

for acct, recommended_sectors in all_recommendations.items():
    if acct in actual_spending_test_period.index:
        actual_spending = actual_spending_test_period.loc[acct, recommended_sectors].sum()
    else:
        actual_spending = 0
    comparison_df = comparison_df.append({
        'ACCT': acct,
        'Recommended Sectors': ', '.join(recommended_sectors),
        'Actual Spending in Recommended Sectors': actual_spending
    }, ignore_index=True)

# Export to Excel with separate sheets
with pd.ExcelWriter('recommendations_and_matrices.xlsx', engine='openpyxl') as writer:
    recommendations_df.to_excel(writer, sheet_name='Customer Recommendations')
    interaction_counts_train.to_excel(writer, sheet_name='Interaction Counts Train')
    interaction_amounts_train.to_excel(writer, sheet_name='Interaction Amounts Train')
    sector_similarity_df.to_excel(writer, sheet_name='Sector Similarity')
    comparison_df.to_excel(writer, sheet_name='Spending Comparison')

print("Exported all data to 'recommendations_and_matrices.xlsx'.")










import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
import openpyxl

# Ensure this package is installed for Excel export
# pip install openpyxl

# Load the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])

# Define your split date
split_date = pd.Timestamp('2023-01-01')
train_df = df[df['DATE'] < split_date]
test_df = df[df['DATE'] >= split_date]

# Function to create and normalize interaction matrices
def create_normalized_matrices(dataframe):
    interaction_counts = dataframe.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
    interaction_amounts = dataframe.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
    scaler = MinMaxScaler()
    counts_normalized = scaler.fit_transform(interaction_counts)
    amounts_normalized = scaler.fit_transform(interaction_amounts)
    combined_normalized = (counts_normalized + amounts_normalized) / 2
    return pd.DataFrame(combined_normalized, index=interaction_counts.index, columns=interaction_counts.columns), interaction_counts.columns

# Calculate sector-to-sector similarity matrix
def calculate_similarity_matrix(interaction_matrix):
    return cosine_similarity(interaction_matrix.T)

# Generate recommendations based on sector similarity
def generate_recommendations_for_all(interaction_matrix, sector_similarity, sectors, top_n=5):
    recommendations = {}
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[account].values.reshape(1, -1)
        similarity_scores = np.dot(user_vector, sector_similarity)[0]
        top_indices = np.argsort(-similarity_scores)[:top_n]
        recommendations[account] = sectors[top_indices].tolist()
    return recommendations

# Evaluate recommendations against the test set
def evaluate_recommendations(recommendations, test_data, sectors):
    hits = 0
    total = 0
    for account, recommended_sectors in recommendations.items():
        actual_sectors = test_data[test_data['ACCT'] == account]['Sector'].unique()
        hit = set(recommended_sectors) & set(actual_sectors)
        hits += len(hit)
        total += len(recommended_sectors)
    average_precision = hits / total if total else 0
    return average_precision

# Preparing matrices and similarity calculation
interaction_matrix_train, sectors = create_normalized_matrices(train_df)
sector_similarity_matrix = calculate_similarity_matrix(interaction_matrix_train)

# Generate recommendations for all customers based on training data
all_recommendations = generate_recommendations_for_all(interaction_matrix_train, sector_similarity_matrix, sectors)

# Evaluate the recommendations using the test dataset
avg_precision = evaluate_recommendations(all_recommendations, test_df, sectors)

# Export to Excel with separate sheets
with pd.ExcelWriter('recommendations_evaluation.xlsx', engine='openpyxl') as writer:
    pd.DataFrame(all_recommendations).to_excel(writer, sheet_name='Recommendations')
    pd.DataFrame(interaction_matrix_train, index=interaction_matrix_train.index, columns=sectors).to_excel(writer, sheet_name='Interaction Matrix')
    pd.DataFrame(sector_similarity_matrix, index=sectors, columns=sectors).to_excel(writer, sheet_name='Sector Similarity')
    pd.DataFrame({'Average Precision': [avg_precision]}).to_excel(writer, sheet_name='Evaluation Metrics')

print(f"Exported all data to 'recommendations_evaluation.xlsx'. Average Precision: {avg_precision}")







import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming you've already loaded your DataFrame 'df' from "IF_MAIN6.csv"

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Function to create pivot tables for counts and amounts
def create_pivot_tables(dataframe, aggfunc):
    return dataframe.pivot_table(
        index='ACCT',
        columns='Sector',
        values='TRAN_AMT',
        aggfunc=aggfunc,
        fill_value=0
    )

# Creating pivot tables
pivot_counts = create_pivot_tables(df, 'count')
pivot_amounts = create_pivot_tables(df, 'sum')
pivot_counts_weekday = create_pivot_tables(df[df['Day_Type'] == 'Weekday'], 'count')
pivot_counts_weekend = create_pivot_tables(df[df['Day_Type'] == 'Weekend'], 'count')
pivot_amounts_weekday = create_pivot_tables(df[df['Day_Type'] == 'Weekday'], 'sum')
pivot_amounts_weekend = create_pivot_tables(df[df['Day_Type'] == 'Weekend'], 'sum')

# Normalize data
def normalize_data(data):
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)
    return pd.DataFrame(scaled_data, index=data.index, columns=data.columns)

# Normalizing all pivot tables
norm_counts = normalize_data(pivot_counts)
norm_amounts = normalize_data(pivot_amounts)
norm_counts_weekday = normalize_data(pivot_counts_weekday)
norm_counts_weekend = normalize_data(pivot_counts_weekend)
norm_amounts_weekday = normalize_data(pivot_amounts_weekday)
norm_amounts_weekend = normalize_data(pivot_amounts_weekend)

# Function to calculate cosine similarity
def calculate_cosine_similarity(data):
    return cosine_similarity(data)

# Function to find top sectors
def find_top_sectors(customer_id, counts_data, amounts_data, n_top=5):
    similarity_matrix = calculate_cosine_similarity(counts_data + amounts_data)
    customer_index = counts_data.index.get_loc(customer_id)
    customer_similarities = similarity_matrix[customer_index]
    
    # Predicting interests based on similarities
    weighted_counts = np.dot(customer_similarities, counts_data)
    weighted_amounts = np.dot(customer_similarities, amounts_data)
    
    # Combining count and amount predictions
    combined_scores = (weighted_counts + weighted_amounts) / 2
    top_sector_indices = combined_scores.argsort()[::-1][:n_top]
    
    top_sectors = [counts_data.columns[i] for i in top_sector_indices]
    return top_sectors

# Example usage
customer_id = df['ACCT'].iloc[0]  # Replace with a specific customer ID as needed

# Overall top sectors
top_sectors_overall = find_top_sectors(customer_id, norm_counts, norm_amounts, n_top=5)
print(f"Customer {customer_id} Top 5 Sectors Overall: {top_sectors_overall}")

# Weekday and Weekend dominant sector
weekday_sector = find_top_sectors(customer_id, norm_counts_weekday, norm_amounts_weekday, n_top=1)
weekend_sector = find_top_sectors(customer_id, norm_counts_weekend, norm_amounts_weekend, n_top=1)
print(f"Customer {customer_id} Dominant Sector Weekday: {weekday_sector}")
print(f"Customer {customer_id} Dominant Sector Weekend: {weekend_sector}")










import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import coo_matrix
from implicit.bpr import BayesianPersonalizedRanking

# Assuming 'df' is your DataFrame loaded with transaction data
# Sample preprocessing steps
df['Date'] = pd.to_datetime(df['Date'])
current_date = pd.to_datetime('2024-03-03')
df['Recency'] = (current_date - df['Date']).dt.days
df.sort_values('Date', inplace=True)

# Encoding categorical data
acct_encoder = LabelEncoder()
df['ACCT'] = acct_encoder.fit_transform(df['ACCT'])

sector_encoder = LabelEncoder()
df['Sector'] = sector_encoder.fit_transform(df['Sector'])

# Feature Engineering: Simplify for demonstration
features = df.groupby(['ACCT', 'Sector']).agg(
    Total_Spend=('TRAN_AMT', 'sum'),
    Transaction_Count=('TRAN_AMT', 'count'),
    Recency=('Recency', 'min')
).reset_index()

# Normalizing features
features['Total_Spend'] = (features['Total_Spend'] - features['Total_Spend'].min()) / (features['Total_Spend'].max() - features['Total_Spend'].min())
features['Transaction_Count'] = (features['Transaction_Count'] - features['Transaction_Count'].min()) / (features['Transaction_Count'].max() - features['Transaction_Count'].min())
features['Recency'] = (features['Recency'] - features['Recency'].min()) / (features['Recency'].max() - features['Recency'].min())

# Interaction matrix with normalized features
interaction_values = features['Total_Spend'] + features['Transaction_Count'] - features['Recency']
interaction_matrix = coo_matrix((interaction_values, (features['ACCT'], features['Sector'])),
                                shape=(df['ACCT'].nunique(), df['Sector'].nunique())).tocsr()

# BPR Model Training
model = BayesianPersonalizedRanking(iterations=100, learning_rate=0.01, regularization=0.01)
model.fit(interaction_matrix.T)

# Mapping ACCT values to their corresponding model indices
acct_to_model_idx = {acct: idx for idx, acct in enumerate(acct_encoder.classes_)}

# Function to generate recommendations using ACCT value directly
def get_recommendations_for_acct(acct_value):
    if acct_value in acct_to_model_idx:
        user_idx = acct_to_model_idx[acct_value]
        scores = model.user_factors[user_idx] @ model.item_factors.T
        recommended_sector_indices = np.argsort(scores)[::-1][:5]  # Top 5 recommendations
        recommended_sectors = sector_encoder.inverse_transform(recommended_sector_indices)
        print(f"Recommended Sectors for Account {acct_value}: {recommended_sectors}")
    else:
        print(f"Account {acct_value} not found.")

# Example usage
# Replace 'some_acct_value' with an actual ACCT value from your dataset
get_recommendations_for_acct(some_acct_value)









import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import coo_matrix
from implicit.bpr import BayesianPersonalizedRanking

# Assuming 'df' is preloaded with your data

# Preprocessing
df['Date'] = pd.to_datetime(df['Date'])
df['Weekday'] = df['Date'].dt.weekday  # 0=Monday, 6=Sunday
df['IsWeekend'] = df['Weekday'] >= 5  # True for weekends
current_date = pd.to_datetime('2024-03-03')
df['Recency'] = (current_date - df['Date']).dt.days

# Encoding
acct_encoder = LabelEncoder()
df['ACCT'] = acct_encoder.fit_transform(df['ACCT'])

sector_encoder = LabelEncoder()
df['Sector'] = sector_encoder.fit_transform(df['Sector'])

occupation_encoder = LabelEncoder()
df['Occupation'] = occupation_encoder.fit_transform(df['Occupation'])

# Feature Engineering: Simplify for demonstration
# Aggregating transaction counts, including weekend flag
agg_funcs = {'TRAN_AMT': 'count', 'IsWeekend': 'mean', 'Recency': 'min'}
features = df.groupby(['ACCT', 'Sector']).agg(agg_funcs).reset_index()

# Normalizing features
features['TRAN_AMT'] = (features['TRAN_AMT'] - features['TRAN_AMT'].min()) / (features['TRAN_AMT'].max() - features['TRAN_AMT'].min())
features['Recency'] = (features['Recency'] - features['Recency'].min()) / (features['Recency'].max() - features['Recency'].min())

# Interaction matrix with transaction count as the basis
interaction_values = features['TRAN_AMT'] - features['Recency']  # Simplified interaction value
interaction_matrix = coo_matrix((interaction_values, 
                                 (features['ACCT'], features['Sector'])),
                                shape=(df['ACCT'].nunique(), df['Sector'].nunique()))

# BPR Model Training
model = BayesianPersonalizedRanking(iterations=100, learning_rate=0.01, lambda_reg=0.01)
model.fit(interaction_matrix.T)

# Predicting and Ranking Sectors for a Specific User
user_id = 0  # Placeholder for an example user ID
scores = model.user_factors[user_id] @ model.item_factors.T
recommended_sector_ids = np.argsort(scores)[::-1]
recommended_sectors = sector_encoder.inverse_transform(recommended_sector_ids)

print(f"Recommended Sectors for User {acct_encoder.inverse_transform([user_id])[0]}: {recommended_sectors[:5]}")

# Analyzing Weekday vs. Weekend Preferences
# This could involve generating separate recommendations based on transactions flagged as weekend or not.
# For a detailed analysis, consider splitting the dataset by 'IsWeekend' and repeating the modeling process for each subset.
