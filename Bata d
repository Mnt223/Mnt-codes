import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
import openpyxl  # Ensure openpyxl is installed for Excel export

# Load the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])  # Convert 'DATE' to datetime format

# Time-based split for training and testing
split_date = pd.Timestamp('2023-01-01')  # Adjust based on your dataset
train_df = df[df['DATE'] < split_date]
test_df = df[df['DATE'] >= split_date]

# Creating Interaction Matrices for Training Data (Counts and Amounts)
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Normalize the interaction matrices
scaler = MinMaxScaler()
interaction_counts_normalized = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized = scaler.fit_transform(interaction_amounts_train)

# Combine normalized matrices with equal weight for demonstration purposes
combined_interaction = (interaction_counts_normalized + interaction_amounts_normalized) / 2

# Calculate sector-to-sector similarity matrix from the combined interaction matrix
sector_similarity = cosine_similarity(combined_interaction.T)
sector_similarity_df = pd.DataFrame(sector_similarity, index=interaction_counts_train.columns, columns=interaction_counts_train.columns)

# Function to generate recommendations for all customers based on sector similarity
def generate_recommendations_for_all(interaction_matrix, sector_similarity_matrix, sectors, top_n=5):
    recommendations = {}
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[[account]].values
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommended_sectors = sectors[top_sector_indices].tolist()
        recommendations[account] = recommended_sectors
    return recommendations

# Generate recommendations for all customers
interaction_matrix_df = pd.DataFrame(combined_interaction, index=interaction_counts_train.index, columns=interaction_counts_train.columns)
all_recommendations = generate_recommendations_for_all(interaction_matrix_df, sector_similarity, interaction_counts_train.columns)

# Prepare summary of actual amounts spent by each customer in each sector during the test period
actual_spending_test_period = test_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Initialize a DataFrame to store the comparison of actual spending in recommended sectors
comparison_df = pd.DataFrame(columns=['ACCT', 'Recommended Sectors', 'Actual Spending in Recommended Sectors'])

for acct, recommended_sectors in all_recommendations.items():
    if acct in actual_spending_test_period.index:
        actual_spending = actual_spending_test_period.loc[acct, recommended_sectors].sum()
    else:
        actual_spending = 0
    comparison_df = comparison_df.append({
        'ACCT': acct,
        'Recommended Sectors': ', '.join(recommended_sectors),
        'Actual Spending in Recommended Sectors': actual_spending
    }, ignore_index=True)

# Export to Excel with separate sheets
with pd.ExcelWriter('recommendations_and_matrices.xlsx', engine='openpyxl') as writer:
    recommendations_df.to_excel(writer, sheet_name='Customer Recommendations')
    interaction_counts_train.to_excel(writer, sheet_name='Interaction Counts Train')
    interaction_amounts_train.to_excel(writer, sheet_name='Interaction Amounts Train')
    sector_similarity_df.to_excel(writer, sheet_name='Sector Similarity')
    comparison_df.to_excel(writer, sheet_name='Spending Comparison')

print("Exported all data to 'recommendations_and_matrices.xlsx'.")










import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
import openpyxl

# Ensure this package is installed for Excel export
# pip install openpyxl

# Load the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])

# Define your split date
split_date = pd.Timestamp('2023-01-01')
train_df = df[df['DATE'] < split_date]
test_df = df[df['DATE'] >= split_date]

# Function to create and normalize interaction matrices
def create_normalized_matrices(dataframe):
    interaction_counts = dataframe.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
    interaction_amounts = dataframe.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
    scaler = MinMaxScaler()
    counts_normalized = scaler.fit_transform(interaction_counts)
    amounts_normalized = scaler.fit_transform(interaction_amounts)
    combined_normalized = (counts_normalized + amounts_normalized) / 2
    return pd.DataFrame(combined_normalized, index=interaction_counts.index, columns=interaction_counts.columns), interaction_counts.columns

# Calculate sector-to-sector similarity matrix
def calculate_similarity_matrix(interaction_matrix):
    return cosine_similarity(interaction_matrix.T)

# Generate recommendations based on sector similarity
def generate_recommendations_for_all(interaction_matrix, sector_similarity, sectors, top_n=5):
    recommendations = {}
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[account].values.reshape(1, -1)
        similarity_scores = np.dot(user_vector, sector_similarity)[0]
        top_indices = np.argsort(-similarity_scores)[:top_n]
        recommendations[account] = sectors[top_indices].tolist()
    return recommendations

# Evaluate recommendations against the test set
def evaluate_recommendations(recommendations, test_data, sectors):
    hits = 0
    total = 0
    for account, recommended_sectors in recommendations.items():
        actual_sectors = test_data[test_data['ACCT'] == account]['Sector'].unique()
        hit = set(recommended_sectors) & set(actual_sectors)
        hits += len(hit)
        total += len(recommended_sectors)
    average_precision = hits / total if total else 0
    return average_precision

# Preparing matrices and similarity calculation
interaction_matrix_train, sectors = create_normalized_matrices(train_df)
sector_similarity_matrix = calculate_similarity_matrix(interaction_matrix_train)

# Generate recommendations for all customers based on training data
all_recommendations = generate_recommendations_for_all(interaction_matrix_train, sector_similarity_matrix, sectors)

# Evaluate the recommendations using the test dataset
avg_precision = evaluate_recommendations(all_recommendations, test_df, sectors)

# Export to Excel with separate sheets
with pd.ExcelWriter('recommendations_evaluation.xlsx', engine='openpyxl') as writer:
    pd.DataFrame(all_recommendations).to_excel(writer, sheet_name='Recommendations')
    pd.DataFrame(interaction_matrix_train, index=interaction_matrix_train.index, columns=sectors).to_excel(writer, sheet_name='Interaction Matrix')
    pd.DataFrame(sector_similarity_matrix, index=sectors, columns=sectors).to_excel(writer, sheet_name='Sector Similarity')
    pd.DataFrame({'Average Precision': [avg_precision]}).to_excel(writer, sheet_name='Evaluation Metrics')

print(f"Exported all data to 'recommendations_evaluation.xlsx'. Average Precision: {avg_precision}")







import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming you've already loaded your DataFrame 'df' from "IF_MAIN6.csv"

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Function to create pivot tables for counts and amounts
def create_pivot_tables(dataframe, aggfunc):
    return dataframe.pivot_table(
        index='ACCT',
        columns='Sector',
        values='TRAN_AMT',
        aggfunc=aggfunc,
        fill_value=0
    )

# Creating pivot tables
pivot_counts = create_pivot_tables(df, 'count')
pivot_amounts = create_pivot_tables(df, 'sum')
pivot_counts_weekday = create_pivot_tables(df[df['Day_Type'] == 'Weekday'], 'count')
pivot_counts_weekend = create_pivot_tables(df[df['Day_Type'] == 'Weekend'], 'count')
pivot_amounts_weekday = create_pivot_tables(df[df['Day_Type'] == 'Weekday'], 'sum')
pivot_amounts_weekend = create_pivot_tables(df[df['Day_Type'] == 'Weekend'], 'sum')

# Normalize data
def normalize_data(data):
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)
    return pd.DataFrame(scaled_data, index=data.index, columns=data.columns)

# Normalizing all pivot tables
norm_counts = normalize_data(pivot_counts)
norm_amounts = normalize_data(pivot_amounts)
norm_counts_weekday = normalize_data(pivot_counts_weekday)
norm_counts_weekend = normalize_data(pivot_counts_weekend)
norm_amounts_weekday = normalize_data(pivot_amounts_weekday)
norm_amounts_weekend = normalize_data(pivot_amounts_weekend)

# Function to calculate cosine similarity
def calculate_cosine_similarity(data):
    return cosine_similarity(data)

# Function to find top sectors
def find_top_sectors(customer_id, counts_data, amounts_data, n_top=5):
    similarity_matrix = calculate_cosine_similarity(counts_data + amounts_data)
    customer_index = counts_data.index.get_loc(customer_id)
    customer_similarities = similarity_matrix[customer_index]
    
    # Predicting interests based on similarities
    weighted_counts = np.dot(customer_similarities, counts_data)
    weighted_amounts = np.dot(customer_similarities, amounts_data)
    
    # Combining count and amount predictions
    combined_scores = (weighted_counts + weighted_amounts) / 2
    top_sector_indices = combined_scores.argsort()[::-1][:n_top]
    
    top_sectors = [counts_data.columns[i] for i in top_sector_indices]
    return top_sectors

# Example usage
customer_id = df['ACCT'].iloc[0]  # Replace with a specific customer ID as needed

# Overall top sectors
top_sectors_overall = find_top_sectors(customer_id, norm_counts, norm_amounts, n_top=5)
print(f"Customer {customer_id} Top 5 Sectors Overall: {top_sectors_overall}")

# Weekday and Weekend dominant sector
weekday_sector = find_top_sectors(customer_id, norm_counts_weekday, norm_amounts_weekday, n_top=1)
weekend_sector = find_top_sectors(customer_id, norm_counts_weekend, norm_amounts_weekend, n_top=1)
print(f"Customer {customer_id} Dominant Sector Weekday: {weekday_sector}")
print(f"Customer {customer_id} Dominant Sector Weekend: {weekend_sector}")










import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import coo_matrix
from implicit.bpr import BayesianPersonalizedRanking

# Assuming 'df' is your DataFrame loaded with transaction data
# Sample preprocessing steps
df['Date'] = pd.to_datetime(df['Date'])
current_date = pd.to_datetime('2024-03-03')
df['Recency'] = (current_date - df['Date']).dt.days
df.sort_values('Date', inplace=True)

# Encoding categorical data
acct_encoder = LabelEncoder()
df['ACCT'] = acct_encoder.fit_transform(df['ACCT'])

sector_encoder = LabelEncoder()
df['Sector'] = sector_encoder.fit_transform(df['Sector'])

# Feature Engineering: Simplify for demonstration
features = df.groupby(['ACCT', 'Sector']).agg(
    Total_Spend=('TRAN_AMT', 'sum'),
    Transaction_Count=('TRAN_AMT', 'count'),
    Recency=('Recency', 'min')
).reset_index()

# Normalizing features
features['Total_Spend'] = (features['Total_Spend'] - features['Total_Spend'].min()) / (features['Total_Spend'].max() - features['Total_Spend'].min())
features['Transaction_Count'] = (features['Transaction_Count'] - features['Transaction_Count'].min()) / (features['Transaction_Count'].max() - features['Transaction_Count'].min())
features['Recency'] = (features['Recency'] - features['Recency'].min()) / (features['Recency'].max() - features['Recency'].min())

# Interaction matrix with normalized features
interaction_values = features['Total_Spend'] + features['Transaction_Count'] - features['Recency']
interaction_matrix = coo_matrix((interaction_values, (features['ACCT'], features['Sector'])),
                                shape=(df['ACCT'].nunique(), df['Sector'].nunique())).tocsr()

# BPR Model Training
model = BayesianPersonalizedRanking(iterations=100, learning_rate=0.01, regularization=0.01)
model.fit(interaction_matrix.T)

# Mapping ACCT values to their corresponding model indices
acct_to_model_idx = {acct: idx for idx, acct in enumerate(acct_encoder.classes_)}

# Function to generate recommendations using ACCT value directly
def get_recommendations_for_acct(acct_value):
    if acct_value in acct_to_model_idx:
        user_idx = acct_to_model_idx[acct_value]
        scores = model.user_factors[user_idx] @ model.item_factors.T
        recommended_sector_indices = np.argsort(scores)[::-1][:5]  # Top 5 recommendations
        recommended_sectors = sector_encoder.inverse_transform(recommended_sector_indices)
        print(f"Recommended Sectors for Account {acct_value}: {recommended_sectors}")
    else:
        print(f"Account {acct_value} not found.")

# Example usage
# Replace 'some_acct_value' with an actual ACCT value from your dataset
get_recommendations_for_acct(some_acct_value)









import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import coo_matrix
from implicit.bpr import BayesianPersonalizedRanking

# Assuming 'df' is preloaded with your data

# Preprocessing
df['Date'] = pd.to_datetime(df['Date'])
df['Weekday'] = df['Date'].dt.weekday  # 0=Monday, 6=Sunday
df['IsWeekend'] = df['Weekday'] >= 5  # True for weekends
current_date = pd.to_datetime('2024-03-03')
df['Recency'] = (current_date - df['Date']).dt.days

# Encoding
acct_encoder = LabelEncoder()
df['ACCT'] = acct_encoder.fit_transform(df['ACCT'])

sector_encoder = LabelEncoder()
df['Sector'] = sector_encoder.fit_transform(df['Sector'])

occupation_encoder = LabelEncoder()
df['Occupation'] = occupation_encoder.fit_transform(df['Occupation'])

# Feature Engineering: Simplify for demonstration
# Aggregating transaction counts, including weekend flag
agg_funcs = {'TRAN_AMT': 'count', 'IsWeekend': 'mean', 'Recency': 'min'}
features = df.groupby(['ACCT', 'Sector']).agg(agg_funcs).reset_index()

# Normalizing features
features['TRAN_AMT'] = (features['TRAN_AMT'] - features['TRAN_AMT'].min()) / (features['TRAN_AMT'].max() - features['TRAN_AMT'].min())
features['Recency'] = (features['Recency'] - features['Recency'].min()) / (features['Recency'].max() - features['Recency'].min())

# Interaction matrix with transaction count as the basis
interaction_values = features['TRAN_AMT'] - features['Recency']  # Simplified interaction value
interaction_matrix = coo_matrix((interaction_values, 
                                 (features['ACCT'], features['Sector'])),
                                shape=(df['ACCT'].nunique(), df['Sector'].nunique()))

# BPR Model Training
model = BayesianPersonalizedRanking(iterations=100, learning_rate=0.01, lambda_reg=0.01)
model.fit(interaction_matrix.T)

# Predicting and Ranking Sectors for a Specific User
user_id = 0  # Placeholder for an example user ID
scores = model.user_factors[user_id] @ model.item_factors.T
recommended_sector_ids = np.argsort(scores)[::-1]
recommended_sectors = sector_encoder.inverse_transform(recommended_sector_ids)

print(f"Recommended Sectors for User {acct_encoder.inverse_transform([user_id])[0]}: {recommended_sectors[:5]}")

# Analyzing Weekday vs. Weekend Preferences
# This could involve generating separate recommendations based on transactions flagged as weekend or not.
# For a detailed analysis, consider splitting the dataset by 'IsWeekend' and repeating the modeling process for each subset.
