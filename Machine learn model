import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('transactions.csv')

# Data preprocessing
df['Date'] = pd.to_datetime(df['Date'])
df['Is_Weekend'] = df['Date'].dt.dayofweek >= 5
df['Days_Since_Last_Trans'] = (pd.to_datetime('now') - df['Date']).dt.days
sectors = ["Entertainment", "Travel", "Shopping", "Restaurant", "Groceries"]

# Feature Engineering
def aggregate_features(df):
    agg_funcs = {'TRAN_AMT': ['sum', 'count'], 'Days_Since_Last_Trans': 'min'}
    df_agg = df.groupby(['ACCT', 'Sector']).agg(agg_funcs).reset_index()
    df_agg.columns = ['ACCT', 'Sector', 'Total_Spend', 'Transaction_Count', 'Recent_Transaction']
    return pd.pivot_table(df_agg, index='ACCT', columns='Sector', fill_value=0).reset_index(drop=True)

interaction_matrix = aggregate_features(df)

# Normalize the interaction matrix
scaler = MinMaxScaler()
interaction_matrix_scaled = scaler.fit_transform(interaction_matrix)

# Autoencoder Model
def build_autoencoder(input_dim):
    input_layer = Input(shape=(input_dim,))
    x = Dense(128, activation='relu')(input_layer)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(64, activation='relu')(x)
    bottleneck = Dense(32, activation='relu', name='bottleneck')(x)
    x = Dense(64, activation='relu')(bottleneck)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    output_layer = Dense(input_dim, activation='sigmoid')(x)
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(lr=0.001), loss='mse')
    return model

# Prepare the data for model training
X_train, X_test = train_test_split(interaction_matrix_scaled, test_size=0.2, random_state=42)

# Initialize and train the autoencoder
autoencoder = build_autoencoder(X_train.shape[1])
history = autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, shuffle=True, validation_data=(X_test, X_test), verbose=1)

# Evaluate the model's performance
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Predict the interaction matrix and identify top sectors
predicted_matrix = autoencoder.predict(interaction_matrix_scaled)

# Assuming sector order in 'interaction_matrix' matches 'sectors', adjust if necessary
top_sector_indices = np.argsort(-predicted_matrix, axis=1)[:, :3]  # Overall top 3 sectors

# Mapping indices to names
sector_names = sectors  # Directly using the predefined 'sectors' list
top_sectors = np.vectorize(lambda x: sector_names[x])(top_sector_indices)

print("Top 3 Sectors for each Customer:", top_sectors)

# Visualization of the top sectors (example for overall top sectors)
sector_counts = pd.Series(top_sectors.flatten()).value_counts()
sns.barplot(x=sector_counts.index, y=sector_counts.values)
plt.title('Distribution of Top 3 Sectors Across All Customers')
plt.xticks(rotation=45)
plt.xlabel('Sectors')
plt.ylabel('Frequency')
plt.show()
