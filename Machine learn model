import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Load dataset (adjust the path to your actual dataset)
df = pd.read_csv('path/to/your/transactions.csv')

# Data preprocessing
df['Date'] = pd.to_datetime(df['Date'])
df['Is_Weekend'] = df['Date'].dt.dayofweek >= 5
df['Days_Since_Last_Trans'] = (pd.Timestamp.now(tz=None) - df['Date']).dt.days
sectors = ["Entertainment", "Travel", "Shopping", "Restaurant", "Groceries"]

# Feature Engineering
def aggregate_features(df, sectors):
    total_spend = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].sum().unstack(fill_value=0)
    total_spend = total_spend.reindex(columns=sectors, fill_value=0)
    scaler = MinMaxScaler()
    total_spend_scaled = scaler.fit_transform(total_spend)
    return total_spend_scaled, total_spend.columns.tolist()

# Prepare data for overall, weekday, and weekend
interaction_matrix, sector_names = aggregate_features(df, sectors)
interaction_matrix_weekday, _ = aggregate_features(df[df['Is_Weekend'] == False], sectors)
interaction_matrix_weekend, _ = aggregate_features(df[df['Is_Weekend'] == True], sectors)

# Autoencoder Model Definition
def build_autoencoder(input_dim):
    input_layer = Input(shape=(input_dim,))
    x = Dense(128, activation='relu')(input_layer)
    x = Dense(64, activation='relu')(x)
    x = Dense(32, activation='relu')(x)
    x = Dense(64, activation='relu')(x)
    x = Dense(128, activation='relu')(x)
    output_layer = Dense(input_dim, activation='sigmoid')(x)
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    return model

# Training
autoencoder = build_autoencoder(interaction_matrix.shape[1])
X_train, X_test = train_test_split(interaction_matrix, test_size=0.2, random_state=42)
history = autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, shuffle=True, validation_data=(X_test, X_test))

# Loss Plot
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Predictions
predicted_matrix = autoencoder.predict(interaction_matrix)
predicted_matrix_weekday = autoencoder.predict(interaction_matrix_weekday)
predicted_matrix_weekend = autoencoder.predict(interaction_matrix_weekend)

# Top Sector Calculation
def map_indices_to_names(indices, names):
    return [names[i] for i in indices]

top_sectors_overall = [map_indices_to_names(indices, sector_names) for indices in np.argsort(-predicted_matrix)[:, :3]]
top_sectors_weekday = [map_indices_to_names([indices], sector_names)[0] for indices in np.argsort(-predicted_matrix_weekday)[:, 0]]
top_sectors_weekend = [map_indices_to_names([indices], sector_names)[0] for indices in np.argsort(-predicted_matrix_weekend)[:, 0]]

# Output Top Sectors
print("Top 3 Sectors for each Customer:\n", top_sectors_overall)
print("Top Weekday Sector for each Customer:\n", top_sectors_weekday)
print("Top Weekend Sector for each Customer:\n", top_sectors_weekend)

# Assuming the sectors are encoded as categorical data in 'df'
# Convert 'Sector' to numerical categories for visualization
df['Sector_Num'] = pd.Categorical(df['Sector'], categories=sectors).codes

# Latent Space Exploration with TSNE
encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[-2].output)  # Adjust to correct encoder layer
encoded_data = encoder.predict(interaction_matrix)

tsne = TSNE(n_components=2, random_state=42)
encoded_data_tsne = tsne.fit_transform(encoded_data)

plt.figure(figsize=(10, 8))
plt.scatter(encoded_data_tsne[:, 0], encoded_data_tsne[:, 1], c=df['Sector_Num'], cmap='viridis')
plt.colorbar()
plt.title('t-SNE Visualization of Encoded Customer Spending')
plt.show()











#
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

# Load your dataset
df = pd.read_csv('path/to/your/transactions.csv')

# Preprocess and feature engineering
df['Date'] = pd.to_datetime(df['Date'])
df['Is_Weekend'] = df['Date'].dt.dayofweek >= 5
df['Days_Since_Last_Trans'] = (pd.Timestamp.now(tz=None) - df['Date']).dt.days
sectors = ["Entertainment", "Travel", "Shopping", "Restaurant", "Groceries"]

def aggregate_features(df, sectors):
    # Pivot table to create features
    pivot = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', 
                           aggfunc=['sum', 'count', 'min'], fill_value=0)
    pivot.columns = ['_'.join([str(func), str(sec)]) for (func, sec) in pivot.columns]
    return pivot

# Generate features for all transactions
interaction_matrix_all = aggregate_features(df, sectors)

# Separate data into weekdays and weekends
interaction_matrix_weekdays = aggregate_features(df[df['Is_Weekend'] == False], sectors)
interaction_matrix_weekends = aggregate_features(df[df['Is_Weekend'] == True], sectors)

# Normalization
scaler = MinMaxScaler()
interaction_matrix_all_scaled = scaler.fit_transform(interaction_matrix_all)
interaction_matrix_weekdays_scaled = scaler.fit_transform(interaction_matrix_weekdays)
interaction_matrix_weekends_scaled = scaler.fit_transform(interaction_matrix_weekends)

# Autoencoder model function
def build_autoencoder(input_dim):
    input_layer = Input(shape=(input_dim,))
    encoded = Dense(128, activation='relu')(input_layer)
    encoded = Dense(64, activation='relu')(encoded)
    decoded = Dense(64, activation='relu')(encoded)
    decoded = Dense(input_dim, activation='sigmoid')(decoded)
    autoencoder = Model(input_layer, decoded)
    autoencoder.compile(optimizer='adam', loss='mse')
    return autoencoder

# Train the model
autoencoder = build_autoencoder(interaction_matrix_all_scaled.shape[1])
X_train, X_test = train_test_split(interaction_matrix_all_scaled, test_size=0.2, random_state=42)
history = autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, 
                          validation_data=(X_test, X_test), verbose=1)

# Plot training and validation loss
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.title('Training and validation loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# Define a function to map sector indices to sector names
def map_indices_to_sectors(indices, sector_names):
    return [sector_names[idx] for idx in indices]

# Predict and extract the top sectors
def get_top_sectors(interaction_matrix_scaled, sector_names, top_n=3):
    predicted = autoencoder.predict(interaction_matrix_scaled)
    top_indices = predicted.argsort(axis=1)[:, -top_n:][:, ::-1]
    top_sectors = np.array(list(map(lambda idx: map_indices_to_sectors(idx, sector_names), top_indices)))
    return top_sectors

# Sector names for mapping
sector_names_all = [col.split('_')[1] for col in interaction_matrix_all.columns if 'sum' in col]
sector_names_weekdays = [col.split('_')[1] for col in interaction_matrix_weekdays.columns if 'sum' in col]
sector_names_weekends = [col.split('_')[1] for col in interaction_matrix_weekends.columns if 'sum' in col]

# Get top 3 sectors overall
top_3_sectors_overall = get_top_sectors(interaction_matrix_all_scaled, sector_names_all, top_n=3)
# Get top sector for weekdays
top_sector_weekdays = get_top_sectors(interaction_matrix_weekdays_scaled, sector_names_weekdays, top_n=1)
# Get top sector for weekends
top_sector_weekends = get_top_sectors(interaction_matrix_weekends_scaled, sector_names_weekends, top_n=1)

print("Top 3 Sectors Overall:\n", top_3_sectors_overall)
print("Top Sector for Weekdays:\n", top_sector_weekdays)
print("Top Sector for Weekends:\n", top_sector_weekends)











Assuming `sector_names` contains the correct sector names:
sector_names = ["Entertainment", "Travel", "Shopping", "Restaurant", "Groceries"]

# Make sure that `top_sector_indices` is generated properly:
# For example, if using `np.argsort` to get top indices, ensure that the array being sorted is the correct one,
# and you are not exceeding the number of sectors in `sector_names`.

# Corrected mapping function using list comprehension instead of np.vectorize for better error handling:
def map_indices_to_names(indices, sector_names):
    # Ensure indices are within the range of sector_names
    if np.max(indices) >= len(sector_names):
        raise ValueError("Sector index out of range. Check the 'top_sector_indices' values.")
    
    # Map indices to sector names using list comprehension
    top_sectors = [[sector_names[i] for i in user_indices] for user_indices in indices]
    return top_sectors

# Assuming `top_sector_indices` is a 2D numpy array with shape (num_users, top_n_sectors):
top_sectors = map_indices_to_names(top_sector_indices, sector_names)

# The rest of your code...






import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('transactions.csv')

# Data preprocessing
df['Date'] = pd.to_datetime(df['Date'])
df['Is_Weekend'] = df['Date'].dt.dayofweek >= 5
df['Days_Since_Last_Trans'] = (pd.to_datetime('now') - df['Date']).dt.days
sectors = ["Entertainment", "Travel", "Shopping", "Restaurant", "Groceries"]

# Feature Engineering
def aggregate_features(df):
    agg_funcs = {'TRAN_AMT': ['sum', 'count'], 'Days_Since_Last_Trans': 'min'}
    df_agg = df.groupby(['ACCT', 'Sector']).agg(agg_funcs).reset_index()
    df_agg.columns = ['ACCT', 'Sector', 'Total_Spend', 'Transaction_Count', 'Recent_Transaction']
    return pd.pivot_table(df_agg, index='ACCT', columns='Sector', fill_value=0).reset_index(drop=True)

interaction_matrix = aggregate_features(df)

# Normalize the interaction matrix
scaler = MinMaxScaler()
interaction_matrix_scaled = scaler.fit_transform(interaction_matrix)

# Autoencoder Model
def build_autoencoder(input_dim):
    input_layer = Input(shape=(input_dim,))
    x = Dense(128, activation='relu')(input_layer)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(64, activation='relu')(x)
    bottleneck = Dense(32, activation='relu', name='bottleneck')(x)
    x = Dense(64, activation='relu')(bottleneck)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    output_layer = Dense(input_dim, activation='sigmoid')(x)
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(lr=0.001), loss='mse')
    return model

# Prepare the data for model training
X_train, X_test = train_test_split(interaction_matrix_scaled, test_size=0.2, random_state=42)

# Initialize and train the autoencoder
autoencoder = build_autoencoder(X_train.shape[1])
history = autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, shuffle=True, validation_data=(X_test, X_test), verbose=1)

# Evaluate the model's performance
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Predict the interaction matrix and identify top sectors
predicted_matrix = autoencoder.predict(interaction_matrix_scaled)

# Assuming sector order in 'interaction_matrix' matches 'sectors', adjust if necessary
top_sector_indices = np.argsort(-predicted_matrix, axis=1)[:, :3]  # Overall top 3 sectors

# Mapping indices to names
sector_names = sectors  # Directly using the predefined 'sectors' list
top_sectors = np.vectorize(lambda x: sector_names[x])(top_sector_indices)

print("Top 3 Sectors for each Customer:", top_sectors)

# Visualization of the top sectors (example for overall top sectors)
sector_counts = pd.Series(top_sectors.flatten()).value_counts()
sns.barplot(x=sector_counts.index, y=sector_counts.values)
plt.title('Distribution of Top 3 Sectors Across All Customers')
plt.xticks(rotation=45)
plt.xlabel('Sectors')
plt.ylabel('Frequency')
plt.show()
