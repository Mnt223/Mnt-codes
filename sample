Yesterday (1st July)
	•	Due to bad weather conditions and power cut, logged in at 1:15 PM.
	•	Most of the time yesterday went into NPS KT for Aishwarya:
	•	Provided Knowledge Transfer on RN NPS process to her.
	•	She is facing some errors/issues while executing, for which I provided clarifications and support.
	•	Connected with multiple stakeholders for walkthrough of the Campaign Dashboard to align on their data requirements.
	•	Worked on GupShub response table:
	•	Identified data flow issues.
	•	Tested the data and connected with the team to check for backup table availability or refresh requirements.
	•	For BrandSalesMI Dashboard published in UID:
	•	Raised access requests for all respective keyholders.
	•	Faced issues with DB-MIM refresh for June:
	•	Multiple datasets are not processed or prepared.
	•	Encountering errors in DB-MIM setup codes.
	•	Planner file for the specific month will only be available in mid-next month.
	•	Action Required: Need to discuss how to proceed with daily refreshes in absence of updated planner files.
	•	Activity MI:
	•	Need to set up a call with Sakshi to understand feasibility of daily run and its conversion to MSD-MIS.
	•	Worked with Pranil on MSD Conversion Dashboard till 10:15 PM:
	•	Prepared multiple measures and key KPIs.
	•	Approximately 30% work completed on MSD tracking conversion.
	•	For CRM 2.0 Dashboard (in UAT):
	•	Received queries on outlier numbers not showing correctly.
	•	Action Required: Connect with Abhijeet to identify and resolve the issue.

⸻

Today (2nd July) – Planned Tasks
	•	Continue working on:
	•	Sales MIS Dashboard
	•	Activity MI
	•	Sales MI Dashboard
	•	Focus on DBMI part, Activity MI, and Planner discussions.
	•	Progress MSD Conversion Dashboard further from current 30% completion.
	•	Campaign Dashboard refresh for all channels is scheduled today; to be completed by end of day.
	•	Trigger LOR file creation process today.












FinalMeasure = 
VAR A = 
    CALCULATE(
        AVERAGE('MSD Trigger Conversion'[POST_2_MON_AVG_CASA]),
        'MSD Trigger Conversion'[CONTACTED_FLAG] = 1
    )
    -
    CALCULATE(
        AVERAGE('MSD Trigger Conversion'[PRE_2_MON_AVG_CASA]),
        'MSD Trigger Conversion'[CONTACTED_FLAG] = 1
    )

VAR B = 
    CALCULATE(
        AVERAGE('MSD Trigger Conversion'[POST_2_MON_AVG_CASA]),
        'MSD Trigger Conversion'[CONTACTED_FLAG] <> 1
    )
    -
    CALCULATE(
        AVERAGE('MSD Trigger Conversion'[PRE_2_MON_AVG_CASA]),
        'MSD Trigger Conversion'[CONTACTED_FLAG] <> 1
    )

VAR ContactedCount = 
    CALCULATE(
        DISTINCTCOUNT('MSD Trigger Conversion'[GA-CASID]),
        'MSD Trigger Conversion'[CONTACTED_FLAG] = 1
    )

RETURN
    IF(
        ContactedCount <> 0,
        (A - B) / ContactedCount,
        BLANK()
    )






contactedgain = 
AVERAGEX(
    FILTER(
        'MSD Trigger Conversion',
        'MSD Trigger Conversion'[CONTACTED_FLAG] = 1 &&
        NOT(ISBLANK('MSD Trigger Conversion'[POST_2_MON_AVG_CASA])) &&
        NOT(ISBLANK('MSD Trigger Conversion'[PRE_2_MON_AVG_CASA]))
    ),
    'MSD Trigger Conversion'[POST_2_MON_AVG_CASA] - 'MSD Trigger Conversion'[PRE_2_MON_AVG_CASA]
)







contactedgain_corrected =
CALCULATE(
    AVERAGE('MSD Trigger Conversion'[POST_2_MON_AVG_CASA]),
    'MSD Trigger Conversion'[CONTACTED_FLAG]=1,
    NOT(ISBLANK('MSD Trigger Conversion'[POST_2_MON_AVG_CASA])),
    NOT(ISBLANK('MSD Trigger Conversion'[PRE_2_MON_AVG_CASA]))
)
-
CALCULATE(
    AVERAGE('MSD Trigger Conversion'[PRE_2_MON_AVG_CASA]),
    'MSD Trigger Conversion'[CONTACTED_FLAG]=1,
    NOT(ISBLANK('MSD Trigger Conversion'[POST_2_MON_AVG_CASA])),
    NOT(ISBLANK('MSD Trigger Conversion'[PRE_2_MON_AVG_CASA]))
)

I noticed that in the last response from GupShup, they only confirmed the repush of the SMS file. Could you please confirm the status of the Email and WhatsApp files as well?

Also, it would be helpful if we could streamline this process so that we receive confirmation from GupShup within a day or two of sharing the files. This will help avoid delays and ensure that our processes run smoothly.

As you’ll be on leave starting Friday, I wanted to check:
	•	If I receive the data today (in the second half) and repush it today, would you be able to share the mapping for non-PegaSend within the next two days?
	•	In your absence, kindly let me know who will be the alternate point of contact for the mapping.

Additionally, I’ve raised a SNOW request for external email access. Hopefully, from the next cycle onward, I will be able to share the files directly with GupShup.

For this cycle, I’m planning to refresh the dashboard for the period 15th May to 15th June.















/* 1. Table with Top Header Inside PROC REPORT */
proc report data=monthly_summary nowd 
    style(report)={borderwidth=1 bordercolor=black cellspacing=3 cellpadding=4 rules=all frame=void}
    style(header)={background=darkred foreground=white font_weight=bold};

    column month targeted_customers respondents conversions;

    define month / display "Month";
    define targeted_customers / display "Target Base";
    define respondents / display "Target Respondents";
    define conversions / display "Approved" format=comma10.;

    /* Centered Header in the Report Before Table Starts */
    compute before _page_ / style={just=center font_weight=bold foreground=dark red font_size=12pt};
        line "Regain Daily Submission";
    endcomp;

run;

/* 2. Bottom Mail-Like Notes (Outside Table) */
ods text = "^S={just=left font_size=10pt}
NOTE:
- Regain Monthly Submission (Globally)
- Targeted Customers: total number of distinct customers targeted/campaign shared for the Regain COE campaign.
- Respondents: Defined based on channel activity:
     > MobileInApp: 'Clicked' on 'Accepted'
     > Email: 'Impression' or 'Clicked'
     > SMS: 'Delivered' (considered 'Opened')
- Approved: Customers who have opted in and approved in the respective month.";









compute before _page_ / style=[just=left font_size=10pt];
   line "NOTE:";
   line "In this global data:";
   line "- Targeted Customers: Total number of customers targeted/shared for the Regain CEO campaign.";
   line "- Respondents: Defined based on channel activity:";
   line "    > In-App – Clicked on 'Accepted'";
   line "    > Email – Clicked on 'Impression' or 'Click'";
   line "    > SMS – Where message was 'Delivered' or 'Opened'";
   line "- Approved: Customers who have opted in and approved in the respective channel.";
endcomp;







proc sql;
    create table monthly_summary as
    select
        a.month,
        count(distinct case when is_targeted = 1 then a.customer_id end) as targeted_customers,
        count(distinct case when is_respondent = 1 then a.customer_id end) as respondents,
        count(distinct c.customer_id) as conversions
    from campaign_flags a
    left join conversion_months c
        on a.customer_id = c.customer_id
        and a.month = c.month
    group by a.month
    order by a.month;
quit;








proc sql;
    create table monthly_summary as
    select
        month,
        count(distinct case when is_targeted = 1 then customer_id end) as targeted_customers,
        count(distinct case when is_respondent = 1 then customer_id end) as respondents
    from campaign_flags
    group by month
    order by month;
quit;






data campaign_flags;
    set campaign_data;
    format month date9.;
    month = intnx('month', event_date, 0, 'b');

    /* Flags */
    is_targeted = (upcase(outcome) ne "PENDING");

    is_respondent = 0;
    if (channel = 'Email' and outcome in ('Impression', 'Click')) or
       (channel = 'Mobile In-App' and outcome = 'Accepted') or
       (channel = 'SMS' and outcome = 'Delivered') then is_respondent = 1;
run;





RM Level Triggers = 
CONCATENATEX(
    FILTER(
        CustomerTriggers,
        CustomerTriggers[RM Name] = MAX(CustomerTriggers[RM Name]) &&
        CustomerTriggers[Customer Name] = MAX(CustomerTriggers[Customer Name])
    ),
    CustomerTriggers[Message Name],
    " | "
)











Hi everyone,

This is to request immediate action to clear space on the IAM server by zipping or deleting files that are no longer required.

@Shouvik @Kinari – Over the past few weeks, we’ve observed a significant degradation in SAS performance, primarily because our daily BAUs are taking more than double their usual runtime. This is causing delays, and several critical BAUs are not running within their scheduled timelines.

Additionally, we are now frequently encountering space issues on the server. SAS jobs are failing due to insufficient space, forcing us to rerun them, which increases system load, consumes unnecessary bandwidth, and disrupts daily timelines.

Kindly treat this as a high priority and initiate cleanup of unused or outdated files on the IAM server to help restore performance stability.

Please confirm once the cleanup is completed or flag if any support is needed.







Hi everyone,

This is to request immediate action to clear space on the IAM server by zipping or deleting files that are no longer needed.

@Shouvik @Kinari – Over the past few weeks, we’ve observed a significant drop in sales performance, primarily due to daily BAUs taking more than double their usual runtime. These delays are directly affecting the execution timelines, and many critical BAUs are not running on time.

In addition, we’re frequently hitting space limitations on the server, leading to code failures during execution. These failures are forcing reruns, which are not only time-consuming but also consuming substantial bandwidth and further disrupting daily operations.

Kindly prioritize this cleanup and free up space on the IAM server at the earliest to ensure smoother processing and timely execution of daily activities.

Please confirm once this is completed or flag any issues that need support.

Best regards,
[Your Name]

⸻

Let me know if you’d like to add escalation lines or a specific action deadline.
















Hi everyone,

Please find below the key updates regarding the Power BI Dashboard for Frontline Opportunity:
	1.	I have successfully created the Power BI dashboard, where leads have been shared with the respective RMs based on their assignments.
	2.	I have also created the SharePoint link and connected the Power BI dashboard to the data source accordingly. In addition, ED groups for production access have been raised. As advised by the Power BI team, we are required to first proceed with UAT (User Acceptance Testing). We will be sharing the entitlement group names with the Power BI team, which they will incorporate during the UAT phase before we move to production.
	3.	I have verified that Power BI provides the option to restrict data export. The “Export Data” feature can be disabled, ensuring that users are unable to download any data. They will only be able to view the dashboard data as per their RLS (Row-Level Security) assignments. We will connect with the Power BI team tomorrow to confirm and finalize this configuration.

Please feel free to reach out if you have any questions or need further clarification.








Hi everyone,

Please find below the key updates regarding the Power BI Dashboard for Frontline Opportunity:
	1.	The Power BI dashboard has been successfully created where leads have been assigned and shared with respective RMs.
	2.	A SharePoint link has also been created and is now connected to the dashboard. Additionally, ED groups for production access have been raised. However, as per the Power BI team’s guidance, we are required to complete the UAT (User Acceptance Testing) first. We will be sharing the entitlement group names with the Power BI team so they can incorporate these during the UAT phase before we proceed to production deployment.
	3.	I have checked that within Power BI, we can restrict data export functionality. The “Export Data” option can be blocked, ensuring that no user can download the data from the dashboard. Users will only be able to view the report as per their RLS (Row-Level Security) assignments. We will be connecting with the Power BI team tomorrow to confirm this setup and finalize the configuration.

Please let me know in case of any questions











Source = SharePoint.Files("https://yourdomain.sharepoint.com/sites/YourSite", [ApiVersion = 15]),
Filtered = Table.SelectRows(Source, each [Extension] = ".xlsx")





Hi Everyone,

Please note that today’s batch for the Call Center will be delayed due to some ongoing system issues we are currently experiencing.

We are already in touch with the IT team for resolution and are working to implement the batch as soon as the issue is resolved.

Thank you for your understanding and patience.









I would like to bring to your attention two performance issues observed today:
	1.	INPS MSF Invitation File:
This file is usually triggered and completed within 1 hour. However:
	•	Last Monday, it took around 2 hours 30 minutes to complete.
	•	Today, I triggered it at 1:16 PM, and as of 3:35 PM, it is still running.
	2.	S-E-G-E-O-P Code:
This code typically runs through CTRL-M and completes within 2 to 5 minutes.
	•	Today, it did not trigger via CTRL-M, so I executed it manually at 3:12 PM.
	•	As of 3:44 PM, the code is still running, which is significantly longer than usual.

Both processes are experiencing abnormal delays, which may indicate performance or resource-related issues.

Requesting your support to investigate the root cause and help resolve these delays at the earliest.












Hi Pavneet,

On 4th June, we sent the Contact Center Invitation file containing a total of 11,681 customers, out of which:
	•	4,215 customers belonged to the eligible base.
	•	7,466 customers had interacted with us, but unfortunately, the communication was sent to them mistakenly and should not have been shared.

As we now need to share the Survey IDs for the non-eligible base (7,466 customers), the TO team has provided us with their base file including the survey IDs.

However, we noticed a discrepancy in the total counts:
	•	Our file had 11,681 customers in total.
	•	The TO-provided file contains only 11,063 customers, indicating a shortfall of 618 customers.

Upon checking with the TO, it was confirmed that:
	•	608 customers were discarded due to incorrect Unit IDs, and hence no survey was triggered for them in the system.

Within the non-eligible base of 7,466 customers, we also found:
	•	501 customers who do not have Survey IDs, which means their records were discarded from the Medallia system, and no survey was sent to them.

Accordingly, I have filtered and prepared the final base of 6,965 customers for whom surveys were actually sent and Survey IDs are available.

Pavneet, I have verified the base. Request you to please review it once and share the file with the TO team so they can proceed with the next steps.











The input file for the survey trigger was supposed to include customer interactions from 2nd and 3rd June only. However, due to an oversight, the file mistakenly included records from 2nd May to 3rd June. As a result, the sample size became significantly larger, and surveys were sent to customers who had interacted in May as well













I would like to highlight the issue we encountered with the recent contact center NPS file.

Background:
The intent was to include only customer interactions from 2nd June onward. However, due to an oversight, the input file mistakenly included interactions from 2nd May to 3rd June. As a result, the sample size increased significantly, and surveys were sent to customers who had interacted in May as well.

Root Cause:
This issue occurred due to a manual error in the code logic. The date filter was not correctly applied, and the necessary adjustments were missed during implementation.

Impact Assessment:
We have reviewed the situation with the Global CX and Medallia teams. Based on their inputs:
	•	The impact is minimal.
	•	No sensitive or written customer data was captured in the survey.
	•	Surveys were sent only to customers who had interacted with us, although from a broader timeframe.
	•	There is no major risk or compliance breach identified.

Corrective Action:
As this is a newly implemented process, we are introducing a maker-checker validation mechanism to strengthen quality control. This additional review layer is currently in progress and will be fully integrated into all future survey runs to prevent recurrence.













As part of the ongoing review, please find below the detailed update on the INPS CC survey file issue, structured under key headings for clarity:

⸻

Background:
The INPS (Contact Center) survey process is based on a 30-day exclusion rule, where customers should not be surveyed again within 30 days of their last survey interaction. The trigger for June was supposed to consider customers who interacted on or after 2nd June 2025.

⸻

What Happened:
Due to an error in the input date logic, the file that was triggered on 4th June included customers who had interacted between 2nd May and 3rd June, rather than only from 2nd June onward. As a result, surveys were sent to a larger set of customers, including some who interacted in May.

⸻

Root Cause:
A manual error in the date filter logic within the code led to incorrect date selection. The intent was to filter customers from 2nd June onward, but the date range was incorrectly set to include the entire period from 2nd May to 3rd June.

⸻

Why It Happened:
This was a human oversight during code implementation and review, in the absence of a formalized checker process. As this is a newly deployed process, sufficient controls and validations were not yet established.

⸻

Impact:
	•	The immediate impact is limited. Surveys were sent to a larger sample, but no sensitive or written customer data was involved.
	•	Some customers who received surveys on 2nd and 3rd May may have been included again in June, violating the 30-day exclusion policy.
	•	Customers who interacted between 4th May and 1st June, and didn’t receive surveys in May, were selected in June. This is acceptable under the exclusion rules, as they weren’t surveyed earlier.

We have also checked with the Group CX and Medallia teams, who confirmed that:
	•	There is no major risk or compliance breach.
	•	No additional communication to customers is required at this stage.

⸻

Quarantine and Preventive Action:
	•	Affected records from 2nd and 3rd May will be quarantined on Medallia’s platform as per standard 30-day exclusion logic.
	•	A maker-checker process is now being formalized and will be implemented to prevent such errors in future cycles.
	•	Final validations are underway to ensure full alignment with survey logic and data filters.

We will share the final impact assessment once all checks are complete.









As a follow-up to our earlier communication regarding the INPS CC survey file issue, I’d like to share a quick update.

We have identified that the root cause was a manual error in the date filter logic, which led to the inclusion of records from 2nd May to 3rd June, instead of only from 2nd June onwards. This resulted in surveys being sent to a larger sample size, including customers who interacted in May.

Key Points:
	•	We have confirmed with the Group CX and Medallia teams that there is no major impact and no need to send any corrective communication to customers.
	•	The only overlap observed was for customers who had already received a survey on 2nd or 3rd May, which could potentially violate the 30-day exclusion policy.
	•	Customers who interacted between 4th May and 1st June were rightly picked up in June, as they were not surveyed earlier.

Next Steps:
	•	We are formalizing a maker-checker process to strengthen control, given this is a newly implemented process.
	•	Final impact validation and code correction are in progress. We will ensure that this is addressed in all future runs.








I would like to highlight the issue we encountered with the recent contact center file.

Background:
The intention was to include only customer interactions from 2nd June onward. However, due to an oversight, the input file included data from 2nd May to 3rd June. As a result, the sample size became significantly larger, and surveys were sent to customers who had interacted in May as well.

Root Cause:
This occurred due to a human error in the code logic. The date filter was not correctly applied, and the necessary adjustments were missed during code implementation.

Impact Assessment:
We have reviewed the situation with the global team and confirmed that the impact is minimal. The survey responses do not capture any sensitive or written customer data, and the surveys were still sent only to customers who had interacted with us, albeit from a slightly extended period. Therefore, no major risk or compliance breach has been identified.

Corrective Action:
We are currently implementing a maker-checker validation process for this workflow, given that it is still relatively new. This additional layer of review is in progress and will be fully integrated to prevent such issues going forward.

Please let us know if any further clarification is required.

Best regards,
[Your Name]















Please find below my planned work items for today:
	1.	PegaData Sharing: Start with sharing the required PegaData for the Global Team.
	2.	Business Analytics Training: Attend the scheduled training session.
	3.	CRM 2.0 Modifications: Work on pending modifications and enhancements.
	4.	HTML Code Enhancements: Continue making progress on the HTML code updates.
	5.	Segmentation Automation: Run the automated segmentation code as per plan.
	6.	JiraTek AD Requests: Raise AD access requests via JiraTek for:
	•	CRM 2.0 Dashboard
	•	HML Dashboard
	7.	Campaign Dashboard Updates: Apply necessary changes and raise the request for the same.
	8.	TRB Analysis: Join the discussion and proceed with finalizing the TRB analysis today.










We had sent the data girls re-push request to Gupshup on the 20th, and they have confirmed that the files have been pushed back.

Could you please check and confirm from your end? Once confirmed, I’ll proceed with refreshing the base.









A quick update on the Branch Dashboard:
	•	JIRA ticket has been raised.
	•	Request for 80 UAT groups submitted – expected completion in 4–5 working days.
	•	Once done, the Power BI team will publish the dashboard in UAT.

As discussed in the last call, Phase 1 launch will be without RLS. So, the UAT version will be shared with RMs without RLS applied.

Pending items for discussion:
	1.	RM Numbers (RMB, BAS, Service RM):
Need to validate and include cross-checker numbers.
Also, align the planar files.
	2.	Advanced Wealth & Revenue Metrics:
@Partho – need your support with data integration for these metrics.











Hi NL,

I wanted to share a quick update on the Branch Dashboard progress.

To enable the UAT publishing, the following steps have already been initiated:
	•	The JIRA ticket has been raised for the deployment.
	•	The request for creating the 80 groups required for UAT access has also been submitted.

This setup is expected to take approximately 4–5 working days. Once the 80-group configuration is completed, the Power BI team will help push the dashboard to UAT.

In the meantime, there are a couple of key areas that require discussion and input for further updates on the dashboard:
	1.	RM Numbers (RMB, BAS, Service RM):
We need to validate and cross-check these numbers. Requesting your availability so we can set up a short discussion to ensure alignment.
	2.	Advanced Wealth Penetration and Revenue Metrics:
@Partho – we’ll need your support here to understand the required data points and how we can effectively incorporate them into the dashboard.

Looking forward to your availability so we can move ahead on both fronts smoothly.








Last year, while working on campaign activities, I had automated the Diap Drop-off to Response Conversion Mail Summary, which is currently running as part of the daily automated code.

There’s been an adhoc request raised by Nidhi regarding this, and Kevin has redirected the query to me.

Could you please confirm if I should take this up, or if it falls under the scope of campaign adhoc requests that need to be handled separately?

Looking forward to your guidance.








There is a fully automated code that is intended to run daily. Currently, I’m triggering it manually, but this should ideally be scheduled through Control-M.

To initiate this, I first reached out to Smriti regarding the campaign flow. She mentioned that it should be handled by the BI team as they manage Control-M jobs, but she wasn’t aware of the exact owner.

I then connected with Anu, who informed me that there are campaign-related Control-M jobs and suggested I reach out to Vandita or Kevin. Accordingly, I drafted and sent a mail requesting Control-M scheduling, but haven’t received any response yet.

Additionally, I would also like to understand whom to reach out to for Control-M scheduling of segmentation MIS jobs.

Would really appreciate it if you could guide me to the right contact(s) for both these use cases so the processes can be streamlined.











proc sql;
    create table want as
    select *,
        /* Break into parts */
        scan(sentdate, 2, ' ') as mon,
        scan(sentdate, 3, ' ') as day,
        scan(sentdate, 4, ' ') as timepart,
        scan(sentdate, 6, ' ') as year,

        /* Create valid datetime string: e.g., '09APR2025 18:23:33' */
        catx(' ', scan(sentdate, 3, ' ') || scan(sentdate, 2, ' ') || scan(sentdate, 6, ' '), scan(sentdate, 4, ' ')) as dt_string,

        /* Convert to SAS datetime */
        input(catx(' ', scan(sentdate, 3, ' ') || scan(sentdate, 2, ' ') || scan(sentdate, 6, ' '), scan(sentdate, 4, ' ')), datetime20.) as sent_datetime format=datetime20.,

        /* Extract date and time */
        datepart(input(catx(' ', scan(sentdate, 3, ' ') || scan(sentdate, 2, ' ') || scan(sentdate, 6, ' '), scan(sentdate, 4, ' ')), datetime20.)) as sent_date format=date9.,
        timepart(input(catx(' ', scan(sentdate, 3, ' ') || scan(sentdate, 2, ' ') || scan(sentdate, 6, ' '), scan(sentdate, 4, ' ')), datetime20.)) as sent_time format=time8.

    from your_dataset;
quit;







Just to update you — I received the product mapping on Friday and refreshed the base for the CRM database that night. The database was further updated on Saturday.

Over the weekend, the HK server was unstable, and several jobs failed due to patch refreshes in the SAS environment.

While reviewing the catalog, I noticed some mismatches across both PEGA and non-PEGA product sources. I have spent significant time aligning the product mapping, updating the CRM Product 14 flag logic, and resolving inconsistencies within the catalog.

Based on my understanding, I’ve made the necessary corrections at my end. It would be great if you could also recheck the catalog once and let me know in case any product mismatches still remain.

Now that the base is ready, I’ll proceed with preparing the CRM summary on Monday.








proc sql;
  create table hml_trb_comparison as
  select 
    trb_segment,
    count(distinct CUSID) as num_customers,
    sum(RELN_TRB_MAX_FINAL_0325) as total_trb format=comma20.0,
    mean(RELN_TRB_MAX_FINAL_0325) as avg_trb format=comma20.2
  from HML_QUALIFIED
  where HML_Qualified_flag = 1 /* restrict to only HML qualified */
  group by trb_segment;
quit;





/* Step 1: Filter HML customers */
data hml_customers;
  set your_dataset;
  if mortgage_flag = 1; /* Replace with your HML logic if different */
run;

/* Step 2: Create TRB Relationship Segment from Flag */
data hml_flag_segment;
  set hml_customers;

  length trb_segment $10;

  if TRUE_PRM_MAX_TRB_FLAG = 'TRUE PREMIER BASIS LAST MONTH TRB >=40 LACS' then trb_segment = '40L+';
  else trb_segment = '<40L';
run;

/* Step 3: Summarize each group */
proc sql;
  create table hml_flag_summary as
  select 
    trb_segment,
    count(distinct CUSID) as num_customers,
    sum(RELN_TRB_MAX_FINAL_0325) as total_trb format=comma20.0,
    mean(RELN_TRB_MAX_FINAL_0325) as avg_trb format=comma20.2
  from hml_flag_segment
  group by trb_segment;
quit;

/* Step 4: Calculate Uplift */
data hml_flag_uplift;
  set hml_flag_summary;
  retain base_trb;
  if trb_segment = '<40L' then base_trb = avg_trb;
  if trb_segment = '40L+' then uplift_pct = round(((avg_trb - base_trb)/base_trb)*100, 0.1);
run;

/* Step 5: Show Final Comparison */
proc print data=hml_flag_uplift noobs label;
  var trb_segment num_customers total_trb avg_trb uplift_pct;
  label
    trb_segment = "TRB Band (from Flag)"
    num_customers = "Customer Count"
    total_trb = "Total TRB"
    avg_trb = "Avg TRB"
    uplift_pct = "Uplift vs <40L (%)";
run;









/* Step 1: Filter mortgage customers with TRB ≥ 40L */
proc sql;
  create table mortgage_40L as
  select *
  from your_dataset
  where mortgage_flag = 1 and RELN_TRB_MAX_FINAL_0325 >= 4000000;
quit;

/* Step 2: Create TRB qualified flag */
data mortgage_40L_flagged;
  set mortgage_40L;
  if TRUE_PRM_MAX_TRB_FLAG = 'NOT A TRUE PREMIER' then Qualified_flag = 0;
  else Qualified_flag = 1;
run;

/* Step 3: Summary by TRB qualification */
proc sql;
  create table mortgage_summary as
  select 
    Qualified_flag,
    count(distinct CUSID) as num_customers,
    sum(RELN_TRB_MAX_FINAL_0325) as total_trb format=comma20.0,
    mean(RELN_TRB_MAX_FINAL_0325) as avg_trb format=comma20.2
  from mortgage_40L_flagged
  group by Qualified_flag;
quit;

/* Step 4: Calculate uplift */
data mortgage_uplift;
  set mortgage_summary;
  retain base_trb;
  if Qualified_flag = 0 then base_trb = avg_trb;
  if Qualified_flag = 1 then do;
    uplift_trb_pct = round(((avg_trb - base_trb) / base_trb) * 100, 0.1);
  end;
run;

/* Step 5: Display final comparison */
proc print data=mortgage_uplift noobs label;
  var Qualified_flag num_customers total_trb avg_trb uplift_trb_pct;
  label
    Qualified_flag = "TRB Qualified"
    num_customers = "Customers ≥ ₹40L"
    total_trb = "Total TRB"
    avg_trb = "Avg TRB"
    uplift_trb_pct = "Uplift %";
run;









/* Step 1: Filter salary-mapped customers */
proc sql;
  create table salary_customers as
  select *
  from your_dataset
  where salary_flag = 1;
quit;

/* Step 2: Group by TRB qualification */
proc sql;
  create table salary_summary as
  select 
    trb_qualified_flag,
    count(distinct customer_id) as num_customers,
    sum(trb_amount) as total_trb format=comma20.0,
    sum(revenue) as total_revenue format=comma20.0,
    mean(trb_amount) as avg_trb format=comma20.2,
    mean(revenue) as avg_revenue format=comma20.2
  from salary_customers
  group by trb_qualified_flag;
quit;

/* Step 3: Calculate uplift */
data salary_uplift;
  set salary_summary;
  retain base_trb base_rev;
  if trb_qualified_flag = 0 then do;
    base_trb = avg_trb;
    base_rev = avg_revenue;
  end;
  if trb_qualified_flag = 1 then do;
    uplift_trb_pct = round(((avg_trb - base_trb) / base_trb) * 100, 0.1);
    uplift_rev_pct = round(((avg_revenue - base_rev) / base_rev) * 100, 0.1);
  end;
run;

/* Step 4: Output */
proc print data=salary_uplift noobs label;
  var trb_qualified_flag num_customers total_trb total_revenue avg_trb avg_revenue uplift_trb_pct uplift_rev_pct;
  label
    trb_qualified_flag = "TRB Qualified"
    num_customers = "Customer Count"
    total_trb = "Total TRB"
    total_revenue = "Total Revenue"
    avg_trb = "Avg TRB"
    avg_revenue = "Avg Revenue"
    uplift_trb_pct = "% Uplift in TRB"
    uplift_rev_pct = "% Uplift in Revenue";
run;










Salary_Bucket = 
SWITCH(
    TRUE(),
    'Table'[Salary] = 0, "0",
    'Table'[Salary] <= 500000, "0 - 5L",
    'Table'[Salary] <= 1500000, "5L - 15L",
    'Table'[Salary] <= 2500000, "15L - 25L",
    'Table'[Salary] <= 3500000, "25L - 35L",
    'Table'[Salary] <= 4000000, "35L - 40L",
    'Table'[Salary] > 4000000, "40L+",
    BLANK()
)







let
    StartDate = #date(2024, 1, 1),
    EndDate = Date.EndOfMonth(DateTime.Date(DateTime.LocalNow())),
    DateList = List.Dates(StartDate, Duration.Days(EndDate - StartDate) + 1, #duration(1, 0, 0, 0)),
    CalendarTable = Table.FromList(DateList, Splitter.SplitByNothing(), {"Date"}),
    AddedYear = Table.AddColumn(CalendarTable, "Year", each Date.Year([Date])),
    AddedMonth = Table.AddColumn(AddedYear, "Month", each Date.Month([Date])),
    AddedMonthName = Table.AddColumn(AddedMonth, "Month Name", each Date.ToText([Date], "MMM")),
    AddedYearMonth = Table.AddColumn(AddedMonthName, "YearMonth", each Date.ToText([Date], "yyyyMM"))
in
    AddedYearMonth






if [Month Name] = "JAN" then 1
else if [Month Name] = "FEB" then 2
else if [Month Name] = "MAR" then 3
else if [Month Name] = "APR" then 4
else if [Month Name] = "MAY" then 5
else if [Month Name] = "JUN" then 6
else if [Month Name] = "JUL" then 7
else if [Month Name] = "AUG" then 8
else if [Month Name] = "SEP" then 9
else if [Month Name] = "OCT" then 10
else if [Month Name] = "NOV" then 11
else if [Month Name] = "DEC" then 12
else null












We’ve built this Sales MI Dashboard in Power BI to track branch-level sales at an RM level, including key performance metrics such as product sales, trigger achievements, and more. This is designed to be a one-stop solution for the front line, offering complete visibility at a daily level.

This initiative integrates 8 different MIS reports into a single interactive dashboard, eliminating the need to access and track multiple Excel files manually.

Previously, teams had to rely on disparate Excel sheets which were time-consuming to open, analyze, and interpret on a daily basis. Based on continuous feedback from the front line, we developed this unified view to save time, reduce manual effort, and directly enhance productivity.

⸻

Next Steps (for Sharon)
Sharon, we would need the final set of inputs to complete the view. Once received, we’d be happy to schedule a detailed walkthrough of the dashboard to demonstrate its functionality and how it supports daily RM performance monitoring.












The mapping for the campaign dashboard was completed on time. However, the refresh for April 15th has not been done this month. This delay is due to unresolved issues with GupShup, specifically because SAS automated mails cannot be sent to external recipients, as confirmed by Raj.

To address this, we’ve implemented a workaround where the SAS-generated mail is received internally and then auto-forwarded to GupShup using Outlook rules. I hope this approach works smoothly and that GupShup receives the files on time without any manual intervention.

As discussed and agreed, the dashboard will be refreshed for the full month of April next week.
Could you please confirm if we are aligned on this? Also, kindly let me know if there are any queries or if further clarification is needed.

Additionally, could you please guide me on who can help with setting up the codes in Control-M?













Hi Souvik,

I connected with Bharat regarding the HML pricing data. He mentioned that the data we are currently using is incorrect and has already shared the correct dataset with you.

As per the updated file, the salary TRB amount needs to be included along with the correct SM, RM, and BAS names.

Also, could you please share with me the code or let me know if there are any pre-set codes available for their current ask?

Bharat has requested the updated output by the upcoming Monday.







%let folder_path = /your/folder/path;

%macro create_folder_if_not_exists(path);
    %if %sysfunc(fileexist(&path)) = 0 %then %do;
        options noxwait noxsync;
        x "mkdir &path";
        %put NOTE: Folder created - &path;
    %end;
    %else %do;
        %put NOTE: Folder already exists - &path;
    %end;
%mend;

%create_folder_if_not_exists(&folder_path);











/* Step 1: Clean previous data */
proc datasets lib=work nolist;
    delete backlog summary;
quit;

/* Step 2: Define paths and thresholds */
%let base_path = /sasdata/hsbc/dil/INM/IMC/external_data_transfer/GupShup/landing/inbound/;
%let unzip_path = /sasdata/hsbc/dil/INM/SASBAU/sandbox/Prachi/Sample_Check/;
%let restricted_path = /sasdata/hsbc/dil/INM/SASBAU/sandbox/Prachi/Restricted_Files/;
%let days_back = 30;
%let size_threshold = 102400; /* 100KB */

/* Step 3: Initialize log dataset */
data backlog;
    length file_date $10 file_status $25 extracted_files $100
           file_size_325 file_size_606 file_size_328 8
           formatted_size_325 formatted_size_606 formatted_size_328 $20;
    stop;
run;

/* Step 4: Macro to check files */
%macro check_email_files();
%do i = 0 %to %eval(&days_back - 1);

    %let raw_date = %sysfunc(intnx(day, %sysfunc(today()), -&i), yymmdd10.);
    %let check_date = %sysfunc(putn(&raw_date, yymmdd10.));
    %let formatted_date = %sysfunc(tranwrd(&check_date, "-", ""));

    %let zip_filename = Email_&formatted_date..zip;
    %let extracted_folder = &unzip_path./&formatted_date;
    %let restricted_folder = &restricted_path./&formatted_date;
    %let full_zip_path = &base_path./&zip_filename;

    %let csv_325 = 325.csv;
    %let csv_606 = 606.csv;
    %let csv_328 = 328.csv;

    /* Step 5: Check ZIP file existence */
    data _null_;
        full_zip_path = resolve("&full_zip_path");
        file_exists = fileexist(full_zip_path);
        call symputx('file_found', file_exists);
    run;

    %if &file_found %then %do;

        x "mkdir -p &extracted_folder";
        x "unzip -o &full_zip_path -d &extracted_folder";
        x "mkdir -p &restricted_folder";

        x "mv &extracted_folder./&csv_325 &restricted_folder 2>/dev/null";
        x "mv &extracted_folder./&csv_606 &restricted_folder 2>/dev/null";
        x "mv &extracted_folder./&csv_328 &restricted_folder 2>/dev/null";

        /* Step 6: Verify extraction success */
        data _null_;
            extracted_path = resolve("&restricted_folder");
            extracted_exists = fileexist(extracted_path);
            call symputx('extracted_found', extracted_exists);
        run;

        %if &extracted_found %then %do;

            %let status = Valid;
            %let extracted_files = ;
            %let file_size_325 = 0;
            %let file_size_606 = 0;
            %let file_size_328 = 0;

            data _null_;
                path_325 = resolve("&restricted_folder./&csv_325");
                path_606 = resolve("&restricted_folder./&csv_606");
                path_328 = resolve("&restricted_folder./&csv_328");

                file_325_size = ifn(fileexist(path_325), fsize(fopen(path_325)), .);
                file_606_size = ifn(fileexist(path_606), fsize(fopen(path_606)), .);
                file_328_size = ifn(fileexist(path_328), fsize(fopen(path_328)), .);

                call symputx('file_size_325', file_325_size);
                call symputx('file_size_606', file_606_size);
                call symputx('file_size_328', file_328_size);
            run;

            %if &file_size_325 > 0 %then %let extracted_files = &extracted_files 325.csv;
            %if &file_size_606 > 0 %then %let extracted_files = &extracted_files 606.csv;
            %if &file_size_328 > 0 %then %let extracted_files = &extracted_files 328.csv;

            %if %length(&extracted_files) = 0 %then %let status = Missing;

            %let formatted_size_325 = %sysfunc(putn(&file_size_325, comma20.));
            %let formatted_size_606 = %sysfunc(putn(&file_size_606, comma20.));
            %let formatted_size_328 = %sysfunc(putn(&file_size_328, comma20.));

            data temp;
                file_date = "&formatted_date";
                file_status = "&status";
                extracted_files = "&extracted_files";
                file_size_325 = &file_size_325;
                file_size_606 = &file_size_606;
                file_size_328 = &file_size_328;
                formatted_size_325 = "&formatted_size_325";
                formatted_size_606 = "&formatted_size_606";
                formatted_size_328 = "&formatted_size_328";
            run;

        %end;
        %else %do;
            data temp;
                file_date = "&formatted_date";
                file_status = "Extraction Failed";
                extracted_files = "-";
                file_size_325 = .;
                file_size_606 = .;
                file_size_328 = .;
                formatted_size_325 = "";
                formatted_size_606 = "";
                formatted_size_328 = "";
            run;
        %end;

        proc append base=backlog data=temp force; run;

    %end;
    %else %do;
        data temp;
            file_date = "&formatted_date";
            file_status = "Missing";
            extracted_files = "-";
            file_size_325 = .;
            file_size_606 = .;
            file_size_328 = .;
            formatted_size_325 = "";
            formatted_size_606 = "";
            formatted_size_328 = "";
        run;

        proc append base=backlog data=temp force; run;
    %end;

%end;
%mend check_email_files;

/* Step 5: Run the macro */
%check_email_files();