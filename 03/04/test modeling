import pandas as pd
import numpy as np
import os
import joblib
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor
from pycaret.classification import setup, compare_models, pull
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
import warnings
warnings.filterwarnings("ignore")

SESSION_ID = 1992
np.random.seed(SESSION_ID)

def preprocess_data(df, target, drop_cols):
    df = df.copy()
    df = df.dropna(axis=1, how='all')
    df = df.loc[:, ~df.columns.duplicated()]
    if drop_cols:
        df.drop(columns=drop_cols, inplace=True, errors='ignore')
    df = df[df[target].notnull()]
    return df

def robust_feature_selection(data, target, missing_thresh=0.7, corr_thresh=0.85, vif_thresh=5.0, top_n=50):
    data = data.copy()
    missing_ratio = data.isnull().mean()
    data = data.loc[:, missing_ratio <= missing_thresh]

    selector = VarianceThreshold()
    num_data = data.select_dtypes(include=[np.number])
    selector.fit(num_data.fillna(0))
    low_var = num_data.columns[~selector.get_support()]
    data.drop(columns=low_var, inplace=True)

    corr = data.corr().abs()
    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
    to_drop = [col for col in upper.columns if any(upper[col] > corr_thresh)]
    data.drop(columns=to_drop, inplace=True)

    X = data.drop(columns=[target]).fillna(0)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    vif = pd.DataFrame()
    vif["feature"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]
    high_vif = vif[vif["VIF"] > vif_thresh]["feature"].tolist()
    data.drop(columns=high_vif, inplace=True)

    corrs = data.corr()[target].abs().sort_values(ascending=False)
    final_feats = corrs.drop(target).head(top_n).index.tolist()
    final_feats.append(target)
    return data[final_feats], final_feats

def gini_score(y_true, y_prob): return 2 * roc_auc_score(y_true, y_prob) - 1

def generate_score(prob, base_score=200, factor=28.8539):
    odds = (1 - prob) / (prob + 1e-8)
    return np.clip(base_score - factor * np.log(odds), 0, 1000)

def calculate_ks(y_true, y_scores):
    fpr, tpr, thresholds = roc_curve(y_true, y_scores)
    return max(tpr - fpr)

def create_deciles(df, score_col, target_col):
    df["decile"] = pd.qcut(df[score_col], 10, labels=False, duplicates='drop') + 1
    return df.groupby("decile").agg(
        total=('decile', 'count'),
        events=(target_col, 'sum'),
        non_events=(target_col, lambda x: x.count() - x.sum())
    ).reset_index()

def calculate_psi(base, current, buckets=10):
    def scale_range(x, min_val, max_val): return (x - min_val) / (max_val - min_val + 1e-8)
    psi_values = []
    for col in base.columns:
        if col not in current.columns: continue
        b_counts, _ = np.histogram(scale_range(base[col], base[col].min(), base[col].max()), bins=buckets)
        c_counts, _ = np.histogram(scale_range(current[col], base[col].min(), base[col].max()), bins=buckets)
        b_perc = b_counts / b_counts.sum()
        c_perc = c_counts / c_counts.sum()
        psi = np.sum((b_perc - c_perc) * np.log((b_perc + 1e-8) / (c_perc + 1e-8)))
        psi_values.append((col, psi))
    return pd.DataFrame(psi_values, columns=["Feature", "PSI"]).sort_values(by="PSI", ascending=False)

def calculate_csi(train, oot, target_col, bins=10):
    csi_list = []
    for col in train.drop(columns=[target_col]).columns:
        try:
            train_bins = pd.qcut(train[col], bins, duplicates='drop')
            oot_bins = pd.qcut(oot[col], bins, duplicates='drop')
            train_dist = train_bins.value_counts(normalize=True)
            oot_dist = oot_bins.value_counts(normalize=True)
            csi = np.sum((train_dist - oot_dist) * np.log((train_dist + 1e-8) / (oot_dist + 1e-8)))
            csi_list.append((col, csi))
        except:
            continue
    return pd.DataFrame(csi_list, columns=["Feature", "CSI"]).sort_values(by="CSI", ascending=False)

def run_model_pipeline(main_data, oot_data, target, id_columns=None, drop_cols=None, top_n_features=50,
                       base_score=200, factor=28.8539, n_iterations=20):

    try:
        print("\nSTEP 1: Preprocessing...")
        main_data = preprocess_data(main_data, target, drop_cols)
        oot_data = preprocess_data(oot_data, target, drop_cols)
        id_main = main_data[id_columns] if id_columns else None
        id_oot = oot_data[id_columns] if id_columns else None
        main_data.drop(columns=id_columns, inplace=True, errors='ignore')
        oot_data.drop(columns=id_columns, inplace=True, errors='ignore')

        print("\nSTEP 2: Feature Selection...")
        final_data, selected_feats = robust_feature_selection(main_data, target, top_n=top_n_features)
        oot_data_filtered = oot_data[selected_feats].dropna()

        print("\nSTEP 3: Algorithm Selection using PyCaret...")
        setup(data=final_data, target=target, session_id=SESSION_ID, silent=True, verbose=False)
        best_model = compare_models()
        best_algo_name = str(best_model).split('(')[0]
        print("Selected Algorithm from PyCaret:", best_algo_name)

        model_class = {
            "CatBoostClassifier": CatBoostClassifier,
            "XGBClassifier": XGBClassifier,
            "LGBMClassifier": LGBMClassifier
        }.get(best_algo_name, CatBoostClassifier)

        print("\nSTEP 4: Model Training (20 Iterations)...")
        X = final_data.drop(columns=[target])
        y = final_data[target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SESSION_ID)

        results = []
        for i in range(n_iterations):
            model = model_class(
                n_estimators=300,
                max_depth=np.random.choice([4, 5, 6]),
                learning_rate=np.random.uniform(0.01, 0.1),
                random_state=SESSION_ID + i,
                verbose=0
            )
            model.fit(X_train, y_train)
            results.append({
                "iteration": i + 1,
                "train_gini": gini_score(y_train, model.predict_proba(X_train)[:, 1]),
                "test_gini": gini_score(y_test, model.predict_proba(X_test)[:, 1]),
                "oot_gini": gini_score(oot_data_filtered[target], model.predict_proba(oot_data_filtered.drop(columns=[target]))[:, 1]),
                "params": model.get_params()
            })

        result_df = pd.DataFrame(results)
        result_df["gini_gap"] = abs(result_df["train_gini"] - result_df["test_gini"])
        best_model_params = result_df.sort_values(by=["gini_gap", "oot_gini"]).iloc[0]["params"]

        print("\nSTEP 5: Final Model Training & Scoring...")
        final_model = model_class(**best_model_params, verbose=0)
        final_model.fit(X, y)

        final_data["model_score"] = generate_score(final_model.predict_proba(X)[:, 1], base_score, factor)
        oot_data_filtered["model_score"] = generate_score(final_model.predict_proba(oot_data_filtered.drop(columns=[target]))[:, 1], base_score, factor)

        print("Train Gini:", gini_score(y, final_model.predict_proba(X)[:, 1]))
        print("OOT Gini :", gini_score(oot_data_filtered[target], final_model.predict_proba(oot_data_filtered.drop(columns=[target]))[:, 1]))
        print("Train KS  :", calculate_ks(y, final_model.predict_proba(X)[:, 1]))
        print("OOT KS   :", calculate_ks(oot_data_filtered[target], final_model.predict_proba(oot_data_filtered.drop(columns=[target]))[:, 1]))

        print("\nSTEP 6: Evaluation & Stability Metrics...")
        decile_train = create_deciles(final_data.copy(), "model_score", target)
        decile_oot = create_deciles(oot_data_filtered.copy(), "model_score", target)
        psi_df = calculate_psi(X, oot_data_filtered.drop(columns=[target, "model_score"]))
        csi_df = calculate_csi(final_data.copy(), oot_data_filtered.copy(), target)

        print("\nSTEP 7: SHAP & Feature Importance...")
        explainer = shap.TreeExplainer(final_model)
        shap_values = explainer.shap_values(X)
        shap.summary_plot(shap_values, X, show=False)
        plt.savefig("shap_summary.png")
        plt.close()

        feat_imp = pd.DataFrame({
            'Feature': X.columns,
            'Importance': final_model.feature_importances_
        }).sort_values(by="Importance", ascending=False)

        print("\nSTEP 8: Saving All Outputs...")
        if id_main is not None:
            final_data = pd.concat([id_main.reset_index(drop=True), final_data.reset_index(drop=True)], axis=1)
        if id_oot is not None:
            oot_data_filtered = pd.concat([id_oot.reset_index(drop=True), oot_data_filtered.reset_index(drop=True)], axis=1)

        final_data.to_csv("main_data_with_scores.csv", index=False)
        oot_data_filtered.to_csv("oot_data_with_scores.csv", index=False)
        result_df.to_excel("model_iterations_gini_log.xlsx", index=False)
        feat_imp.to_excel("feature_importance.xlsx", index=False)
        psi_df.to_excel("psi_report.xlsx", index=False)
        csi_df.to_excel("csi_report.xlsx", index=False)
        decile_train.to_excel("train_decile.xlsx", index=False)
        decile_oot.to_excel("oot_decile.xlsx", index=False)
        joblib.dump(final_model, "final_model.pkl")

        print("\nALL DONE â€” Model and outputs are saved.")
    except Exception as e:
        print(f"PIPELINE FAILED: {str(e)}")


run_model_pipeline(main_data, oot_data, target='mevent', id_columns=['cusid'])