common_columns = list(set(train_data.columns) & set(oot_data.columns))
train_data = train_data[common_columns]
oot_data = oot_data[common_columns]



import numpy as np
import pandas as pd

def csi_numeric(dev_data, rev_data, var):
    """
    Computes CSI (Characteristic Stability Index) for a given numerical variable.
    
    Args:
    dev_data (pd.DataFrame): Development dataset
    rev_data (pd.DataFrame): Review dataset
    var (str): Column name (numeric variable)

    Returns:
    pd.DataFrame: DataFrame with CSI values
    """
    try:
        # Create percentiles based on ranking
        dev_tile = dev_data[var].rank(method='first', pct=True) * 5
        dev_tile = dev_tile.astype(int)
        
        rev_tile = rev_data[var].rank(method='first', pct=True) * 5
        rev_tile = rev_tile.astype(int)

        # Count occurrences in each percentile for both datasets
        dev_counts = dev_tile.value_counts(normalize=True).sort_index()
        rev_counts = rev_tile.value_counts(normalize=True).sort_index()

        # Align indices to ensure consistency
        all_pentiles = sorted(set(dev_counts.index).union(set(rev_counts.index)))
        dev_counts = dev_counts.reindex(all_pentiles, fill_value=0)
        rev_counts = rev_counts.reindex(all_pentiles, fill_value=0)

        # Compute CSI using the formula: (P1 - P2) * ln(P1 / P2)
        csi_values = (dev_counts - rev_counts) * np.log(dev_counts / rev_counts)
        csi_values = csi_values.replace([np.inf, -np.inf, np.nan], 0)  # Handle division errors
        
        # Final CSI value (sum of CSI across pentiles)
        csi_value = csi_values.sum()

        return pd.DataFrame({'feature_name': [var], 'csi': [round(csi_value, 2)]})
    
    except Exception as e:
        print(f"Error processing {var}: {e}")
        return pd.DataFrame({'feature_name': [var], 'csi': [None]})  # Ensure it always returns a DataFrame

def csi(dev_data, rev_data):
    """
    Computes CSI for all numeric variables in the dataset.
    
    Args:
    dev_data (pd.DataFrame): Development dataset
    rev_data (pd.DataFrame): Review dataset

    Returns:
    pd.DataFrame: CSI results for all numeric features
    """
    # Select only numeric columns
    cols_numeric = dev_data.select_dtypes(include=[np.number]).columns
    # Exclude specific columns that are not relevant
    exclude_cols = ["cust_num", "gbflag", "score", "samplingweight", "mevent", "pentile"]
    cols_numeric = [col for col in cols_numeric if col not in exclude_cols]

    CSI_N = []
    for col in cols_numeric:
        CSI_N.append(csi_numeric(dev_data, rev_data, col))

    # Concatenate results
    csi_df = pd.concat(CSI_N, ignore_index=True)

    # Apply observation conditions
    csi_df['observation'] = np.where(
        csi_df['csi'] >= 30, "Impacted the stability of the variable",
        np.where((csi_df['csi'] >= 20) & (csi_df['csi'] < 30), 
                 "Continue and Investigate the variable", 
                 "Working perfectly")
    )

    return csi_df

# Example Usage (Replace with actual DataFrames)
# train_data and oot_data should be actual pandas DataFrames
train_data = pd.DataFrame({
    'var1': np.random.randn(1000),
    'var2': np.random.randn(1000),
    'var3': np.random.randn(1000)
})

oot_data = pd.DataFrame({
    'var1': np.random.randn(1000),
    'var2': np.random.randn(1000),
    'var3': np.random.randn(1000)
})

# Compute CSI
train_oot_csi = csi(train_data, oot_data)
print(train_oot_csi)







# Running PSI Calculation
psi_train_test, total_psi_train_test = psi(scr_train, scr_test)

# Renaming Columns for Readability
psi_train_test = psi_train_test.rename(columns={'score_band': 'score_band', 
                                                'scr_train_perc': 'x_perc', 
                                                'scr_test_perc': 'y_perc'})

# Converting percentages into a 0-100 scale and rounding
psi_train_test['x_perc'] = round(psi_train_test['x_perc'] * 100, 2)
psi_train_test['y_perc'] = round(psi_train_test['y_perc'] * 100, 2)

# Rounding PSI values
psi_train_test['psi'] = round(psi_train_test['psi'], 2)

# Printing Total PSI
print(f"Total PSI (Train vs Test): {round(psi_train_test.psi.sum(), 3)}")

# Printing PSI Table
print("PSI Table (Train vs Test):")
print(psi_train_test)



import numpy as np
import pandas as pd

def psi_table(dev_data, val_data):
    dev_data.columns = dev_data.columns.str.lower()
    val_data.columns = val_data.columns.str.lower()

    if "samplingweight" not in dev_data.columns:
        dev_data["samplingweight"] = 1
    if "samplingweight" not in val_data.columns:
        val_data["samplingweight"] = 1

    # Drop NaN values in score to avoid issues
    dev_data = dev_data.dropna(subset=["score"])
    val_data = val_data.dropna(subset=["score"])

    # Define bin edges dynamically based on percentiles
    min_score = dev_data["score"].min()
    q1 = np.percentile(dev_data["score"], 25)
    q2 = np.percentile(dev_data["score"], 50)  # Median
    q3 = np.percentile(dev_data["score"], 75)
    max_score = dev_data["score"].max()

    bin_edges = [min_score, q1, q2, q3, max_score]  # 5 edges for 4 bins

    # Assign bins and store the exact score ranges
    bin_labels = [f"{int(bin_edges[i])}-{int(bin_edges[i+1])}" for i in range(len(bin_edges) - 1)]
    
    dev_data["pentile"] = pd.cut(dev_data["score"], bins=bin_edges, labels=bin_labels, include_lowest=True)
    val_data["pentile"] = pd.cut(val_data["score"], bins=bin_edges, labels=bin_labels, include_lowest=True)

    # Compute percentage in each bin
    dev_pentile_summary = dev_data.groupby("pentile")["samplingweight"].sum() / dev_data["samplingweight"].sum() * 100
    val_pentile_summary = val_data.groupby("pentile")["samplingweight"].sum() / val_data["samplingweight"].sum() * 100

    # Convert to DataFrame
    pentile_summary = pd.DataFrame({"score_band": dev_pentile_summary.index,
                                    "development_%": dev_pentile_summary.values,
                                    "validation_%": val_pentile_summary.reindex(dev_pentile_summary.index, fill_value=0).values})

    # Calculate PSI
    pentile_summary["psi"] = (pentile_summary["development_%"] - pentile_summary["validation_%"]) * \
                             np.log((pentile_summary["development_%"] + 1e-6) / (pentile_summary["validation_%"] + 1e-6))

    return pentile_summary.round(3)








def psi_table(dev_data, val_data):
    dev_data.columns = dev_data.columns.str.lower()
    val_data.columns = val_data.columns.str.lower()

    if "samplingweight" not in dev_data.columns:
        dev_data["samplingweight"] = 1
    if "samplingweight" not in val_data.columns:
        val_data["samplingweight"] = 1

    # Drop NaN values in score to avoid issues
    dev_data = dev_data.dropna(subset=["score"])
    val_data = val_data.dropna(subset=["score"])

    # Define 5 bin edges to create 4 bins
    min_score = dev_data["score"].min()
    q1 = np.percentile(dev_data["score"], 25)
    q2 = np.percentile(dev_data["score"], 50)  # Median
    q3 = np.percentile(dev_data["score"], 75)
    max_score = dev_data["score"].max()

    bin_edges = [min_score, q1, q2, q3, max_score]
    bin_labels = ["Low", "Middle 1", "Middle 2", "High"]  # 4 labels for 5 edges

    # Assign bins explicitly
    dev_data["pentile"] = pd.cut(dev_data["score"], bins=bin_edges, labels=bin_labels, include_lowest=True)
    val_data["pentile"] = pd.cut(val_data["score"], bins=bin_edges, labels=bin_labels, include_lowest=True)

    # Compute percentage in each bin
    dev_pentile_summary = dev_data.groupby("pentile")["samplingweight"].sum() / dev_data["samplingweight"].sum() * 100
    val_pentile_summary = val_data.groupby("pentile")["samplingweight"].sum() / val_data["samplingweight"].sum() * 100

    # Convert to DataFrame
    pentile_summary = pd.DataFrame({"score_band": dev_pentile_summary.index,
                                    "development_%": dev_pentile_summary.values,
                                    "validation_%": val_pentile_summary.reindex(dev_pentile_summary.index, fill_value=0).values})

    # Calculate PSI
    pentile_summary["psi"] = (pentile_summary["development_%"] - pentile_summary["validation_%"]) * \
                             np.log((pentile_summary["development_%"] + 1e-6) / (pentile_summary["validation_%"] + 1e-6))

    return pentile_summary.round(3)





def psi_table(dev_data, val_data):
    dev_data.columns = dev_data.columns.str.lower()
    val_data.columns = val_data.columns.str.lower()

    if "samplingweight" not in dev_data.columns:
        dev_data["samplingweight"] = 1
    if "samplingweight" not in val_data.columns:
        val_data["samplingweight"] = 1

    # Drop NaN values in score to avoid issues
    dev_data = dev_data.dropna(subset=["score"])
    val_data = val_data.dropna(subset=["score"])

    # Define custom bin edges (ensures "low" and "high" bands always exist)
    min_score = dev_data["score"].min()
    max_score = dev_data["score"].max()
    bin_edges = [min_score, np.percentile(dev_data["score"], 25), 
                 np.percentile(dev_data["score"], 75), max_score]

    # Assign bins explicitly
    dev_data["pentile"] = pd.cut(dev_data["score"], bins=bin_edges, labels=["Low", "Middle 1", "Middle 2", "High"], include_lowest=True)
    val_data["pentile"] = pd.cut(val_data["score"], bins=bin_edges, labels=["Low", "Middle 1", "Middle 2", "High"], include_lowest=True)

    # Compute percentage in each bin
    dev_pentile_summary = dev_data.groupby("pentile")["samplingweight"].sum() / dev_data["samplingweight"].sum() * 100
    val_pentile_summary = val_data.groupby("pentile")["samplingweight"].sum() / val_data["samplingweight"].sum() * 100

    # Convert to DataFrame
    pentile_summary = pd.DataFrame({"score_band": dev_pentile_summary.index,
                                    "development_%": dev_pentile_summary.values,
                                    "validation_%": val_pentile_summary.reindex(dev_pentile_summary.index, fill_value=0).values})

    # Calculate PSI
    pentile_summary["psi"] = (pentile_summary["development_%"] - pentile_summary["validation_%"]) * \
                             np.log((pentile_summary["development_%"] + 1e-6) / (pentile_summary["validation_%"] + 1e-6))

    return pentile_summary.round(3)










def psi_table(dev_data, val_data):
    dev_data.columns = dev_data.columns.str.lower()
    val_data.columns = val_data.columns.str.lower()

    if "samplingweight" not in dev_data.columns:
        dev_data["samplingweight"] = 1
    if "samplingweight" not in val_data.columns:
        val_data["samplingweight"] = 1

    # Ensure there are no NaN scores
    dev_data = dev_data.dropna(subset=["score"])
    val_data = val_data.dropna(subset=["score"])

    dev_data["pentile"] = pd.qcut(dev_data["score"], 5, labels=False, duplicates="drop") + 1

    dev_tile = dev_data.groupby("pentile")['score'].agg(['min', 'max']).reset_index()
    dev_tile.loc[0, "min"] = dev_data["score"].min()  
    dev_tile.loc[4, "max"] = dev_data["score"].max()  

    dev_tile["pentile"] = dev_tile["pentile"] + 1

    dev_pentile_summary = pentile_calculation(dev_data, dev_tile, "development_%")
    val_pentile_summary = pentile_calculation(val_data, dev_tile, "validation_%")

    pentile_summary = pd.merge(dev_tile, dev_pentile_summary, how="inner", on="pentile")
    pentile_summary = pd.merge(pentile_summary, val_pentile_summary, how="inner", on="pentile")

    pentile_summary["score_band"] = pentile_summary["min"].astype(str) + "-" + pentile_summary["max"].astype(str)
    pentile_summary = pentile_summary[["score_band", "development_%", "validation_%"]]

    pentile_summary["psi"] = (pentile_summary["development_%"] - pentile_summary["validation_%"]) * \
                             np.log((pentile_summary["development_%"] + 1e-6) / (pentile_summary["validation_%"] + 1e-6))

    pentile_summary.sort_index(axis=0, ascending=False, inplace=True)

    return pentile_summary.round(3)






import numpy as np
import pandas as pd

# 1ï¸âƒ£ PSI Function
def psi(X, Y):
    X['pentile'] = pd.qcut(X['score'], 5, labels=False) + 1  

    # Define Boundary Conditions
    X_tile = X.groupby("pentile")['score'].agg(['min', 'max']).reset_index()
    X_tile.loc[0, "min"] = -np.inf  
    X_tile.loc[4, "max"] = np.inf  

    # Assign Pentile Bins to Train & Test Data
    X_bins = pd.cut(X['score'], bins=X_tile['min'].tolist() + [X_tile['max'].iloc[-1]], labels=False, include_lowest=True)
    Y_bins = pd.cut(Y['score'], bins=X_tile['min'].tolist() + [X_tile['max'].iloc[-1]], labels=False, include_lowest=True)

    # Compute Normalized Counts
    X_counts = X_bins.value_counts(normalize=True).sort_index()
    Y_counts = Y_bins.value_counts(normalize=True).sort_index()

    # Fill Missing Bins with Small Values
    X_counts = X_counts.reindex(range(5), fill_value=1e-6)
    Y_counts = Y_counts.reindex(range(5), fill_value=1e-6)

    # Compute PSI for Each Bin
    psi_values = (X_counts - Y_counts) * np.log(X_counts / Y_counts)

    # Create PSI Breakdown Table
    psi_df = pd.DataFrame({
        "Score Band": X_tile['min'].astype(str) + " - " + X_tile['max'].astype(str),
        "Train_Percentage": X_counts.values * 100,
        "Test_Percentage": Y_counts.values * 100,
        "PSI": psi_values.values
    })

    # Compute Total PSI
    total_psi = round(psi_values.sum(), 3)

    return psi_df.round(3), total_psi

# 2ï¸âƒ£ PSI Table Function
def psi_table(dev_data, val_data):
    dev_data.columns = dev_data.columns.str.lower()
    val_data.columns = val_data.columns.str.lower()

    if "samplingweight" not in dev_data.columns:
        dev_data["samplingweight"] = 1
    if "samplingweight" not in val_data.columns:
        val_data["samplingweight"] = 1

    dev_data["pentile"] = pd.qcut(dev_data["score"], 5, labels=False) + 1

    dev_tile = dev_data.groupby("pentile")['score'].agg(['min', 'max']).reset_index()
    dev_tile.loc[0, "min"] = -10000  
    dev_tile.loc[4, "max"] = 10000   
    dev_tile["pentile"] = dev_tile["pentile"] + 1

    dev_pentile_summary = pentile_calculation(dev_data, dev_tile, "development_%")
    val_pentile_summary = pentile_calculation(val_data, dev_tile, "validation_%")

    pentile_summary = pd.merge(dev_tile, dev_pentile_summary, how="inner", on="pentile")
    pentile_summary = pd.merge(pentile_summary, val_pentile_summary, how="inner", on="pentile")

    pentile_summary.loc[0, "min"] = "low"
    pentile_summary.loc[4, "max"] = "high"
    pentile_summary["score_band"] = pentile_summary["min"].astype(str) + "-" + pentile_summary["max"].astype(str)
    pentile_summary = pentile_summary[["score_band", "development_%", "validation_%"]]

    pentile_summary["psi"] = (pentile_summary["development_%"] - pentile_summary["validation_%"]) * \
                             np.log(pentile_summary["development_%"] / pentile_summary["validation_%"])

    pentile_summary.sort_index(axis=0, ascending=False, inplace=True)

    return pentile_summary.round(3)

# 3ï¸âƒ£ Pentile Calculation Function
def pentile_calculation(data, tile, col_name):
    data.columns = data.columns.str.lower()

    data["pentile_band"] = pd.cut(data["score"], bins=tile["min"].tolist() + [tile["max"].iloc[-1]], labels=False, include_lowest=True)
    
    pentile_summary = pd.DataFrame(((data.groupby("pentile_band")["samplingweight"].sum() /
                                     data["samplingweight"].sum()) * 100).reset_index())

    pentile_summary.columns = ["pentile", col_name]

    return pentile_summary

# ðŸš€ Running PSI Calculations (No Data Loading)
psi_train_test, total_psi_train_test = psi(scr_train, scr_test)
psi_train_test_table = psi_table(scr_train, scr_test)

# Save Outputs
psi_train_test.to_csv(path + 'psi_train_test.csv', encoding='utf-8', index=False)
psi_train_test_table.to_csv(path + 'psi_train_test_table.csv', encoding='utf-8', index=False)

# Print Results
print(f"Total PSI (Train vs Test): {total_psi_train_test}")
print("PSI Table (Train vs Test):")
print(psi_train_test_table)











import sqlite3
import pandasql as ps

def psi(X, Y):
    conn = sqlite3.connect(":memory:")  # Use an in-memory SQLite DB

    X['pentile'] = pd.qcut(X['score'], 5, labels=False) + 1  

    # Define boundary conditions for bins
    X_tile = pd.DataFrame(X.groupby("pentile").agg({"score": [np.min, np.max]})).reset_index()
    X_tile.columns = ["pentile", "min", "max"]
    X_tile.loc[0, "min"] = -np.inf
    X_tile.loc[4, "max"] = np.inf

    sqlcode = """
        SELECT c.pentile, c.cnt as X_count, c.X_tot, d.cnt as Y_count, d.Y_tot
        FROM (SELECT a.*, b.* FROM 
              (SELECT b.pentile, COUNT(*) AS cnt FROM X a 
               LEFT JOIN X_tile b 
               ON a.score >= b.min AND a.score < b.max 
               GROUP BY b.pentile) a
               CROSS JOIN (SELECT COUNT(*) AS X_tot FROM X) b) c
        LEFT JOIN
             (SELECT a.*, b.* FROM 
              (SELECT b.pentile, COUNT(*) AS cnt FROM Y a 
               LEFT JOIN X_tile b 
               ON a.score >= b.min AND a.score < b.max 
               GROUP BY b.pentile) a
               CROSS JOIN (SELECT COUNT(*) AS Y_tot FROM Y) b) d
        ON c.pentile = d.pentile
    """
    
    # Pass SQLite connection explicitly
    psi_stg0 = ps.sqldf(sqlcode, locals(), conn)  

    psi_stg0['X_perc'] = psi_stg0['X_count'] / psi_stg0['X_tot']
    psi_stg0['Y_perc'] = psi_stg0['Y_count'] / psi_stg0['Y_tot']
    psi_stg0['PSI'] = (psi_stg0['X_perc'] - psi_stg0['Y_perc']) * np.log(psi_stg0['X_perc'] / psi_stg0['Y_perc'])

    psi_stg1 = pd.merge(psi_stg0, X_tile, how="left", left_on="pentile", right_on="pentile")
    psi_stg1["score_band"] = psi_stg1["min"].astype(str) + "-" + psi_stg1["max"].astype(str)

    psi_result = psi_stg1[['score_band', 'X_perc', 'Y_perc', 'PSI']].round(3)
    total_psi = round(psi_result['PSI'].sum(), 3)

    conn.close()  # Close the SQLite connection

    return psi_result, total_psi






nt
import numpy as np
import pandas as pd
import pandasql as ps  # SQL-style DataFrame operations

# 1ï¸âƒ£ PSI Function (Train vs Test)
def psi(X, Y):
    X['pentile'] = pd.qcut(X['score'], 5, labels=False) + 1  

    # Define boundary conditions for bins
    X_tile = pd.DataFrame(X.groupby("pentile").agg({"score": [np.min, np.max]})).reset_index()
    X_tile.columns = ["pentile", "min", "max"]
    X_tile.loc[0, "min"] = -np.inf
    X_tile.loc[4, "max"] = np.inf

    sqlcode = """
        SELECT c.pentile, c.cnt as X_count, c.X_tot, d.cnt as Y_count, d.Y_tot
        FROM (SELECT a.*, b.* FROM 
              (SELECT b.pentile, COUNT(*) AS cnt FROM X a 
               LEFT JOIN X_tile b 
               ON a.score >= b.min AND a.score < b.max 
               GROUP BY b.pentile) a
               CROSS JOIN (SELECT COUNT(*) AS X_tot FROM X) b) c
        LEFT JOIN
             (SELECT a.*, b.* FROM 
              (SELECT b.pentile, COUNT(*) AS cnt FROM Y a 
               LEFT JOIN X_tile b 
               ON a.score >= b.min AND a.score < b.max 
               GROUP BY b.pentile) a
               CROSS JOIN (SELECT COUNT(*) AS Y_tot FROM Y) b) d
        ON c.pentile = d.pentile
    """
    
    psi_stg0 = ps.sqldf(sqlcode, locals())  

    psi_stg0['X_perc'] = psi_stg0['X_count'] / psi_stg0['X_tot']
    psi_stg0['Y_perc'] = psi_stg0['Y_count'] / psi_stg0['Y_tot']
    psi_stg0['PSI'] = (psi_stg0['X_perc'] - psi_stg0['Y_perc']) * np.log(psi_stg0['X_perc'] / psi_stg0['Y_perc'])

    psi_stg1 = pd.merge(psi_stg0, X_tile, how="left", left_on="pentile", right_on="pentile")
    psi_stg1["score_band"] = psi_stg1["min"].astype(str) + "-" + psi_stg1["max"].astype(str)

    psi_result = psi_stg1[['score_band', 'X_perc', 'Y_perc', 'PSI']].round(3)
    total_psi = round(psi_result['PSI'].sum(), 3)

    return psi_result, total_psi

# 2ï¸âƒ£ PSI Table Function (Train vs Test/OOT)
def psi_table(dev_data, val_data):
    dev_data.columns = dev_data.columns.str.lower()
    val_data.columns = val_data.columns.str.lower()

    if "samplingweight" not in dev_data.columns:
        dev_data["samplingweight"] = 1
    if "samplingweight" not in val_data.columns:
        val_data["samplingweight"] = 1

    dev_data["pentile"] = pd.qcut(dev_data["score"], 5, labels=False) + 1

    dev_tile = pd.DataFrame(dev_data.groupby("pentile").agg({"score": [np.min, np.max]})).reset_index()
    dev_tile.columns = ["pentile", "min", "max"]
    dev_tile.loc[0, "min"] = -10000  
    dev_tile.loc[4, "max"] = 10000   
    dev_tile["pentile"] = dev_tile["pentile"] + 1

    dev_pentile_summary = pentile_calculation(dev_data, dev_tile, "development_%")
    val_pentile_summary = pentile_calculation(val_data, dev_tile, "validation_%")

    pentile_summary = pd.merge(dev_tile, dev_pentile_summary, how="inner", on="pentile")
    pentile_summary = pd.merge(pentile_summary, val_pentile_summary, how="inner", on="pentile")

    pentile_summary.loc[0, "min"] = "low"
    pentile_summary.loc[4, "max"] = "high"
    pentile_summary["score_band"] = pentile_summary["min"].astype(str) + "-" + pentile_summary["max"].astype(str)
    pentile_summary = pentile_summary[["score_band", "development_%", "validation_%"]]

    pentile_summary["psi"] = (pentile_summary["development_%"] - pentile_summary["validation_%"]) * \
                             np.log(pentile_summary["development_%"] / pentile_summary["validation_%"])

    pentile_summary.sort_index(axis=0, ascending=False, inplace=True)

    return pentile_summary.round(3)

# 3ï¸âƒ£ Pentile Calculation Function
def pentile_calculation(data, tile, col_name):
    data.columns = data.columns.str.lower()

    data["pentile_band"] = np.where((data["score"] >= tile.loc[0, "min"]) & (data["score"] < tile.loc[0, "max"]), 1,
        np.where((data["score"] >= tile.loc[1, "min"]) & (data["score"] < tile.loc[1, "max"]), 2,
        np.where((data["score"] >= tile.loc[2, "min"]) & (data["score"] < tile.loc[2, "max"]), 3,
        np.where((data["score"] >= tile.loc[3, "min"]) & (data["score"] < tile.loc[3, "max"]), 4,
        np.where((data["score"] >= tile.loc[4, "min"]), 5, 0))))))
    
    pentile_summary = pd.DataFrame(((data.groupby("pentile_band").agg({"samplingweight": ["sum"]}) /
                                     sum(data["samplingweight"])) * 100).reset_index())

    pentile_summary.columns = ["pentile", col_name]

    return pentile_summary

# ðŸš€ How to Use These Functions
path = "/opt/jupyter/notebook/"  # Change path accordingly

# Load Train & Test data
scr_train = pd.read_csv(path + 'scr_train.csv', encoding='cp1252')
scr_test = pd.read_csv(path + 'scr_test.csv', encoding='cp1252')

# Compute PSI between Train & Test
psi_train_test, total_psi_train_test = psi(scr_train, scr_test)

# Save PSI Results
psi_train_test.to_csv(path + 'psi_train_test.csv', encoding='utf-8', index=False)

# Compute PSI Table
psi_train_test_table = psi_table(scr_train, scr_test)

# Save PSI Table
psi_train_test_table.to_csv(path + 'psi_train_test_table.csv', encoding='utf-8', index=False)

# Print PSI Score
print(f"Total PSI (Train vs Test): {total_psi_train_test}")
print("PSI Table (Train vs Test):")
print(psi_train_test_table)










import seaborn as sns
import matplotlib.pyplot as plt

# Reshape data
rank_table = rank_out.melt(id_vars=["score_band"], var_name="cols", value_name="event_rate")

# Create catplot
g = sns.catplot(
    x="score_band", 
    y="event_rate", 
    hue="cols", 
    data=rank_table, 
    kind="bar", 
    height=5, 
    aspect=2
)

# Set title using `fig.suptitle()`
g.fig.suptitle("Rank Ordering - Out of Time", fontsize=14)

# Save figure
g.savefig(path + "Rank_Ordering_OOT.png", dpi=50, bbox_inches='tight')

# Show plot
plt.show()













/* Step 1: Running PROC FREQ to get counts */
PROC FREQ DATA=FINAL_WTRFL_8 NOPRINT;
    TABLE DROP_FLAG_SAL3L / OUT=DROP_FREQ (DROP=PERCENT);
RUN;

/* Step 2: Converting DROP_FLAG_SAL3L to Numeric */
DATA DROP_FREQ_NUM;
    SET DROP_FREQ;
    DROP_FLAG_NUM = INPUT(DROP_FLAG_SAL3L, 8.); /* Convert string to numeric */
    IF DROP_FLAG_NUM = . THEN DELETE; /* Remove invalid values */
    DROP DROP_FLAG_SAL3L;
    RENAME DROP_FLAG_NUM = DROP_FLAG_SAL3L;
RUN;

/* Step 3: Sorting Data */
PROC SORT DATA=DROP_FREQ_NUM;
    BY DROP_FLAG_SAL3L;
RUN;

/* Step 4: Compute Total Base (Initial Count Before Any Drop) */
PROC SQL;
    SELECT SUM(Frequency) INTO :TOTAL_BASE FROM DROP_FREQ_NUM;
QUIT;

/* Step 5: Creating a Full List of Drop Flags (1 to Maximum Found) */
DATA DROP_FULL;
    DO DROP_FLAG_SAL3L = 1 TO 18; /* Ensure all numbers from 1 to 18 are included */
        OUTPUT;
    END;
RUN;

/* Step 6: Merging Original Data with Full Drop Flag List */
DATA DROP_COMPLETE;
    MERGE DROP_FULL (IN=A) DROP_FREQ_NUM (IN=B);
    BY DROP_FLAG_SAL3L;
    IF NOT B THEN Frequency = 0; /* If missing, set frequency to 0 */
RUN;

/* Step 7: Creating Cumulative Reduction Table */
DATA DROP_SUMMARY;
    SET DROP_COMPLETE;
    RETAIN Starting_Customers Remaining_Customers;
    
    /* Set Initial Base using Macro Variable */
    IF _N_ = 1 THEN DO;
        Remaining_Customers = &TOTAL_BASE;
    END;

    /* Set the starting value before subtraction */
    Starting_Customers = Remaining_Customers;

    /* Compute Remaining Customers after each drop */
    Remaining_Customers = Remaining_Customers - Frequency;

    /* Output dataset */
    OUTPUT;
RUN;

/* Step 8: Adding Initial Base Row */
DATA DROP_FINAL;
    SET DROP_SUMMARY;

    /* Insert Base Row at the Start */
    IF _N_ = 1 THEN DO;
        DROP_FLAG_SAL3L = 0; /* Numeric representation for base */
        Frequency = 0;
        Starting_Customers = .; /* No previous value for the base */
        Remaining_Customers = &TOTAL_BASE;
        OUTPUT;
    END;

    OUTPUT;
RUN;

/* Step 9: Display the Final Drop Summary Table */
PROC PRINT DATA=DROP_FINAL NOOBS;
    TITLE "Step-wise Drop Flag Reduction Table (With All Numbers & String to Numeric Conversion)";
RUN;












/* Step 1: Running PROC FREQ to get counts */
PROC FREQ DATA=FINAL_WTRFL_8 NOPRINT;
    TABLE DROP_FLAG_SAL3L / OUT=DROP_FREQ (DROP=PERCENT);
RUN;

/* Step 2: Converting DROP_FLAG_SAL3L to Numeric */
DATA DROP_FREQ_NUM;
    SET DROP_FREQ;
    DROP_FLAG_NUM = INPUT(DROP_FLAG_SAL3L, 8.); /* Convert to Numeric */
    IF DROP_FLAG_NUM = . THEN DELETE; /* Remove invalid values */
    DROP DROP_FLAG_SAL3L;
    RENAME DROP_FLAG_NUM = DROP_FLAG_SAL3L;
RUN;

/* Step 3: Sorting Data */
PROC SORT DATA=DROP_FREQ_NUM;
    BY DROP_FLAG_SAL3L;
RUN;

/* Step 4: Creating a Full List of Drop Flags (1 to Maximum Found) */
DATA DROP_FULL;
    DO DROP_FLAG_SAL3L = 1 TO 18; /* Ensure all numbers from 1 to 18 are included */
        OUTPUT;
    END;
RUN;

/* Step 5: Merging Original Data with Full Drop Flag List */
DATA DROP_COMPLETE;
    MERGE DROP_FULL (IN=A) DROP_FREQ_NUM (IN=B);
    BY DROP_FLAG_SAL3L;
    IF NOT B THEN Frequency = 0; /* If missing, set frequency to 0 */
RUN;

/* Step 6: Creating Cumulative Reduction Table */
DATA DROP_SUMMARY;
    SET DROP_COMPLETE;
    RETAIN Initial_Base Starting_Customers Remaining_Customers;

    /* Initialize the base with the sum of all frequencies */
    IF _N_ = 1 THEN DO;
        CALL SYMPUTX("TOTAL_BASE", SUM(Frequency));
        Initial_Base = &TOTAL_BASE;
        Remaining_Customers = Initial_Base;
    END;

    /* Set the starting value before subtraction */
    Starting_Customers = Remaining_Customers;

    /* Compute Remaining Customers after each drop */
    Remaining_Customers = Remaining_Customers - Frequency;

    /* Output dataset */
    OUTPUT;
RUN;

/* Step 7: Adding Initial Base Row */
DATA DROP_FINAL;
    SET DROP_SUMMARY;

    /* Insert Base Row at the Start */
    IF _N_ = 1 THEN DO;
        DROP_FLAG_SAL3L = 0; /* Numeric representation for base */
        Frequency = 0;
        Starting_Customers = .; /* No previous value for the base */
        Remaining_Customers = &TOTAL_BASE;
        OUTPUT;
    END;

    OUTPUT;
RUN;

/* Step 8: Display the Final Drop Summary Table */
PROC PRINT DATA=DROP_FINAL NOOBS;
    TITLE "Step-wise Drop Flag Reduction Table (With All Numbers & String to Numeric Conversion)";
RUN;













/* Step 1: Running PROC FREQ to get counts */
PROC FREQ DATA=FINAL_WTRFL_8 NOPRINT;
    TABLE DROP_FLAG_SAL3L / OUT=DROP_FREQ (DROP=PERCENT);
RUN;

/* Step 2: Sorting Data to Maintain Order */
PROC SORT DATA=DROP_FREQ;
    BY DROP_FLAG_SAL3L;
RUN;

/* Step 3: Creating a Full List of Drop Flags (1 to Maximum Found) */
DATA DROP_FULL;
    DO DROP_FLAG_SAL3L = 1 TO 18; /* Adjust based on the max value */
        OUTPUT;
    END;
RUN;

/* Step 4: Merging Original Data with Full Drop Flag List */
DATA DROP_COMPLETE;
    MERGE DROP_FULL (IN=A) DROP_FREQ (IN=B);
    BY DROP_FLAG_SAL3L;
    IF NOT B THEN Frequency = 0; /* If missing, set frequency to 0 */
RUN;

/* Step 5: Creating Cumulative Reduction Table */
DATA DROP_SUMMARY;
    SET DROP_COMPLETE;
    RETAIN Initial_Base Starting_Customers Remaining_Customers;

    /* Initialize the base with the sum of all frequencies */
    IF _N_ = 1 THEN DO;
        CALL SYMPUTX("TOTAL_BASE", SUM(Frequency));
        Initial_Base = &TOTAL_BASE;
        Remaining_Customers = Initial_Base;
    END;

    /* Set the starting value before subtraction */
    Starting_Customers = Remaining_Customers;

    /* Compute Remaining Customers after each drop */
    Remaining_Customers = Remaining_Customers - Frequency;

    /* Output dataset */
    OUTPUT;
RUN;

/* Step 6: Adding Initial Base Row */
DATA DROP_FINAL;
    SET DROP_SUMMARY;

    /* Insert Base Row at the Start */
    IF _N_ = 1 THEN DO;
        DROP_FLAG_SAL3L = "Base";
        Frequency = 0;
        Starting_Customers = .; /* No previous value for the base */
        Remaining_Customers = &TOTAL_BASE;
        OUTPUT;
    END;

    OUTPUT;
RUN;

/* Step 7: Display the Final Drop Summary Table */
PROC PRINT DATA=DROP_FINAL NOOBS;
    TITLE "Step-wise Drop Flag Reduction Table (With All Numbers)";
RUN;













filename zipout "/home/your_username/Segmentation_Report_&month_year..zip";

/* Zip the XML file */
x "zip -j &zipout. &excel_file.";

/* Send the email with ZIP attached */
filename outbox email
    to=("business_stakeholders@company.com") 
    from="your_email@company.com"
    subject="Monthly Segmentation Report - &month_year."
    content_type="text/html"
    attach=("&zipout.");

data _null_;
    file outbox;
    put '<html>';
    put '<head><meta content="text/html; charset=ISO-8859-1" http-equiv="content-type"></head>';
    put '<body style="font-size:10pt;font-family:Calibri, sans-serif;">';
    put '<b>Dear Team,</b><br><br>';
    put 'Please find attached the Monthly Segmentation Report.<br>';
    put '<b>Segmentation Data (EOP, EBS, Non-EBS)</b> is in Sheet 1.<br>';
    put '<b>Daily EBS Total</b> (with dates as columns) is in Sheet 2.<br>';
    put '<br><b>Note:</b> This file is updated daily throughout the month.<br>';
    put '<br>Thanks & Regards,<br>';
    put '<b>Business Intelligence Team</b><br>';
    put '</body></html>';
run;











filename xmlout "&excel_file.";

ods tagsets.excelxp file=xmlout style=sasweb 
    options(embedded_titles='yes' 
            sheet_name='Segmentation Data' /* First Sheet */
            suppress_bylines='yes' 
            autofilter='all' 
            embedded_footnotes='yes' 
            absolute_column_width='15,15,15' 
            sheet_interval='none'  
            skip_space='2');  

ods listing close;
ods html close;

/* Export Table 1: Segmentation EOP */
title "Table 1: Total EOP";
proc print data=Segmentation_EOP_Total noobs;
    format Advance GPB Mass_Mkt Premier Total_Sum 12.3; 
run;

/* Add a blank row separator */
ods text=" ";

/* Export Table 2: EBS */
title "Table 2: EBS";
proc print data=Segmentation_EOP_EBS noobs;
    format Advance GPB Mass_Mkt Premier Total_Sum 12.3;
run;

/* Add a blank row separator */
ods text=" ";

/* Export Table 3: Non EBS */
title "Table 3: Non EBS";
proc print data=Segmentation_EOP_NONEBS noobs;
    format Advance GPB Mass_Mkt Premier Total_Sum 12.3;
run;

/* Export Daily EBS Total in Sheet 2 */
ods tagsets.excelxp options(sheet_name='Daily EBS Total' sheet_interval='table');

title "Daily EBS Total (by Date)";
proc print data=Daily_EBS_History noobs;
    format _numeric_ 12.3;
run;

/* Close ODS */
ods tagsets.excelxp close;
ods listing;











/* Step 1: Create Dataset for Today's EBS Total */
data Daily_EBS_Snap;
    set Segmentation_EOP_EBS;
    keep Total_Sum;
    format Total_Sum 12.3;
    retain Date;
    Date = today(); /* Store today's date as a column */
run;

/* Step 2: Load Only Current Month's Data */
data Daily_EBS_History;
    set Daily_EBS_Snap;
    where month(Date) = month(today()) and year(Date) = year(today()); /* Ensures only this month's data */
run;

/* Step 3: Transpose Data to Have Dates as Columns */
proc transpose data=Daily_EBS_History out=Daily_EBS_Transposed(drop=_name_);
    id Date; /* Make date a column */
    var Total_Sum;
run;

/* Step 4: Append Today's Data to the Historical Dataset */
proc append base=Daily_EBS_History data=Daily_EBS_Transposed force;
run;










/* Load Previous Data if it Exists */
data Daily_EBS_History;
    set Daily_EBS_Snap;
run;

/* Create Today's EBS Data with Date as Column */
data Daily_EBS_Snap;
    set Segmentation_EOP_EBS;
    keep Total_Sum;
    format Total_Sum 12.3;
    retain Date;
    Date = today(); /* Store today's date as a column */
run;

/* Transpose Data to Have Dates as Columns */
proc transpose data=Daily_EBS_Snap out=Daily_EBS_Transposed(drop=_name_);
    id Date; /* Make date a column */
    var Total_Sum;
run;

/* Append Today's Data to the Historical Dataset */
proc append base=Daily_EBS_History data=Daily_EBS_Transposed force;
run;

filename xmlout "&excel_file.";

ods tagsets.excelxp file=xmlout style=sasweb 
    options(embedded_titles='yes' 
            sheet_name='Segmentation Data' /* First Sheet */
            suppress_bylines='yes' 
            autofilter='all' 
            embedded_footnotes='yes' 
            absolute_column_width='15,15,15' 
            sheet_interval='none'  
            skip_space='2');  

ods listing close;
ods html close;

/* Export Table 1: Segmentation EOP */
title "Table 1: Total EOP";
proc print data=Segmentation_EOP_Total noobs;
    format Advance GPB Mass_Mkt Premier Total_Sum 12.3; 
run;

/* Add a blank row separator */
ods text=" ";

/* Export Table 2: EBS */
title "Table 2: EBS";
proc print data=Segmentation_EOP_EBS noobs;
    format Advance GPB Mass_Mkt Premier Total_Sum 12.3;
run;

/* Add a blank row separator */
ods text=" ";

/* Export Table 3: Non EBS */
title "Table 3: Non EBS";
proc print data=Segmentation_EOP_NONEBS noobs;
    format Advance GPB Mass_Mkt Premier Total_Sum 12.3;
run;

/* Export Daily EBS Total in Sheet 2 */
ods tagsets.excelxp options(sheet_name='Daily EBS Total' sheet_interval='table');

title "Daily EBS Total (by Date)";
proc print data=Daily_EBS_History noobs;
    format _numeric_ 12.3;
run;

/* Close ODS */
ods tagsets.excelxp close;
ods listing;














ods tagsets.excelxp options(sheet_name='Daily EBS Total' sheet_interval='table');  

/* Create dataset for today's EBS Total */
data Daily_EBS_Snap;
    set Segmentation_EOP_EBS;
    keep Total_Sum;
    format Date date9. Total_Sum 12.3;
    Date = today(); /* Add today's date */
run;

/* Append to the existing Excel file */
proc append base=Daily_EBS_Snap data=Daily_EBS_Snap force;
run;

/* Export the appended Daily EBS data */
proc print data=Daily_EBS_Snap noobs;
run;

/* Close ODS */
ods tagsets.excelxp close;
ods listing;









/* Define the output XML file */
filename xmlout "/approvl/mix_nas/inm/INMDAU/IM/SASBAU/sandbox/Prachi/Segmentation_EOP_Report_20250211.xml";

/* Step 2: Export Three Tables into a Single XML File (One Sheet) */
ods tagsets.excelxp file=xmlout style=sasweb 
    options(embedded_titles='yes' 
            sheet_name='Segmentation EOP' 
            suppress_bylines='yes' 
            autofilter='all' 
            embedded_footnotes='yes' 
            absolute_column_width='15,15,15');

/* Close listings to avoid unwanted output */
ods listing close;
ods html close;

/* Export Table 1: Total EOP */
title "Table 1: Total EOP";
proc print data=work.Segmentation_EOP noobs;
run;

/* Add a blank row as a separator */
data _null_;
    file print;
    put ' ';
run;

/* Export Table 2: EBS */
title "Table 2: EBS";
proc print data=work.Segmentation_EBS noobs;
run;

/* Add a blank row as a separator */
data _null_;
    file print;
    put ' ';
run;

/* Export Table 3: Non EBS */
title "Table 3: Non EBS";
proc print data=work.Segmentation_Non_EBS noobs;
run;

/* Close ODS */
ods tagsets.excelxp close;
ods listing;







Growth_Indicator = 
VAR GrowthValue = SUM('YourTable'[Growth])
RETURN 
    IF(GrowthValue > 0, "â–² " & FORMAT(GrowthValue, "0.00%"), "â–¼ " & FORMAT(GrowthValue, "0.00%"))








DateTable = 
VAR MinDate = MIN('YourTable'[Month_text])
VAR MaxDate = MAX('YourTable'[Month_text])
RETURN
ADDCOLUMNS(
    FILTER(CALENDAR(MinDate, MaxDate), 
           YEAR([Date]) >= YEAR(MinDate) && YEAR([Date]) <= YEAR(MaxDate)),
    "Year", YEAR([Date]),
    "Month", FORMAT([Date], "MMM"),
    "MonthNumber", MONTH([Date])
)











/* Step 1: Define Macros for File Paths */
%let ydate = %sysfunc(today(), yymmddn8.);  

/* Define file paths */
%let xml_file = /home/your_username/Segmentation_EOP_Report_&ydate..xml;
%let zip_file = /home/your_username/Segmentation_EOP_Report_&ydate..zip;

filename xmlout "&xml_file.";

/* Step 2: Export Three Tables into a Single XML File */
ods tagsets.excelxp file=xmlout style=sasweb 
    options(embedded_titles='yes' sheet_name='Sheet1' zoom='75');

ods listing close;
ods html close;

/* Export Table 1: Total EOP */
title "Table 1: Total EOP";
proc print data=work.Total_EOP noobs;
run;

/* Export Table 2: EBS */
title "Table 2: EBS";
proc print data=work.EBS noobs;
run;

/* Export Table 3: Non EBS */
title "Table 3: Non EBS";
proc print data=work.Non_EBS noobs;
run;

/* Close ODS */
ods tagsets.excelxp close;
ods listing;

/* Step 3: Compress the XML File into a ZIP Folder */
filename zipout "&zip_file.";

/* Use X command to create ZIP */
x "zip -j &zip_file. &xml_file.";

/* Step 4: Send Email with ZIP File Attached */
filename outbox email
    to=("business_stakeholders@company.com")  /* Change recipient */
    from="your_email@company.com"             /* Change sender */
    subject="Segmentation EOP Report"
    content_type="text/html"
    attach=("&zip_file.");

data _null_;
    file outbox;
    put '<html>';
    put '<head><meta content="text/html; charset=ISO-8859-1" http-equiv="content-type"></head>';
    put '<body style="font-size:10pt;font-family:Calibri, sans-serif;">';
    put '<b>Dear Business Team,</b><br><br>';
    put 'Please find attached the required segmentation report in a ZIP folder.<br><br>';
    put 'The ZIP file contains an XML report with the following tables:<br>';
    put '<ul>';
    put '<li><b>Table 1: Total EOP</b></li>';
    put '<li><b>Table 2: EBS</b></li>';
    put '<li><b>Table 3: Non EBS</b></li>';
    put '</ul>';
    put '<br>';
    put '<b>Instructions:</b><br>';
    put '1. Download the ZIP file.<br>';
    put '2. Extract the XML file.<br>';
    put '3. Open the XML file in Excel.<br><br>';
    put '<b>Note:</b> This report is classified as <b>HIGHLY RESTRICTED</b>. Do not share externally.<br>';
    put '<br>Thanks & Regards,<br>';
    put '<b>Business Intelligence Team</b><br>';
    put '</body></html>';
run;
















/* Define output file for Excel */
filename xlout "INM-RPT-418-Segmentation_EOP_Report_&ydate..xml";

/* Start ODS TAGSETS.EXCELXP to create Excel-compatible XML */
ods tagsets.excelxp file=xlout style=XLStatistical 
    options(embedded_titles='yes' sheet_name='Segmentation EOP' zoom='75');

ods listing close;
ods html close;

/* Table 1: Total EOP */
title "Table 1: Total EOP";
proc print data=Total_EOP noobs;
run;

/* Table 2: EBS */
title "Table 2: EBS";
proc print data=EBS noobs;
run;

/* Table 3: Non EBS */
title "Table 3: Non EBS";
proc print data=Non_EBS noobs;
run;

/* Close ODS */
ods tagsets.excelxp close;
ods listing;

/* Define Email with Attachment */
filename outbox email
    to=("receiver@domain.com") /* Change recipient */
    from="your_email@domain.com" /* Change sender */
    subject="Segmentation EOP Report"
    content_type="text/html"
    attach=("INM-RPT-418-Segmentation_EOP_Report_&ydate..xml"); /* Attach Excel File */

/* Write Email Body */
data _null_;
    file outbox;
    put '<html>';
    put '<head><meta content="text/html; charset=ISO-8859-1" http-equiv="content-type"></head>';
    put '<body style="font-size:10pt;font-family:Calibri, sans-serif;">';

    put '<b>Dear Team,</b><br><br>';
    put 'Please find attached the required segmentation report in Excel format.<br><br>';

    /* Table Structure in Email Body */
    put '<b>Table 1: Total EOP</b><br>';
    put '<table border="1" cellspacing="0" cellpadding="4">';
    put '<tr bgcolor="#D3D3D3"><th>Column1</th><th>Column2</th><th>Column3</th></tr>'; /* Adjust Columns */
    do until (last);
        set Total_EOP end=last;
        put '<tr><td>' column1 '</td><td>' column2 '</td><td>' column3 '</td></tr>';
    end;
    put '</table><br><br>';

    put '<b>Table 2: EBS</b><br>';
    put '<table border="1" cellspacing="0" cellpadding="4">';
    put '<tr bgcolor="#D3D3D3"><th>Column1</th><th>Column2</th><th>Column3</th></tr>';
    do until (last);
        set EBS end=last;
        put '<tr><td>' column1 '</td><td>' column2 '</td><td>' column3 '</td></tr>';
    end;
    put '</table><br><br>';

    put '<b>Table 3: Non EBS</b><br>';
    put '<table border="1" cellspacing="0" cellpadding="4">';
    put '<tr bgcolor="#D3D3D3"><th>Column1</th><th>Column2</th><th>Column3</th></tr>';
    do until (last);
        set Non_EBS end=last;
        put '<tr><td>' column1 '</td><td>' column2 '</td><td>' column3 '</td></tr>';
    end;
    put '</table><br><br>';

    /* Confidentiality Notice */
    put '<b>Note:</b> Please validate the SQL outputs before usage and revert for any discrepancies.<br>';
    put '<b>Confidentiality Notice:</b> This is a system-generated email, and the data is classified as <b>HIGHLY RESTRICTED</b>. Do not share externally.<br>';
    put '<br>';
    put 'Thanks & Regards,<br>';
    put '<b>Business Intelligence</b><br>';
    put '<b>HSBC Technology and Services - Service Delivery</b><br>';

    put '</body></html>';
run;






filename xlout "INM-RPT-418-Segmentation_EOP_Report_&ydate..xml";

ods tagsets.excelxp file=xlout style=XLStatistical 
    options(embedded_titles='yes' sheet_name='Segmentation EOP' zoom='75');

ods listing close;
ods html close;

title "Table 1: Total EOP";
proc print data=Total_EOP noobs;
run;

title "Table 2: EBS";
proc print data=EBS noobs;
run;

title "Table 3: Non EBS";
proc print data=Non_EBS noobs;
run;

ods tagsets.excelxp close;
ods listing;






filename outbox email
    to=("receiver@domain.com")
    from="your_email@domain.com"
    subject="Segmentation EOP Report"
    content_type="text/html";

data _null_;
    file outbox;
    put '<html>';
    put '<head><meta content="text/html; charset=ISO-8859-1" http-equiv="content-type"></head>';
    put '<body style="font-size:10pt;font-family:Calibri, sans-serif;">';

    put '<b>Dear Team,</b><br><br>';

    /* TOTAL EOP SECTION */
    put '<b>Total EOP</b><br>';
    put '<table border="1" cellspacing="0" cellpadding="4">';
    put '<tr bgcolor="#D3D3D3"><th>Column1</th><th>Column2</th><th>Column3</th></tr>'; /* Adjust Column Names */
    do until (last);
        set Total_EOP end=last;
        put '<tr><td>' column1 '</td><td>' column2 '</td><td>' column3 '</td></tr>'; /* Adjust as per dataset */
    end;
    put '</table><br><br>';

    /* EBS SECTION */
    put '<b>EBS</b><br>';
    put '<table border="1" cellspacing="0" cellpadding="4">';
    put '<tr bgcolor="#D3D3D3"><th>Column1</th><th>Column2</th><th>Column3</th></tr>';
    do until (last);
        set EBS end=last;
        put '<tr><td>' column1 '</td><td>' column2 '</td><td>' column3 '</td></tr>';
    end;
    put '</table><br><br>';

    /* NON-EBS SECTION */
    put '<b>Non EBS</b><br>';
    put '<table border="1" cellspacing="0" cellpadding="4">';
    put '<tr bgcolor="#D3D3D3"><th>Column1</th><th>Column2</th><th>Column3</th></tr>';
    do until (last);
        set Non_EBS end=last;
        put '<tr><td>' column1 '</td><td>' column2 '</td><td>' column3 '</td></tr>';
    end;
    put '</table><br><br>';

    put '<br>';
    put '<b>Note:</b> Please validate the SQL outputs before usage and revert for any discrepancies.<br>';
    put '<b>Confidentiality Notice:</b> This is a system-generated email, and the data is classified as <b>HIGHLY RESTRICTED</b>. Do not share externally.<br>';
    put '<br>';
    put 'Thanks & Regards,<br>';
    put '<b>Business Intelligence</b><br>';
    put '<b>HSBC Technology and Services - Service Delivery</b><br>';

    put '</body></html>';
run;






Cost = 
VAR MessageCount = 
    SWITCH(
        TRUE(),
        Summary1[Channel] = "SMS" && Summary1[NR_Flag] = 0 && Summary1[SampleTextLength] <= 160, 1,
        Summary1[Channel] = "SMS" && Summary1[NR_Flag] = 0 && Summary1[SampleTextLength] > 160 && Summary1[SampleTextLength] <= 320, 2,
        Summary1[Channel] = "SMS" && Summary1[NR_Flag] = 0 && Summary1[SampleTextLength] > 320 && Summary1[SampleTextLength] <= 480, 3,
        Summary1[Channel] = "SMS" && Summary1[NR_Flag] = 0 && Summary1[SampleTextLength] > 480, 4,
        Summary1[Channel] = "SMS" && Summary1[NR_Flag] = 1, 1,
        0
    )

VAR SmsCost = 
    IF(Summary1[NR_Flag] = 0, MessageCount * 0.13, MessageCount * 1.00)

RETURN 
    SWITCH(
        TRUE(),
        Summary1[Channel] = "WhatsApp" && Summary1[Delivered] = 1, 0.88,
        Summary1[Channel] = "Email" && Summary1[Delivered] = 1, 0.2,
        Summary1[Channel] = "SMS", SmsCost,
        0
    )



%let base_path = /sasdata/hsbc/dil/INM/IM/SASBAU/sandbox/Prachi/NonPega_sms/;
%let unzip_path = /sasdata/hsbc/dil/INM/IM/SASBAU/sandbox/Prachi/NonPega_sms_unzipped/;
%let days_back = 45;                      /* 0 to 44 days */
%let size_threshold = 102400;             /* 100KB threshold */
%let recipient_email = your.email@domain.com; /* Update email */

/* Delete previous data */
proc datasets lib=work nolist;
    delete backlog summary;
quit;

/* Initialize dataset */
data backlog;
    length file_date $10 file_status $25 file_size 8 formatted_size $20;
    stop;
run;

%macro check_files;

%do i = 0 %to %eval(&days_back - 1);
    /* Generate date components */
    %let raw_date = %sysfunc(intnx(day, %sysfunc(today()), -&i, b));
    %let check_date = %sysfunc(putn(&raw_date, yymmddd10.));
    
    /* Construct paths */
    %let folder_path = &base_path;
    %let extracted_folder = &unzip_path.&check_date/;
    %let file_name = &check_date..zip;
    %let full_file_path = &folder_path.&file_name;

    /* Check if ZIP file exists */
    data _null_;
        length file_path $512;
        file_path = resolve("&full_file_path");
        file_exists = fileexist(file_path);
        call symputx('file_found', file_exists);
    run;

    %if &file_found %then %do;
        /* UNZIPPING THE FILE */
        x "mkdir -p &extracted_folder"; /* Ensure directory exists */
        x "unzip -o &full_file_path -d &extracted_folder";

        /* Verify extraction */
        data _null_;
            length extracted_path $512;
            extracted_path = resolve("&extracted_folder");
            extracted_exists = fileexist(extracted_path);
            call symputx('extracted_found', extracted_exists);
        run;

        %if &extracted_found %then %do;
            /* 3. File Validation */
            %let valid_zip = 1;
            %let status = Valid;
            %let file_size = 0; /* Ensure file_size is initialized */

            /* Get extracted folder size */
            data _null_;
                infile "du -sb &extracted_folder" pipe;
                input extracted_size 8.;
                call symputx('file_size', extracted_size);
            run;

            %if not &valid_zip %then %let status = Corrupt;
            %if %sysevalf(&file_size < &size_threshold) %then %let status = &status (Below Threshold);

            /* Append file status */
            data temp;
                file_date = "&check_date";
                file_status = "&status";
                file_size = &file_size;
            run;
            proc append base=backlog data=temp; run;
        %end;
        %else %do;
            /* If extraction failed */
            data temp;
                file_date = "&check_date";
                file_status = "Extraction Failed";
                file_size = .;
            run;
            proc append base=backlog data=temp; run;
        %end;
    %end;
    %else %do;
        /* Case: File missing */
        data temp;
            file_date = "&check_date";
            file_status = "Missing";
            file_size = .;
        run;
        proc append base=backlog data=temp; run;
    %end;
%end;
%mend;

%check_files;

/* Convert file size to KB, MB, or GB */
data backlog;
    set backlog;
    if file_size > 1073741824 then formatted_size = catx(" ", round(file_size/1073741824, 0.01), "GB");
    else if file_size > 1048576 then formatted_size = catx(" ", round(file_size/1048576, 0.01), "MB");
    else if file_size > 1024 then formatted_size = catx(" ", round(file_size/1024, 0.01), "KB");
    else if file_size > 0 then formatted_size = catx(" ", file_size, "Bytes");
    else formatted_size = "-";
run;

/* Create summary statistics */
proc sql;
    create table summary as
    select 
        file_date,
        sum(case when file_status = 'Valid' then 1 else 0 end) as Valid_Files,
        sum(case when file_status = 'Missing' then 1 else 0 end) as Missing_Files,
        sum(case when index(file_status, 'Corrupt') > 0 then 1 else 0 end) as Corrupt_Files,
        sum(case when index(file_status, 'Threshold') > 0 then 1 else 0 end) as Size_Warnings
    from backlog
    group by file_date;
quit;

/* Send HTML email report */
filename mail email &recipient_email
    subject="Server File Audit Report - Last &days_back Days"
    type="text/html";

data _null_;
    file mail;
    put '<html><body style="font-family: Arial, sans-serif; margin: 20px;">';
    
    put '<h2 style="color: #2c3e50; border-bottom: 2px solid #3498db;">';
    put "Server File Audit Report";
    put '</h2>';
    
    put '<div style="margin-bottom: 30px;">';
    put "<p><strong>Base Path:</strong> &base_path</p>";
    put "<p><strong>Report Period:</strong> Last &days_back Days</p>";
    put "<p><strong>Generated:</strong> %sysfunc(datetime(), datetime20.)</p>";
    put '</div>';
    
    put '<h3 style="color: #34495e;">Detailed Report</h3>';
    put '<table border="1" style="border-collapse: collapse; width: 100%; margin-bottom: 30px;">';
    put '<tr style="background-color: #3498db; color: white;">';
    put '<th>Date</th><th>File Status</th><th>File Size</th></tr>';
    
    do until (eof);
        set backlog end=eof;
        put '<tr>';
        put '<td style="padding: 8px;">' file_date '</td>';
        put '<td style="padding: 8px;">' file_status '</td>';
        put '<td style="padding: 8px;">' formatted_size '</td>';
        put '</tr>';
    end;
    
    put '</table>';
    
    put '<h3 style="color: #34495e;">Summary Statistics</h3>';
    put '<table border="1" style="border-collapse: collapse; width: 100%;">';
    put '<tr style="background-color: #3498db; color: white;">';
    put '<th>Date</th><th>Valid Files</th>';
    put '<th>Missing Files</th><th>Corrupt Files</th><th>Size Warnings</th></tr>';
    
    do until (eof2);
        set summary end=eof2;
        put '<tr>';
        put '<td style="padding: 8px;">' file_date '</td>';
        put '<td style="padding: 8px; text-align: center;">' Valid_Files '</td>';
        put '<td style="padding: 8px; text-align: center;">' Missing_Files '</td>';
        put '<td style="padding: 8px; text-align: center;">' Corrupt_Files '</td>';
        put '<td style="padding: 8px; text-align: center;">' Size_Warnings '</td>';
        put '</tr>';
    end;
    
    put '</table>';
    put '</body></html>';
run;

/* DELETE EXTRACTED FILES AFTER PROCESSING */
x "rm -rf &unzip_path";








%let base_path = /sasdata/hsbc/dil/INM/IM/SASBAU/sandbox/Prachi/NonPega_sms/;
%let unzip_path = /sasdata/hsbc/dil/INM/IM/SASBAU/sandbox/Prachi/NonPega_sms_unzipped/;
%let days_back = 45;                      /* 0 to 44 days */
%let size_threshold = 102400;             /* 100KB threshold */
%let recipient_email = your.email@domain.com; /* Update email */

/* Delete previous data */
proc datasets lib=work nolist;
    delete backlog summary;
quit;

/* Initialize dataset */
data backlog;
    length file_date $10 folder_status $20 file_status $25 file_size 8;
    stop;
run;

%macro check_files;

%do i = 0 %to %eval(&days_back - 1);
    /* Generate date components */
    %let raw_date = %sysfunc(intnx(day, %sysfunc(today()), -&i, b));
    %let check_date = %sysfunc(putn(&raw_date, yymmddd10.));
    
    /* Construct paths */
    %let folder_path = &base_path;
    %let extracted_folder = &unzip_path.&check_date/;
    %let file_name = &check_date..zip;
    %let full_file_path = &folder_path.&file_name;

    /* Debugging output */
    %put #########################################;
    %put Checking: &folder_path;
    %put Looking for: &file_name;
    
    /* Check if ZIP file exists */
    data _null_;
        length file_path $512;
        file_path = resolve("&full_file_path");
        file_exists = fileexist(file_path);
        call symputx('file_found', file_exists);
    run;

    %if &file_found %then %do;
        /* UNZIPPING THE FILE */
        x "mkdir -p &extracted_folder"; /* Ensure directory exists */
        x "unzip -o &full_file_path -d &extracted_folder";

        /* Verify extraction */
        data _null_;
            length extracted_path $512;
            extracted_path = resolve("&extracted_folder");
            extracted_exists = fileexist(extracted_path);
            call symputx('extracted_found', extracted_exists);
        run;

        %if &extracted_found %then %do;
            /* 3. File Validation */
            %let valid_zip = 1;
            %let status = Valid;
            %let file_size = 0; /* Ensure file_size is initialized */

            data _null_;
                length extracted_files $512;
                extracted_files = resolve("&extracted_folder/*");
                file_count = fileexist(extracted_files);
                if file_count = 0 then call symputx('valid_zip', 0);
            run;

            /* Check extracted folder size */
            data _null_;
                infile "du -sb &extracted_folder" pipe;
                input extracted_size 8.;
                call symputx('file_size', extracted_size);
            run;

            %if not &valid_zip %then %let status = Corrupt;
            %if %sysevalf(&file_size < &size_threshold) %then %let status = &status (Below Threshold);

            /* Append file status */
            data temp;
                file_date = "&check_date";
                folder_status = "Exists";
                file_status = "&status";
                file_size = &file_size;
            run;
            proc append base=backlog data=temp; run;
        %end;
        %else %do;
            /* If extraction failed */
            data temp;
                file_date = "&check_date";
                folder_status = "Exists";
                file_status = "Extraction Failed";
                file_size = .;
            run;
            proc append base=backlog data=temp; run;
        %end;
    %end;
    %else %do;
        /* Case: File missing */
        data temp;
            file_date = "&check_date";
            folder_status = "Exists";
            file_status = "Missing";
            file_size = .;
        run;
        proc append base=backlog data=temp; run;
    %end;
%end;
%mend;

%check_files;

/* Create summary statistics */
proc sql;
    create table summary as
    select 
        file_date,
        sum(case when folder_status = 'Exists' then 1 else 0 end) as Folders_Exist,
        sum(case when file_status = 'Valid' then 1 else 0 end) as Valid_Files,
        sum(case when file_status = 'Missing' then 1 else 0 end) as Missing_Files,
        sum(case when index(file_status, 'Corrupt') > 0 then 1 else 0 end) as Corrupt_Files,
        sum(case when index(file_status, 'Threshold') > 0 then 1 else 0 end) as Size_Warnings
    from backlog
    group by file_date;
quit;

/* Send HTML email report */
filename mail email &recipient_email
    subject="Server File Audit Report - Last &days_back Days"
    type="text/html";

data _null_;
    file mail;
    put '<html><body style="font-family: Arial, sans-serif; margin: 20px;">';
    
    put '<h2 style="color: #2c3e50; border-bottom: 2px solid #3498db;">';
    put "Server File Audit Report";
    put '</h2>';
    
    put '<div style="margin-bottom: 30px;">';
    put "<p><strong>Base Path:</strong> &base_path</p>";
    put "<p><strong>Report Period:</strong> Last &days_back Days</p>";
    put "<p><strong>Generated:</strong> %sysfunc(datetime(), datetime20.)</p>";
    put '</div>';
    
    put '<h3 style="color: #34495e;">Summary Statistics</h3>';
    put '<table border="1" style="border-collapse: collapse; width: 100%;">';
    put '<tr style="background-color: #3498db; color: white;">';
    put '<th>Date</th><th>Folders Exist</th><th>Valid Files</th>';
    put '<th>Missing Files</th><th>Corrupt Files</th><th>Size Warnings</th></tr>';
    
    do until (eof2);
        set summary end=eof2;
        put '<tr>';
        put '<td style="padding: 8px;">' file_date '</td>';
        put '<td style="padding: 8px; text-align: center;">' Folders_Exist '</td>';
        put '<td style="padding: 8px; text-align: center;">' Valid_Files '</td>';
        put '<td style="padding: 8px; text-align: center;">' Missing_Files '</td>';
        put '<td style="padding: 8px; text-align: center;">' Corrupt_Files '</td>';
        put '<td style="padding: 8px; text-align: center;">' Size_Warnings '</td>';
        put '</tr>';
    end;
    
    put '</table>';
    put '</body></html>';
run;

/* DELETE EXTRACTED FILES AFTER PROCESSING */
x "rm -rf &unzip_path";
