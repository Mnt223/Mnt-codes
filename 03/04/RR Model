import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import shap
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from pycaret.classification import *
import logging

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

@dataclass
class ModelConfig:
    """Configuration settings for the ML pipeline."""
    target_column: str
    train_size: float = 0.7
    random_state: int = 42
    correlation_threshold: float = 0.85
    vif_threshold: float = 5
    feature_selection_threshold: float = 0.005
    important_business_features: List[str] = None

class FeatureProcessor:
    """Handles feature processing and selection."""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.label_encoders = {}
        self.numeric_features = []
        self.categorical_features = []
        
    def identify_feature_types(self, data: pd.DataFrame) -> Tuple[List[str], List[str]]:
        """Identify numeric and categorical features in the dataset."""
        self.numeric_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
        self.categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()
        return self.numeric_features, self.categorical_features
    
    def handle_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:
        """Fill missing values in the dataset."""
        df = data.copy()
        for col in self.numeric_features:
            df[col] = df[col].fillna(df[col].median())
        for col in self.categorical_features:
            df[col] = df[col].fillna(df[col].mode()[0])
        return df
    
    def encode_categorical_variables(self, data: pd.DataFrame) -> pd.DataFrame:
        """Encode categorical variables using LabelEncoder."""
        df = data.copy()
        for col in self.categorical_features:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
            self.label_encoders[col] = le
        return df
    
    def remove_correlated_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Remove highly correlated features."""
        df = data.copy()
        correlation_matrix = df[self.numeric_features].corr().abs()
        upper_triangle = correlation_matrix.where(
            np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
        )
        highly_correlated = [
            column for column in upper_triangle.columns 
            if any(upper_triangle[column] > self.config.correlation_threshold)
        ]
        return df.drop(columns=highly_correlated)
    
    def calculate_vif(self, data: pd.DataFrame) -> pd.DataFrame:
        """Calculate Variance Inflation Factor and remove high VIF features."""
        df = data.copy()
        X = add_constant(df[self.numeric_features])
        vif_data = pd.DataFrame()
        vif_data["Feature"] = X.columns
        vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
        low_vif_features = vif_data[vif_data["VIF"] < self.config.vif_threshold]["Feature"].tolist()
        return df[low_vif_features]

class ModelBuilder:
    """Handles model building, evaluation, and visualization."""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.final_model = None
        self.selected_features = []
        
    def setup_pycaret(self, data: pd.DataFrame) -> None:
        """Set up PyCaret environment."""
        clf = setup(
            data=data,
            target=self.config.target_column,
            train_size=self.config.train_size,
            remove_multicollinearity=True,
            multicollinearity_threshold=self.config.correlation_threshold,
            feature_selection=True,
            feature_selection_threshold=self.config.feature_selection_threshold,
            ignore_low_variance=True,
            remove_outliers=False,
            silent=True,
            session_id=self.config.random_state
        )
        self.selected_features = get_config('X_train').columns.tolist()
        
    def train_model(self) -> None:
        """Train and tune the model."""
        self.final_model = compare_models(sort='AUC', n_select=1)
        self.final_model = tune_model(self.final_model, optimize='AUC', n_iter=10)
        self.final_model = finalize_model(self.final_model)
        
    def generate_feature_importance(self, output_dir: Path) -> None:
        """Generate and save feature importance analysis."""
        feature_imp = pd.DataFrame(
            self.final_model.feature_importances_,
            index=self.selected_features,
            columns=['Importance']
        )
        feature_imp = feature_imp.sort_values(by="Importance", ascending=False)
        feature_imp.to_csv(output_dir / "feature_importance.csv", index=True)
        
        # SHAP analysis
        explainer = shap.TreeExplainer(self.final_model)
        shap_values = explainer.shap_values(get_config('X_train'))
        plt.figure()
        shap.summary_plot(shap_values, get_config('X_train'))
        plt.savefig(output_dir / "shap_feature_importance.png")
        plt.close()
        
    def generate_evaluation_plots(self, output_dir: Path) -> None:
        """Generate and save model evaluation plots."""
        plot_types = ['auc', 'pr', 'feature']
        for plot_type in plot_types:
            plt.figure()
            plot_model(self.final_model, plot=plot_type)
            plt.savefig(output_dir / f"{plot_type}_plot.png")
            plt.close()
            
    def generate_gains_chart(self, output_dir: Path) -> None:
        """Generate and save gains chart."""
        gains_df = get_config('y_test').reset_index(drop=True)
        gains_df['prediction'] = predict_model(self.final_model, data=get_config('X_test'))['Score']
        gains_df['Decile'] = pd.qcut(gains_df['prediction'], q=10, labels=False, duplicates='drop')
        
        plt.figure(figsize=(10, 6))
        sns.barplot(
            x=gains_df.groupby("Decile")["prediction"].mean().index,
            y=gains_df.groupby("Decile")["prediction"].mean().values
        )
        plt.title("Gains Chart (Decile Analysis)")
        plt.xlabel("Decile")
        plt.ylabel("Mean Prediction Score")
        plt.savefig(output_dir / "gains_chart.png")
        plt.close()

def main():
    # Create output directory
    output_dir = Path("model_output")
    output_dir.mkdir(exist_ok=True)
    
    # Initialize configuration
    config = ModelConfig(
        target_column='mevent',
        important_business_features=['cust_tenure', 'credit_card_limit', 'account_balance']
    )
    
    try:
        # Load data
        logging.info("Loading data...")
        data = pd.read_csv("your_data.csv")
        
        # Initialize processors
        feature_processor = FeatureProcessor(config)
        model_builder = ModelBuilder(config)
        
        # Process features
        logging.info("Processing features...")
        feature_processor.identify_feature_types(data)
        data = feature_processor.handle_missing_values(data)
        data = feature_processor.encode_categorical_variables(data)
        data = feature_processor.remove_correlated_features(data)
        data = feature_processor.calculate_vif(data)
        
        # Save all features before selection
        pd.DataFrame({'Features': data.columns.tolist()}).to_csv(
            output_dir / "all_features.csv",
            index=False
        )
        
        # Build and evaluate model
        logging.info("Building model...")
        model_builder.setup_pycaret(data)
        model_builder.train_model()
        
        # Generate visualizations and save results
        logging.info("Generating visualizations and saving results...")
        model_builder.generate_feature_importance(output_dir)
        model_builder.generate_evaluation_plots(output_dir)
        model_builder.generate_gains_chart(output_dir)
        
        # Save model
        save_model(model_builder.final_model, output_dir / "final_model")
        
        logging.info("Pipeline completed successfully!")
        
    except Exception as e:
        logging.error(f"Error in pipeline: {str(e)}")
        raise

if __name__ == "__main__":
    main()









import pandas as pd

# Ensure 'mevent' is an integer
frequency_table['mevent'] = frequency_table['mevent'].astype(int)

# Use pivot_table instead of pivot to handle duplicate entries
frequency_tab = frequency_table.pivot_table(
    index='selection_month', 
    columns='mevent', 
    values='total',
    aggfunc='sum',  # Aggregate sum in case of duplicate rows
    fill_value=0  # Replace missing values with 0
).reset_index()

# Rename columns properly
frequency_tab.columns.name = None  # Remove pivot_table auto-generated column name
frequency_tab.rename(columns={0: 'Non Event', 1: 'Event'}, inplace=True)

# Compute Total Base
frequency_tab['Total Base'] = frequency_tab['Non Event'] + frequency_tab['Event']

# Compute Event Rate
frequency_tab['Event Rate in %'] = ((frequency_tab['Event'] * 100) / frequency_tab['Total Base']).round(2)

# Convert Selection Month to string
frequency_tab['Selection Month'] = frequency_tab['Selection Month'].astype(str)

# Display the cleaned dataframe
import ace_tools as tools
tools.display_dataframe_to_user(name="Frequency Table", dataframe=frequency_tab)




import pandas as pd

# Ensure 'mevent' is an integer
frequency['mevent'] = frequency['mevent'].astype(int)

# Aggregate total sum for each 'selection_month' and 'mevent' to remove duplicates
frequency_table = frequency.groupby(['selection_month', 'mevent'], as_index=False).agg({'total': 'sum'})

# Debugging step: Check for issues before pivoting
print(frequency_table.head())  # Check structure before pivot
print(frequency_table.dtypes)  # Ensure 'mevent' and 'selection_month' are correct

# Use pivot instead of pivot_table to avoid unstack() errors
frequency_tab = frequency_table.pivot(
    index='selection_month', 
    columns='mevent', 
    values='total'
).fillna(0).reset_index()

# Rename columns properly
frequency_tab.columns.name = None  # Remove pivot_table auto-generated column name
frequency_tab.rename(columns={0: 'Non Event', 1: 'Event'}, inplace=True)

# Compute Total Base
frequency_tab['Total Base'] = frequency_tab['Non Event'] + frequency_tab['Event']

# Compute Event Rate
frequency_tab['Event Rate in %'] = ((frequency_tab['Event'] * 100) / frequency_tab['Total Base']).round(2)

# Convert Selection Month to string
frequency_tab['Selection Month'] = frequency_tab['Selection Month'].astype(str)

# Display the cleaned dataframe
import ace_tools as tools
tools.display_dataframe_to_user(name="Frequency Table", dataframe=frequency_tab)








m
import pandas as pd

# Ensure 'mevent' is an integer
frequency['mevent'] = frequency['mevent'].astype(int)

# Aggregate total sum for each 'selection_month' and 'mevent' to remove duplicates
frequency_table = frequency.groupby(['selection_month', 'mevent'], as_index=False).agg({'total': 'sum'})

# Debugging step: Check for issues before pivoting
print(frequency_table.head())  # Check structure before pivot
print(frequency_table.dtypes)  # Ensure 'mevent' and 'selection_month' are correct

# Use pivot instead of pivot_table to avoid unstack() errors
frequency_tab = frequency_table.pivot(
    index='selection_month', 
    columns='mevent', 
    values='total'
).fillna(0).reset_index()

# Rename columns properly
frequency_tab.columns.name = None  # Remove pivot_table auto-generated column name
frequency_tab.rename(columns={0: 'Non Event', 1: 'Event'}, inplace=True)

# Compute Total Base
frequency_tab['Total Base'] = frequency_tab['Non Event'] + frequency_tab['Event']

# Compute Event Rate
frequency_tab['Event Rate in %'] = ((frequency_tab['Event'] * 100) / frequency_tab['Total Base']).round(2)

# Convert Selection Month to string
frequency_tab['Selection Month'] = frequency_tab['Selection Month'].astype(str)

# Display the cleaned dataframe
import ace_tools as tools
tools.display_dataframe_to_user(name="Frequency Table", dataframe=frequency_tab)




import pandas as pd

# Assuming 'raw' is your original dataframe
frequency = raw[raw['final_cus_seg'] != 'Card Only'].groupby(
    ['selection_month', 'samplingweight', 'mevent'],
    as_index=False
).agg({'gbflag': 'sum'})

# Compute total based on sampling weight
frequency['total'] = frequency['samplingweight'] * frequency['gbflag']

# Aggregate by selection_month and mevent
frequency_table = frequency.groupby(['selection_month', 'mevent'], as_index=False).agg({'total': 'sum'})

# Convert total column to int
frequency_table['total'] = frequency_table['total'].astype('int64')

# Create pivot table
frequency_tab = frequency_table.pivot_table(
    index=['selection_month'],
    columns='mevent',
    values='total',
    aggfunc='sum',
    fill_value=0
).reset_index()

# Rename columns properly
frequency_tab.columns = ['Selection Month', 'Non Event', 'Event']

# Compute total base
frequency_tab['Total Base'] = frequency_tab['Non Event'] + frequency_tab['Event']

# Compute event rate in percentage
frequency_tab['Event Rate in %'] = ((frequency_tab['Event'] * 100) / frequency_tab['Total Base']).round(decimals=2)

# Ensure Selection Month is a string
frequency_tab['Selection Month'] = frequency_tab['Selection Month'].astype(str)

# Save to CSV
frequency_tab.to_csv('frequency_tab.csv', encoding='utf-8', index=False)

# Display result
import ace_tools as tools
tools.display_dataframe_to_user(name="Frequency Table", dataframe=frequency_tab)



# Take intersection of all feature selection methods
final_selected_features = list(set(selected_features_rfe) & set(selected_features_mi) & set(selected_features_shap))

# Keep only final selected features
raw = raw[['mevent'] + final_selected_features]
print(f"Final selected features: {len(final_selected_features)}")








from catboost import CatBoostClassifier
import shap

def select_features_by_shap(X, y, model=None, n_features=30):
    """
    Selects the top N features based on SHAP values.
    """
    if model is None:
        model = CatBoostClassifier(iterations=100, depth=5, learning_rate=0.1, silent=True)
        model.fit(X, y)

    explainer = shap.Explainer(model, X)
    shap_values = explainer(X)

    shap_importance = np.abs(shap_values.values).mean(axis=0)
    shap_df = pd.DataFrame({'Feature': X.columns, 'SHAP Value': shap_importance}).sort_values(by='SHAP Value', ascending=False)

    selected_features = shap_df['Feature'].head(n_features).tolist()
    print(f"Selected {len(selected_features)} features using SHAP Importance.")
    
    return selected_features

# Apply SHAP Selection
selected_features_shap = select_features_by_shap(raw[num_features], raw['mevent'], n_features=50)

# Keep only selected features
raw = raw[['mevent'] + selected_features_shap]








from sklearn.feature_selection import mutual_info_classif

def select_features_by_mutual_info(X, y, n_features=30):
    """
    Selects features based on Mutual Information Score.
    """
    mi_scores = mutual_info_classif(X, y, discrete_features=False)
    mi_df = pd.DataFrame({'Feature': X.columns, 'MI Score': mi_scores}).sort_values(by='MI Score', ascending=False)

    selected_features = mi_df['Feature'].head(n_features).tolist()
    print(f"Selected {len(selected_features)} features using Mutual Information.")
    
    return selected_features

# Apply Mutual Information Selection
selected_features_mi = select_features_by_mutual_info(raw[num_features], raw['mevent'], n_features=50)

# Keep only selected features
raw = raw[['mevent'] + selected_features_mi]













from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

def feature_selection_rfe(X, y, n_features=30):
    """
    Applies Recursive Feature Elimination (RFE) to select the top N features.
    """
    model = RandomForestClassifier(n_estimators=100, random_state=42)

    selector = RFE(model, n_features_to_select=n_features, step=1)
    selector.fit(X, y)

    selected_features = X.columns[selector.support_].tolist()
    print(f"Selected {len(selected_features)} features using RFE.")
    
    return selected_features

# Ensure correct numeric feature selection
num_features = raw.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Apply RFE on numeric features
selected_features_rfe = feature_selection_rfe(raw[num_features], raw['mevent'], n_features=50)

# Keep only selected features
raw = raw[['mevent'] + selected_features_rfe]










from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

def drop_high_vif_features(df, threshold=10):
    """
    Drops features with a high Variance Inflation Factor (VIF) score.
    """
    # Ensure only numeric columns are used
    numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

    # Ensure columns exist before applying VIF
    if not numeric_features:
        print("No numeric features found for VIF calculation.")
        return df

    X = add_constant(df[numeric_features])  # Add constant term for VIF calculation

    # Calculate VIF for each feature
    vif_data = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)

    # Identify features with VIF above threshold
    to_drop = vif_data[vif_data > threshold].index.tolist()

    print(f"Dropping {len(to_drop)} features with VIF > {threshold}: {to_drop}")

    return df.drop(columns=to_drop, errors="ignore")  # Avoid KeyError

# Apply VIF reduction
raw = drop_high_vif_features(raw)












# Import necessary libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import RFE, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from catboost import CatBoostClassifier
from pycaret.classification import setup, compare_models, tune_model, get_config, create_model, predict_model
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import shap

# Define paths
data_path = "/opt/jupyter/notebook/RR_Model_Development_Data.csv"
output_path = "/opt/jupyter/notebook/notebooks/jupyter/Output_RR/"
if not os.path.exists(output_path):
    os.makedirs(output_path)

# Load Data
raw = pd.read_csv(data_path, encoding="cp1252")
raw.columns = [x.lower() for x in raw.columns]
raw = raw.rename(columns={'custid': 'cust_num'})

# Drop columns with more than 80% missing values
missing_threshold = 0.8
missing_fraction = raw.isnull().sum() / len(raw)
cols_to_drop_missing = missing_fraction[missing_fraction > missing_threshold].index.tolist()
raw = raw.drop(columns=cols_to_drop_missing)

# Drop columns with very low variance
low_variance_cols = raw.var().loc[lambda x: x < 0.01].index.tolist()
raw = raw.drop(columns=low_variance_cols)

# Fill missing values
raw.fillna(raw.mode().iloc[0], inplace=True)

# Convert customer ID to string and standardize length
raw['cust_num'] = raw['cust_num'].astype(str).apply(lambda x: x.zfill(9))

# Identify categorical and numerical variables
cat_features = list(raw.select_dtypes(include=['object']).columns)
num_features = list(raw.select_dtypes(include=['int64', 'float64']).columns)

# Remove target variable from num_features list
num_features.remove('mevent')

### 1. **Correlation-based Feature Selection**
def drop_correlated_features(df, threshold=0.9):
    corr_matrix = df.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]
    return df.drop(columns=to_drop)

raw = drop_correlated_features(raw, threshold=0.9)

### 2. **Variance Inflation Factor (VIF)**
def drop_high_vif_features(df, threshold=10):
    X = add_constant(df)
    vif_data = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
    to_drop = vif_data[vif_data > threshold].index.tolist()
    return df.drop(columns=to_drop)

raw = drop_high_vif_features(raw[num_features])

### 3. **Recursive Feature Elimination (RFE)**
# Use Logistic Regression or Random Forest to select top features
def feature_selection_rfe(X, y, n_features=30):
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    selector = RFE(model, n_features_to_select=n_features, step=1)
    selector.fit(X, y)
    selected_features = X.columns[selector.support_].tolist()
    return selected_features

selected_features_rfe = feature_selection_rfe(raw[num_features], raw['mevent'], n_features=50)

### 4. **Mutual Information Gain**
def select_features_by_mutual_info(X, y, n_features=30):
    mi_scores = mutual_info_classif(X, y, discrete_features=False)
    mi_df = pd.DataFrame({'Feature': X.columns, 'MI Score': mi_scores}).sort_values(by='MI Score', ascending=False)
    return mi_df['Feature'].head(n_features).tolist()

selected_features_mi = select_features_by_mutual_info(raw[num_features], raw['mevent'], n_features=50)

### 5. **SHAP Feature Importance**
def select_features_by_shap(X, y, model=None, n_features=30):
    if model is None:
        model = CatBoostClassifier(iterations=100, depth=5, learning_rate=0.1, silent=True)
        model.fit(X, y)
    
    explainer = shap.Explainer(model, X)
    shap_values = explainer(X)
    shap_importance = np.abs(shap_values.values).mean(axis=0)
    shap_df = pd.DataFrame({'Feature': X.columns, 'SHAP Value': shap_importance}).sort_values(by='SHAP Value', ascending=False)
    return shap_df['Feature'].head(n_features).tolist()

selected_features_shap = select_features_by_shap(raw[num_features], raw['mevent'], n_features=50)

# **Final Selected Features (Intersection of All Methods)**
final_selected_features = list(set(selected_features_rfe) & set(selected_features_mi) & set(selected_features_shap))
raw = raw[['mevent'] + final_selected_features]

# PyCaret Model Training
clf = setup(
    raw, 
    target='mevent',
    numeric_imputation='mean',
    categorical_imputation='mode',
    remove_outliers=True,
    train_size=0.7,
    session_id=1992
)

# Train the best model
best_model = compare_models(fold=5, round=2, sort='AUC', n_select=1)

# Tune the best model
tuned_model = tune_model(best_model, fold=5, optimize='AUC')

# Save Feature Importance
feature_importance = pd.DataFrame({'Feature': final_selected_features, 'Importance': tuned_model.feature_importances_})
feature_importance.to_csv(output_path + "final_feature_importance.csv", index=False)

# Save the final dataset with selected features
raw.to_csv(output_path + "final_selected_data.csv", index=False)
