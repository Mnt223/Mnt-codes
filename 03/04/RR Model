u
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import shap
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from catboost import Pool, CatBoostClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve
from pycaret.classification import *
import matplotlib.patches as patches
import matplotlib.ticker as mtick
import lime.lime_tabular
from scipy import stats
import pandasql as ps
import os
import json

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

# Install necessary packages
get_ipython().system('pip install --upgrade pandas')
get_ipython().system('pip install seaborn --upgrade')

# Check versions
sns._version_
pd._version_

# Copy dataset from Hadoop
get_ipython().system('hadoop fs -copyToLocal gs://RR_Model_Development_Data.csv /opt/jupyter/notebook/')

# Create output directory
if not os.path.exists('Output_RR'):
    os.makedirs('Output_RR')

path = os.path.join(os.getcwd(), 'Output_RR/')
path
data_path = "/opt/jupyter/notebook/RR_Model_Development_Data.csv"

# Load dataset
raw = pd.read_csv(data_path, encoding='cp1252')
raw.columns = [x.lower() for x in raw.columns]
raw = raw.rename(columns={'custid': 'cust_num'})

print(raw.shape)
print(raw['selection_month'].unique())
print(raw['mevent'].value_counts())
raw.head()

# Process customer numbers
raw['cust_num'] = raw['cust_num'].astype(str).apply(lambda x: x.zfill(9))
raw.head()

# Frequency table generation
frequency = raw.groupby(['selection_month', 'samplingweight', 'mevent'], as_index=False).agg({'gbflag': len})
frequency['total'] = frequency['samplingweight'] * frequency['gbflag']
frequency_table = frequency.groupby(['selection_month', 'mevent'], as_index=False).agg({'total': sum})
frequency_table['total'] = frequency_table['total'].astype('int64')

frequency_tab = pd.DataFrame(frequency_table.pivot_table(index=['selection_month'], columns='mevent',
                                                          aggfunc=sum, fill_value=0).reset_index())
frequency_tab.columns = ['Selection Month', 'Non Event', 'Event']
frequency_tab['Total Base'] = frequency_tab['Non Event'] + frequency_tab['Event']
frequency_tab['Event Rate in %'] = ((frequency_tab['Event'] * 100) / frequency_tab['Total Base']).round(2)
frequency_tab['Selection Month'] = frequency_tab['Selection Month'].astype(str)

# Display dataframe
import ace_tools as tools
tools.display_dataframe_to_user(name="Frequency Table", dataframe=frequency_tab)

raw.describe()

# Drop columns with all missing values
cols_to_drop_all_missing = raw.columns[raw.eq(0).all()].tolist()
raw1 = raw.drop(columns=cols_to_drop_all_missing)
print(raw1.shape)
print(raw1['mevent'].value_counts())

# Handle missing values
raw2 = raw1.fillna(raw1.mode().iloc[0])
print(raw2.shape)
print(raw2['mevent'].value_counts())

# Drop irrelevant columns
drop_cols = ['ga_cust_id', 'gndr_cde', 'gbflag', 'model', 'fin_annual_sal_src', 'gender']
dataset_raw = raw2.drop(columns=drop_cols, axis=1)
print(dataset_raw.shape)
print(dataset_raw['mevent'].value_counts())

# Save dataset
dataset_raw.to_csv(path + 'dev_data.csv', encoding='utf-8', index=False)

# Setup PyCaret
cat_feat1 = list(dataset_raw.select_dtypes(include=['object']).columns)
int_feat1 = list(dataset_raw.select_dtypes(include=['int64', 'float64', 'float32']).columns)
int_feat1.remove('mevent')

clf = setup(dataset_raw, target='mevent',
            ignore_features=['selectionprob', 'cust_num', 'samplingweight', 'selection_month', 'key', 'age_yr_gam', 'age_flag'],
            numeric_imputation='mean',
            categorical_imputation='mode',
            remove_outliers=True,
            train_size=0.70,
            session_id=1992)

# Compare models
base_model = compare_models(fold=3, round=2, sort='AUC', n_select=1)
base_model_init_results = pull()
base_model_init_results = pd.DataFrame(base_model_init_results)
base_model_init_results.to_csv(path + 'base_model_init_results.csv', encoding='utf-8', index=False)

# Get training data
X_train = get_config('X_train')
y_train = get_config('y_train')

# Feature selection
def drop_correlated_features(df, threshold=0.9):
    corr_matrix = df.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]
    return df.drop(columns=to_drop)

X_train_corr = drop_correlated_features(X_train, threshold=0.9)

# Feature importance
feature_names = pd.DataFrame(X_train_corr.columns)
feature_names['var'] = feature_names
feature_imp = pd.DataFrame(base_model.feature_importances_)
feature_imp['imp'] = feature_imp
var_imp = pd.concat([feature_names, feature_imp], axis=1)
var_imp = var_imp[['var', 'imp']].sort_values(['imp'], ascending=False)

# Hyperparameter tuning
params = {
    'n_estimators': [40, 50, 75, 100],
    'learning_rate': [0.05, 0.1, 0.15, 0.2],
    'max_depth': [2, 3, 4],
    'min_samples_split': [5, 10, 15, 20],
    'min_samples_leaf': [4, 6, 10]
}
gbc_custom_v1 = tune_model(base_model, custom_grid=params)

# Save model
save_model(gbc_custom_v1, path + 'model_tune_gbc')

# Generate ROC curve
fpr, tpr, thresholds = roc_curve(y_train, gbc_custom_v1.predict_proba(X_train)[:, 1])
plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')
plt.plot(fpr, tpr, marker='.', label='GBC')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.savefig(path + 'ROC.png', dpi=50, bbox_inches='tight')

# Save training results
scr_all, scr_train, scr_test = predict_model(gbc_custom_v1, dataset_raw, raw_score=True)
scr_all.to_csv(path + 'scr_all.csv', encoding='utf-8', index=False)

# Display summary statistics
print(f"Train Gini: {roc_auc_score(y_train, gbc_custom_v1.predict_proba(X_train)[:, 1]) * 2 - 1}")




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import shap
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from pycaret.classification import *
import logging

# Set up logging and output directory
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
output_dir = Path("model_output")
output_dir.mkdir(exist_ok=True)

# Configuration parameters
TARGET_COLUMN = 'mevent'
TRAIN_SIZE = 0.7
RANDOM_STATE = 42
CORRELATION_THRESHOLD = 0.85
VIF_THRESHOLD = 5
FEATURE_SELECTION_THRESHOLD = 0.005
IMPORTANT_BUSINESS_FEATURES = ['cust_tenure', 'credit_card_limit', 'account_balance']

try:
    # Step 1: Load Dataset
    logging.info("Loading dataset...")
    data = pd.read_csv("your_data.csv")  # Replace with actual file path
    
    # Step 2: Identify Numeric & Categorical Variables
    logging.info("Identifying feature types...")
    numeric_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # Step 3: Handle Missing Values
    logging.info("Handling missing values...")
    for col in numeric_features:
        data[col] = data[col].fillna(data[col].median())
    
    for col in categorical_features:
        data[col] = data[col].fillna(data[col].mode()[0])
    
    # Step 4: Encode Categorical Variables
    logging.info("Encoding categorical variables...")
    label_encoders = {}
    for col in categorical_features:
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])
        label_encoders[col] = le
    
    # Step 5: Save Initial Features
    all_features = data.columns.tolist()
    pd.DataFrame({'Features': all_features}).to_csv(output_dir / "all_features.csv", index=False)
    
    # Step 6: Remove Highly Correlated Variables
    logging.info("Removing highly correlated features...")
    correlation_matrix = data[numeric_features].corr().abs()
    upper_triangle = correlation_matrix.where(
        np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
    )
    highly_correlated = [
        column for column in upper_triangle.columns 
        if any(upper_triangle[column] > CORRELATION_THRESHOLD)
    ]
    data.drop(columns=highly_correlated, inplace=True)
    
    # Step 7: Calculate and Filter by VIF
    logging.info("Calculating VIF and removing high VIF features...")
    X = add_constant(data[numeric_features])
    vif_data = pd.DataFrame()
    vif_data["Feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    low_vif_features = vif_data[vif_data["VIF"] < VIF_THRESHOLD]["Feature"].tolist()
    data = data[low_vif_features]
    
    # Step 8: Split Data
    logging.info("Splitting data into train and test sets...")
    X_train, X_test, y_train, y_test = train_test_split(
        data.drop(columns=[TARGET_COLUMN]), 
        data[TARGET_COLUMN], 
        test_size=1-TRAIN_SIZE, 
        random_state=RANDOM_STATE
    )
    
    # Step 9: PyCaret Setup and Feature Selection
    logging.info("Setting up PyCaret and performing feature selection...")
    clf = setup(
        data=data,
        target=TARGET_COLUMN,
        train_size=TRAIN_SIZE,
        remove_multicollinearity=True,
        multicollinearity_threshold=CORRELATION_THRESHOLD,
        feature_selection=True,
        feature_selection_threshold=FEATURE_SELECTION_THRESHOLD,
        ignore_low_variance=True,
        remove_outliers=False,
        silent=True,
        session_id=RANDOM_STATE
    )
    
    # Step 10: Get and Save Selected Features
    selected_features = get_config('X_train').columns.tolist()
    pd.DataFrame({'Features': selected_features}).to_csv(
        output_dir / "selected_features.csv", 
        index=False
    )
    
    # Step 11: Check Removed Features
    removed_features = list(set(all_features) - set(selected_features))
    pd.DataFrame({'Removed Features': removed_features}).to_csv(
        output_dir / "removed_features.csv", 
        index=False
    )
    
    # Step 12: Check Important Business Features
    missing_business_features = [
        feature for feature in IMPORTANT_BUSINESS_FEATURES 
        if feature not in selected_features
    ]
    if missing_business_features:
        logging.warning(f"Important business features removed: {missing_business_features}")
        selected_features = list(set(selected_features) | set(IMPORTANT_BUSINESS_FEATURES))
    
    # Step 13: Model Selection and Training
    logging.info("Training and tuning model...")
    best_model = compare_models(sort='AUC', n_select=1)
    tuned_model = tune_model(best_model, optimize='AUC', n_iter=10)
    final_model = finalize_model(tuned_model)
    
    # Step 14: Save Model
    logging.info("Saving final model...")
    save_model(final_model, output_dir / "final_model")
    
    # Step 15: Feature Importance Analysis
    logging.info("Generating feature importance analysis...")
    feature_imp = pd.DataFrame(
        final_model.feature_importances_,
        index=selected_features,
        columns=['Importance']
    )
    feature_imp = feature_imp.sort_values(by="Importance", ascending=False)
    feature_imp.to_csv(output_dir / "feature_importance.csv", index=True)
    
    # Step 16: SHAP Analysis
    logging.info("Generating SHAP analysis...")
    explainer = shap.TreeExplainer(final_model)
    shap_values = explainer.shap_values(get_config('X_train'))
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, get_config('X_train'))
    plt.savefig(output_dir / "shap_feature_importance.png")
    plt.close()
    
    # Step 17: Generate Evaluation Plots
    logging.info("Generating evaluation plots...")
    evaluate_model(final_model)
    
    plot_types = ['auc', 'pr', 'feature']
    for plot_type in plot_types:
        plt.figure()
        plot_model(final_model, plot=plot_type)
        plt.savefig(output_dir / f"{plot_type}_plot.png")
        plt.close()
    
    # Step 18: Generate Gains Chart
    logging.info("Generating gains chart...")
    gains_df = get_config('y_test').reset_index(drop=True)
    gains_df['prediction'] = predict_model(final_model, data=get_config('X_test'))['Score']
    gains_df['Decile'] = pd.qcut(gains_df['prediction'], q=10, labels=False, duplicates='drop')
    
    plt.figure(figsize=(10, 6))
    sns.barplot(
        x=gains_df.groupby("Decile")["prediction"].mean().index,
        y=gains_df.groupby("Decile")["prediction"].mean().values
    )
    plt.title("Gains Chart (Decile Analysis)")
    plt.xlabel("Decile")
    plt.ylabel("Mean Prediction Score")
    plt.savefig(output_dir / "gains_chart.png")
    plt.close()
    
    # Step 19: Print Final Summary
    logging.info("Generating final summary...")
    print("\nFinal Summary:")
    print(f"Total Features Before Selection: {len(all_features)}")
    print(f"Total Features After Selection: {len(selected_features)}")
    print(f"Important Business Features Removed: {missing_business_features}")
    print("\nTop 10 Features by Importance:")
    print(feature_imp.head(10))
    
    logging.info("Pipeline completed successfully!")

except Exception as e:
    logging.error(f"Error in pipeline: {str(e)}")
    raise









import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import shap
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from pycaret.classification import *
import logging

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

@dataclass
class ModelConfig:
    """Configuration settings for the ML pipeline."""
    target_column: str
    train_size: float = 0.7
    random_state: int = 42
    correlation_threshold: float = 0.85
    vif_threshold: float = 5
    feature_selection_threshold: float = 0.005
    important_business_features: List[str] = None

class FeatureProcessor:
    """Handles feature processing and selection."""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.label_encoders = {}
        self.numeric_features = []
        self.categorical_features = []
        
    def identify_feature_types(self, data: pd.DataFrame) -> Tuple[List[str], List[str]]:
        """Identify numeric and categorical features in the dataset."""
        self.numeric_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
        self.categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()
        return self.numeric_features, self.categorical_features
    
    def handle_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:
        """Fill missing values in the dataset."""
        df = data.copy()
        for col in self.numeric_features:
            df[col] = df[col].fillna(df[col].median())
        for col in self.categorical_features:
            df[col] = df[col].fillna(df[col].mode()[0])
        return df
    
    def encode_categorical_variables(self, data: pd.DataFrame) -> pd.DataFrame:
        """Encode categorical variables using LabelEncoder."""
        df = data.copy()
        for col in self.categorical_features:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
            self.label_encoders[col] = le
        return df
    
    def remove_correlated_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Remove highly correlated features."""
        df = data.copy()
        correlation_matrix = df[self.numeric_features].corr().abs()
        upper_triangle = correlation_matrix.where(
            np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
        )
        highly_correlated = [
            column for column in upper_triangle.columns 
            if any(upper_triangle[column] > self.config.correlation_threshold)
        ]
        return df.drop(columns=highly_correlated)
    
    def calculate_vif(self, data: pd.DataFrame) -> pd.DataFrame:
        """Calculate Variance Inflation Factor and remove high VIF features."""
        df = data.copy()
        X = add_constant(df[self.numeric_features])
        vif_data = pd.DataFrame()
        vif_data["Feature"] = X.columns
        vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
        low_vif_features = vif_data[vif_data["VIF"] < self.config.vif_threshold]["Feature"].tolist()
        return df[low_vif_features]

class ModelBuilder:
    """Handles model building, evaluation, and visualization."""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.final_model = None
        self.selected_features = []
        
    def setup_pycaret(self, data: pd.DataFrame) -> None:
        """Set up PyCaret environment."""
        clf = setup(
            data=data,
            target=self.config.target_column,
            train_size=self.config.train_size,
            remove_multicollinearity=True,
            multicollinearity_threshold=self.config.correlation_threshold,
            feature_selection=True,
            feature_selection_threshold=self.config.feature_selection_threshold,
            ignore_low_variance=True,
            remove_outliers=False,
            silent=True,
            session_id=self.config.random_state
        )
        self.selected_features = get_config('X_train').columns.tolist()
        
    def train_model(self) -> None:
        """Train and tune the model."""
        self.final_model = compare_models(sort='AUC', n_select=1)
        self.final_model = tune_model(self.final_model, optimize='AUC', n_iter=10)
        self.final_model = finalize_model(self.final_model)
        
    def generate_feature_importance(self, output_dir: Path) -> None:
        """Generate and save feature importance analysis."""
        feature_imp = pd.DataFrame(
            self.final_model.feature_importances_,
            index=self.selected_features,
            columns=['Importance']
        )
        feature_imp = feature_imp.sort_values(by="Importance", ascending=False)
        feature_imp.to_csv(output_dir / "feature_importance.csv", index=True)
        
        # SHAP analysis
        explainer = shap.TreeExplainer(self.final_model)
        shap_values = explainer.shap_values(get_config('X_train'))
        plt.figure()
        shap.summary_plot(shap_values, get_config('X_train'))
        plt.savefig(output_dir / "shap_feature_importance.png")
        plt.close()
        
    def generate_evaluation_plots(self, output_dir: Path) -> None:
        """Generate and save model evaluation plots."""
        plot_types = ['auc', 'pr', 'feature']
        for plot_type in plot_types:
            plt.figure()
            plot_model(self.final_model, plot=plot_type)
            plt.savefig(output_dir / f"{plot_type}_plot.png")
            plt.close()
            
    def generate_gains_chart(self, output_dir: Path) -> None:
        """Generate and save gains chart."""
        gains_df = get_config('y_test').reset_index(drop=True)
        gains_df['prediction'] = predict_model(self.final_model, data=get_config('X_test'))['Score']
        gains_df['Decile'] = pd.qcut(gains_df['prediction'], q=10, labels=False, duplicates='drop')
        
        plt.figure(figsize=(10, 6))
        sns.barplot(
            x=gains_df.groupby("Decile")["prediction"].mean().index,
            y=gains_df.groupby("Decile")["prediction"].mean().values
        )
        plt.title("Gains Chart (Decile Analysis)")
        plt.xlabel("Decile")
        plt.ylabel("Mean Prediction Score")
        plt.savefig(output_dir / "gains_chart.png")
        plt.close()

def main():
    # Create output directory
    output_dir = Path("model_output")
    output_dir.mkdir(exist_ok=True)
    
    # Initialize configuration
    config = ModelConfig(
        target_column='mevent',
        important_business_features=['cust_tenure', 'credit_card_limit', 'account_balance']
    )
    
    try:
        # Load data
        logging.info("Loading data...")
        data = pd.read_csv("your_data.csv")
        
        # Initialize processors
        feature_processor = FeatureProcessor(config)
        model_builder = ModelBuilder(config)
        
        # Process features
        logging.info("Processing features...")
        feature_processor.identify_feature_types(data)
        data = feature_processor.handle_missing_values(data)
        data = feature_processor.encode_categorical_variables(data)
        data = feature_processor.remove_correlated_features(data)
        data = feature_processor.calculate_vif(data)
        
        # Save all features before selection
        pd.DataFrame({'Features': data.columns.tolist()}).to_csv(
            output_dir / "all_features.csv",
            index=False
        )
        
        # Build and evaluate model
        logging.info("Building model...")
        model_builder.setup_pycaret(data)
        model_builder.train_model()
        
        # Generate visualizations and save results
        logging.info("Generating visualizations and saving results...")
        model_builder.generate_feature_importance(output_dir)
        model_builder.generate_evaluation_plots(output_dir)
        model_builder.generate_gains_chart(output_dir)
        
        # Save model
        save_model(model_builder.final_model, output_dir / "final_model")
        
        logging.info("Pipeline completed successfully!")
        
    except Exception as e:
        logging.error(f"Error in pipeline: {str(e)}")
        raise

if __name__ == "__main__":
    main()









import pandas as pd

# Ensure 'mevent' is an integer
frequency_table['mevent'] = frequency_table['mevent'].astype(int)

# Use pivot_table instead of pivot to handle duplicate entries
frequency_tab = frequency_table.pivot_table(
    index='selection_month', 
    columns='mevent', 
    values='total',
    aggfunc='sum',  # Aggregate sum in case of duplicate rows
    fill_value=0  # Replace missing values with 0
).reset_index()

# Rename columns properly
frequency_tab.columns.name = None  # Remove pivot_table auto-generated column name
frequency_tab.rename(columns={0: 'Non Event', 1: 'Event'}, inplace=True)

# Compute Total Base
frequency_tab['Total Base'] = frequency_tab['Non Event'] + frequency_tab['Event']

# Compute Event Rate
frequency_tab['Event Rate in %'] = ((frequency_tab['Event'] * 100) / frequency_tab['Total Base']).round(2)

# Convert Selection Month to string
frequency_tab['Selection Month'] = frequency_tab['Selection Month'].astype(str)

# Display the cleaned dataframe
import ace_tools as tools
tools.display_dataframe_to_user(name="Frequency Table", dataframe=frequency_tab)




import pandas as pd

# Ensure 'mevent' is an integer
frequency['mevent'] = frequency['mevent'].astype(int)

# Aggregate total sum for each 'selection_month' and 'mevent' to remove duplicates
frequency_table = frequency.groupby(['selection_month', 'mevent'], as_index=False).agg({'total': 'sum'})

# Debugging step: Check for issues before pivoting
print(frequency_table.head())  # Check structure before pivot
print(frequency_table.dtypes)  # Ensure 'mevent' and 'selection_month' are correct

# Use pivot instead of pivot_table to avoid unstack() errors
frequency_tab = frequency_table.pivot(
    index='selection_month', 
    columns='mevent', 
    values='total'
).fillna(0).reset_index()

# Rename columns properly
frequency_tab.columns.name = None  # Remove pivot_table auto-generated column name
frequency_tab.rename(columns={0: 'Non Event', 1: 'Event'}, inplace=True)

# Compute Total Base
frequency_tab['Total Base'] = frequency_tab['Non Event'] + frequency_tab['Event']

# Compute Event Rate
frequency_tab['Event Rate in %'] = ((frequency_tab['Event'] * 100) / frequency_tab['Total Base']).round(2)

# Convert Selection Month to string
frequency_tab['Selection Month'] = frequency_tab['Selection Month'].astype(str)

# Display the cleaned dataframe
import ace_tools as tools
tools.display_dataframe_to_user(name="Frequency Table", dataframe=frequency_tab)








m
import pandas as pd

# Ensure 'mevent' is an integer
frequency['mevent'] = frequency['mevent'].astype(int)

# Aggregate total sum for each 'selection_month' and 'mevent' to remove duplicates
frequency_table = frequency.groupby(['selection_month', 'mevent'], as_index=False).agg({'total': 'sum'})

# Debugging step: Check for issues before pivoting
print(frequency_table.head())  # Check structure before pivot
print(frequency_table.dtypes)  # Ensure 'mevent' and 'selection_month' are correct

# Use pivot instead of pivot_table to avoid unstack() errors
frequency_tab = frequency_table.pivot(
    index='selection_month', 
    columns='mevent', 
    values='total'
).fillna(0).reset_index()

# Rename columns properly
frequency_tab.columns.name = None  # Remove pivot_table auto-generated column name
frequency_tab.rename(columns={0: 'Non Event', 1: 'Event'}, inplace=True)

# Compute Total Base
frequency_tab['Total Base'] = frequency_tab['Non Event'] + frequency_tab['Event']

# Compute Event Rate
frequency_tab['Event Rate in %'] = ((frequency_tab['Event'] * 100) / frequency_tab['Total Base']).round(2)

# Convert Selection Month to string
frequency_tab['Selection Month'] = frequency_tab['Selection Month'].astype(str)

# Display the cleaned dataframe
import ace_tools as tools
tools.display_dataframe_to_user(name="Frequency Table", dataframe=frequency_tab)




import pandas as pd

# Assuming 'raw' is your original dataframe
frequency = raw[raw['final_cus_seg'] != 'Card Only'].groupby(
    ['selection_month', 'samplingweight', 'mevent'],
    as_index=False
).agg({'gbflag': 'sum'})

# Compute total based on sampling weight
frequency['total'] = frequency['samplingweight'] * frequency['gbflag']

# Aggregate by selection_month and mevent
frequency_table = frequency.groupby(['selection_month', 'mevent'], as_index=False).agg({'total': 'sum'})

# Convert total column to int
frequency_table['total'] = frequency_table['total'].astype('int64')

# Create pivot table
frequency_tab = frequency_table.pivot_table(
    index=['selection_month'],
    columns='mevent',
    values='total',
    aggfunc='sum',
    fill_value=0
).reset_index()

# Rename columns properly
frequency_tab.columns = ['Selection Month', 'Non Event', 'Event']

# Compute total base
frequency_tab['Total Base'] = frequency_tab['Non Event'] + frequency_tab['Event']

# Compute event rate in percentage
frequency_tab['Event Rate in %'] = ((frequency_tab['Event'] * 100) / frequency_tab['Total Base']).round(decimals=2)

# Ensure Selection Month is a string
frequency_tab['Selection Month'] = frequency_tab['Selection Month'].astype(str)

# Save to CSV
frequency_tab.to_csv('frequency_tab.csv', encoding='utf-8', index=False)

# Display result
import ace_tools as tools
tools.display_dataframe_to_user(name="Frequency Table", dataframe=frequency_tab)



# Take intersection of all feature selection methods
final_selected_features = list(set(selected_features_rfe) & set(selected_features_mi) & set(selected_features_shap))

# Keep only final selected features
raw = raw[['mevent'] + final_selected_features]
print(f"Final selected features: {len(final_selected_features)}")








from catboost import CatBoostClassifier
import shap

def select_features_by_shap(X, y, model=None, n_features=30):
    """
    Selects the top N features based on SHAP values.
    """
    if model is None:
        model = CatBoostClassifier(iterations=100, depth=5, learning_rate=0.1, silent=True)
        model.fit(X, y)

    explainer = shap.Explainer(model, X)
    shap_values = explainer(X)

    shap_importance = np.abs(shap_values.values).mean(axis=0)
    shap_df = pd.DataFrame({'Feature': X.columns, 'SHAP Value': shap_importance}).sort_values(by='SHAP Value', ascending=False)

    selected_features = shap_df['Feature'].head(n_features).tolist()
    print(f"Selected {len(selected_features)} features using SHAP Importance.")
    
    return selected_features

# Apply SHAP Selection
selected_features_shap = select_features_by_shap(raw[num_features], raw['mevent'], n_features=50)

# Keep only selected features
raw = raw[['mevent'] + selected_features_shap]








from sklearn.feature_selection import mutual_info_classif

def select_features_by_mutual_info(X, y, n_features=30):
    """
    Selects features based on Mutual Information Score.
    """
    mi_scores = mutual_info_classif(X, y, discrete_features=False)
    mi_df = pd.DataFrame({'Feature': X.columns, 'MI Score': mi_scores}).sort_values(by='MI Score', ascending=False)

    selected_features = mi_df['Feature'].head(n_features).tolist()
    print(f"Selected {len(selected_features)} features using Mutual Information.")
    
    return selected_features

# Apply Mutual Information Selection
selected_features_mi = select_features_by_mutual_info(raw[num_features], raw['mevent'], n_features=50)

# Keep only selected features
raw = raw[['mevent'] + selected_features_mi]













from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

def feature_selection_rfe(X, y, n_features=30):
    """
    Applies Recursive Feature Elimination (RFE) to select the top N features.
    """
    model = RandomForestClassifier(n_estimators=100, random_state=42)

    selector = RFE(model, n_features_to_select=n_features, step=1)
    selector.fit(X, y)

    selected_features = X.columns[selector.support_].tolist()
    print(f"Selected {len(selected_features)} features using RFE.")
    
    return selected_features

# Ensure correct numeric feature selection
num_features = raw.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Apply RFE on numeric features
selected_features_rfe = feature_selection_rfe(raw[num_features], raw['mevent'], n_features=50)

# Keep only selected features
raw = raw[['mevent'] + selected_features_rfe]










from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

def drop_high_vif_features(df, threshold=10):
    """
    Drops features with a high Variance Inflation Factor (VIF) score.
    """
    # Ensure only numeric columns are used
    numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

    # Ensure columns exist before applying VIF
    if not numeric_features:
        print("No numeric features found for VIF calculation.")
        return df

    X = add_constant(df[numeric_features])  # Add constant term for VIF calculation

    # Calculate VIF for each feature
    vif_data = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)

    # Identify features with VIF above threshold
    to_drop = vif_data[vif_data > threshold].index.tolist()

    print(f"Dropping {len(to_drop)} features with VIF > {threshold}: {to_drop}")

    return df.drop(columns=to_drop, errors="ignore")  # Avoid KeyError

# Apply VIF reduction
raw = drop_high_vif_features(raw)












# Import necessary libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import RFE, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from catboost import CatBoostClassifier
from pycaret.classification import setup, compare_models, tune_model, get_config, create_model, predict_model
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import shap

# Define paths
data_path = "/opt/jupyter/notebook/RR_Model_Development_Data.csv"
output_path = "/opt/jupyter/notebook/notebooks/jupyter/Output_RR/"
if not os.path.exists(output_path):
    os.makedirs(output_path)

# Load Data
raw = pd.read_csv(data_path, encoding="cp1252")
raw.columns = [x.lower() for x in raw.columns]
raw = raw.rename(columns={'custid': 'cust_num'})

# Drop columns with more than 80% missing values
missing_threshold = 0.8
missing_fraction = raw.isnull().sum() / len(raw)
cols_to_drop_missing = missing_fraction[missing_fraction > missing_threshold].index.tolist()
raw = raw.drop(columns=cols_to_drop_missing)

# Drop columns with very low variance
low_variance_cols = raw.var().loc[lambda x: x < 0.01].index.tolist()
raw = raw.drop(columns=low_variance_cols)

# Fill missing values
raw.fillna(raw.mode().iloc[0], inplace=True)

# Convert customer ID to string and standardize length
raw['cust_num'] = raw['cust_num'].astype(str).apply(lambda x: x.zfill(9))

# Identify categorical and numerical variables
cat_features = list(raw.select_dtypes(include=['object']).columns)
num_features = list(raw.select_dtypes(include=['int64', 'float64']).columns)

# Remove target variable from num_features list
num_features.remove('mevent')

### 1. **Correlation-based Feature Selection**
def drop_correlated_features(df, threshold=0.9):
    corr_matrix = df.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]
    return df.drop(columns=to_drop)

raw = drop_correlated_features(raw, threshold=0.9)

### 2. **Variance Inflation Factor (VIF)**
def drop_high_vif_features(df, threshold=10):
    X = add_constant(df)
    vif_data = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
    to_drop = vif_data[vif_data > threshold].index.tolist()
    return df.drop(columns=to_drop)

raw = drop_high_vif_features(raw[num_features])

### 3. **Recursive Feature Elimination (RFE)**
# Use Logistic Regression or Random Forest to select top features
def feature_selection_rfe(X, y, n_features=30):
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    selector = RFE(model, n_features_to_select=n_features, step=1)
    selector.fit(X, y)
    selected_features = X.columns[selector.support_].tolist()
    return selected_features

selected_features_rfe = feature_selection_rfe(raw[num_features], raw['mevent'], n_features=50)

### 4. **Mutual Information Gain**
def select_features_by_mutual_info(X, y, n_features=30):
    mi_scores = mutual_info_classif(X, y, discrete_features=False)
    mi_df = pd.DataFrame({'Feature': X.columns, 'MI Score': mi_scores}).sort_values(by='MI Score', ascending=False)
    return mi_df['Feature'].head(n_features).tolist()

selected_features_mi = select_features_by_mutual_info(raw[num_features], raw['mevent'], n_features=50)

### 5. **SHAP Feature Importance**
def select_features_by_shap(X, y, model=None, n_features=30):
    if model is None:
        model = CatBoostClassifier(iterations=100, depth=5, learning_rate=0.1, silent=True)
        model.fit(X, y)
    
    explainer = shap.Explainer(model, X)
    shap_values = explainer(X)
    shap_importance = np.abs(shap_values.values).mean(axis=0)
    shap_df = pd.DataFrame({'Feature': X.columns, 'SHAP Value': shap_importance}).sort_values(by='SHAP Value', ascending=False)
    return shap_df['Feature'].head(n_features).tolist()

selected_features_shap = select_features_by_shap(raw[num_features], raw['mevent'], n_features=50)

# **Final Selected Features (Intersection of All Methods)**
final_selected_features = list(set(selected_features_rfe) & set(selected_features_mi) & set(selected_features_shap))
raw = raw[['mevent'] + final_selected_features]

# PyCaret Model Training
clf = setup(
    raw, 
    target='mevent',
    numeric_imputation='mean',
    categorical_imputation='mode',
    remove_outliers=True,
    train_size=0.7,
    session_id=1992
)

# Train the best model
best_model = compare_models(fold=5, round=2, sort='AUC', n_select=1)

# Tune the best model
tuned_model = tune_model(best_model, fold=5, optimize='AUC')

# Save Feature Importance
feature_importance = pd.DataFrame({'Feature': final_selected_features, 'Importance': tuned_model.feature_importances_})
feature_importance.to_csv(output_path + "final_feature_importance.csv", index=False)

# Save the final dataset with selected features
raw.to_csv(output_path + "final_selected_data.csv", index=False)
