shap & psi

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report
import shap

# Function to calculate Precision, Recall, and F1-Score
def evaluate_classification(model, X, y, dataset_name):
    y_pred = model.predict(X)
    y_proba = model.predict_proba(X)[:, 1]  # Probability of event

    precision = precision_score(y, y_pred)
    recall = recall_score(y, y_pred)
    f1 = f1_score(y, y_pred)

    print(f"\nðŸ”¹ Classification Report for {dataset_name}:")
    print(classification_report(y, y_pred))

    return {
        "Dataset": dataset_name,
        "Precision": precision,
        "Recall": recall,
        "F1-Score": f1
    }

# Evaluate Train, Test, and OOT
train_metrics = evaluate_classification(final_model, X_train, y_train, "Train")
test_metrics = evaluate_classification(final_model, X_test, y_test, "Test")
oot_metrics = evaluate_classification(final_model, X_oot_features, y_oot, "OOT")

# Create DataFrame for easy comparison
metrics_df = pd.DataFrame([train_metrics, test_metrics, oot_metrics])
print("\nðŸ”¹ Precision, Recall, and F1-Score Across Datasets:")
print(metrics_df)

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix"):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Non-Event", "Event"], yticklabels=["Non-Event", "Event"])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(title)
    plt.show()

# Plot Confusion Matrix for OOT
y_oot_pred = final_model.predict(X_oot_features)
plot_confusion_matrix(y_oot, y_oot_pred, title="Confusion Matrix - OOT")

# Function to calculate PSI
def calculate_psi(expected, actual, bins=10):
    expected_counts, expected_bins = np.histogram(expected, bins=bins)
    actual_counts, _ = np.histogram(actual, bins=expected_bins)

    expected_percents = expected_counts / np.sum(expected_counts)
    actual_percents = actual_counts / np.sum(actual_counts)

    psi_values = (expected_percents - actual_percents) * np.log(expected_percents / actual_percents)
    psi_values = np.nan_to_num(psi_values)

    return np.sum(psi_values)

# Calculate PSI between Train and OOT for key features
psi_results = {}
for col in X_train.columns:
    psi_results[col] = calculate_psi(X_train[col], X_oot_features[col])

# Convert PSI results to DataFrame
psi_df = pd.DataFrame(list(psi_results.items()), columns=["Feature", "PSI"])
psi_df = psi_df.sort_values(by="PSI", ascending=False)

# Display PSI results
print("\nðŸ”¹ Population Stability Index (PSI) Between Train & OOT:")
print(psi_df.head(10))  # Display top 10 features with highest PSI

# Function to plot SHAP feature importance
def plot_shap_importance(model, X):
    explainer = shap.Explainer(model, X)
    shap_values = explainer(X)

    # Summary Plot (Overall Feature Importance)
    shap.summary_plot(shap_values, X)

    # Bar Chart for Top 10 Features
    shap_values_mean = np.abs(shap_values.values).mean(axis=0)
    shap_importance = pd.DataFrame({"Feature": X.columns, "SHAP Importance": shap_values_mean})
    shap_importance = shap_importance.sort_values(by="SHAP Importance", ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(x="SHAP Importance", y="Feature", data=shap_importance[:10], palette="viridis")
    plt.title("Top 10 Most Important Features (SHAP)")
    plt.show()

# Generate SHAP Feature Importance Plots
plot_shap_importance(final_model, X_train)

# Extract feature importance from model
feature_importance = pd.DataFrame({
    "Feature": X_train.columns,
    "Importance": final_model.feature_importances_
}).sort_values(by="Importance", ascending=False)

# Display all feature importance values
print("\nðŸ”¹ Feature Importance for All Variables:")
print(feature_importance)

# Display top 10 features
print("\nðŸ”¹ Top 10 Most Important Features:")
print(feature_importance.head(10))

# Plot Top 10 Feature Importances
plt.figure(figsize=(10, 6))
sns.barplot(x="Importance", y="Feature", data=feature_importance[:10], palette="magma")
plt.title("Top 10 Most Important Features")
plt.show()


















Auc, charts

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score

# Function to create Gain Matrix with full event distribution analysis
def create_gain_matrix(model, X, y, dataset_name):
    # Get predicted probabilities
    y_pred_proba = model.predict_proba(X)[:, 1]  

    # Create DataFrame with Actual vs Predicted Probabilities
    df = pd.DataFrame({"Actual": y, "Predicted_Prob": y_pred_proba})
    
    # Create Deciles (10 bins based on predicted probability)
    df["Decile"] = pd.qcut(df["Predicted_Prob"], 10, labels=False, duplicates="drop")
    
    # Sort deciles from highest probability to lowest (Decile 1 = Highest Risk)
    df["Decile"] = 10 - df["Decile"]  

    # Compute Gain Matrix Metrics
    gain_matrix = df.groupby("Decile").agg(
        Total_Customers=("Actual", "count"),
        Events=("Actual", "sum")  # Count of positive class (event)
    ).reset_index()

    # Compute Non-Events
    gain_matrix["Non-Events"] = gain_matrix["Total_Customers"] - gain_matrix["Events"]
    
    # Cumulative Sums
    gain_matrix["Cumulative_Non_Events"] = gain_matrix["Non-Events"].cumsum()
    gain_matrix["Cumulative_Events"] = gain_matrix["Events"].cumsum()

    # Compute Percent Cumulative Events & Non-Events
    total_events = gain_matrix["Events"].sum()
    total_non_events = gain_matrix["Non-Events"].sum()

    gain_matrix["% Cumulative Non-Events"] = (gain_matrix["Cumulative_Non_Events"] / total_non_events) * 100
    gain_matrix["% Cumulative Events"] = (gain_matrix["Cumulative_Events"] / total_events) * 100

    # Compute Event Rate per Decile
    gain_matrix["Event Rate"] = gain_matrix["Events"] / gain_matrix["Total_Customers"]

    # Compute Cumulative Event Rate
    gain_matrix["Cumulative Event Rate"] = gain_matrix["Cumulative_Events"] / gain_matrix["Total_Customers"].sum()

    # Compute Separation (Difference in % Cumulative Events and Non-Events)
    gain_matrix["Separation"] = gain_matrix["% Cumulative Events"] - gain_matrix["% Cumulative Non-Events"]

    # Compute Lift (Cumulative Event Rate vs. Overall Event Rate)
    overall_event_rate = total_events / (total_events + total_non_events)
    gain_matrix["Lift"] = gain_matrix["Cumulative Event Rate"] / overall_event_rate

    # Add Dataset Name
    gain_matrix["Dataset"] = dataset_name

    return gain_matrix

# Function to calculate AUC
def calculate_auc(model, X, y):
    y_pred_proba = model.predict_proba(X)[:, 1]
    return roc_auc_score(y, y_pred_proba)

# Compute Gain Matrices
train_gain_matrix = create_gain_matrix(final_model, X_train, y_train, "Train")
test_gain_matrix = create_gain_matrix(final_model, X_test, y_test, "Test")
oot_gain_matrix = create_gain_matrix(final_model, X_oot_features, y_oot, "OOT")

# Compute AUC Scores
train_auc = calculate_auc(final_model, X_train, y_train)
test_auc = calculate_auc(final_model, X_test, y_test)
oot_auc = calculate_auc(final_model, X_oot_features, y_oot)

# Combine Gain Matrices for Display
gain_matrix_df = pd.concat([train_gain_matrix, test_gain_matrix, oot_gain_matrix])

# Display Gain Matrix Table
print("\n--- Gain Matrix (Decile Analysis) ---")
print(gain_matrix_df.to_string(index=False))

# Display AUC Scores
auc_scores_df = pd.DataFrame({
    "Dataset": ["Train", "Test", "OOT"],
    "AUC Score": [train_auc, test_auc, oot_auc]
})

print("\n--- AUC Scores ---")
print(auc_scores_df.to_string(index=False))

# Plot Cumulative Gain Chart
plt.figure(figsize=(8, 6))
plt.plot(train_gain_matrix["Decile"], train_gain_matrix["% Cumulative Events"], marker='o', label="Train")
plt.plot(test_gain_matrix["Decile"], test_gain_matrix["% Cumulative Events"], marker='s', label="Test")
plt.plot(oot_gain_matrix["Decile"], oot_gain_matrix["% Cumulative Events"], marker='^', label="OOT")

plt.xlabel("Decile (1 = Highest Risk)")
plt.ylabel("Cumulative % of Events")
plt.title("Cumulative Gain Chart Based on Deciles")
plt.legend()
plt.grid(True)
plt.show()

# Plot Lift Chart
plt.figure(figsize=(8, 6))
plt.plot(train_gain_matrix["Decile"], train_gain_matrix["Lift"], marker='o', label="Train")
plt.plot(test_gain_matrix["Decile"], test_gain_matrix["Lift"], marker='s', label="Test")
plt.plot(oot_gain_matrix["Decile"], oot_gain_matrix["Lift"], marker='^', label="OOT")

plt.xlabel("Decile (1 = Highest Risk)")
plt.ylabel("Lift")
plt.title("Lift Chart Based on Deciles")
plt.legend()
plt.grid(True)
plt.show()















Main Model

import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import roc_auc_score
import joblib

# Load Data (Ensure `Dev_data1` and `X_oot` are preloaded as DataFrames)
target_column = "mevent"  # Update with actual target variable name
ignore_features = ["cust_num", "samplingweight", "selection_time"]  # Features to ignore

# Remove ignored features from model input, but keep them in the dataset
X = Dev_data1.drop(columns=[target_column] + ignore_features)
y = Dev_data1[target_column]

# Ensure consistent Train-Test Split using random_state=1992
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1992)

# Prepare OOT dataset (Ensure features match Train/Test datasets)
y_oot = X_oot[target_column]
X_oot_features = X_oot.drop(columns=[target_column] + ignore_features)
X_oot_features = X_oot_features[X_train.columns]  # Ensure feature consistency

# Function to calculate Gini Score
def calculate_gini(model, X, y):
    y_pred = model.predict_proba(X)[:, 1]  # Get probabilities
    auc = roc_auc_score(y, y_pred)
    gini = 2 * auc - 1  # Convert AUC to Gini
    return gini

# Function to generate Scores and Odds
def generate_scores_odds(model, X):
    probs = model.predict_proba(X)[:, 1]  # Get probability of class 1
    scores = np.log(probs / (1 - probs))  # Log odds transformation
    return pd.DataFrame({"Score": scores, "Odds": probs / (1 - probs)})

# Define Hyperparameter Search Space (20 Iterations)
param_grid = {
    "learning_rate": np.linspace(0.01, 0.3, 10),
    "max_depth": np.arange(3, 10),
    "min_samples_leaf": np.arange(1, 50, 5),
    "n_estimators": np.arange(100, 1000, 100),
    "subsample": np.linspace(0.5, 1.0, 5)
}

# Gradient Boosting Classifier with fixed random_state
gb_model = GradientBoostingClassifier(random_state=1992)

# Use RandomizedSearchCV for Hyperparameter Tuning (20 Iterations) with fixed random_state
random_search = RandomizedSearchCV(
    estimator=gb_model,
    param_distributions=param_grid,
    n_iter=20,  # 20 Iterations
    scoring="roc_auc",
    cv=5,
    random_state=1992,  # Fixed random state for consistency
    verbose=1,
    n_jobs=-1
)

# Fit the Model
random_search.fit(X_train, y_train)

# Store Results for Each Iteration
results = []
for i, params in enumerate(random_search.cv_results_["params"]):
    best_model = GradientBoostingClassifier(**params, random_state=1992)  # Ensure consistent model training
    best_model.fit(X_train, y_train)

    # Compute Gini Scores
    train_gini = calculate_gini(best_model, X_train, y_train)
    test_gini = calculate_gini(best_model, X_test, y_test)
    oot_gini = calculate_gini(best_model, X_oot_features, y_oot)

    # Store iteration results
    results.append({
        "Iteration": i + 1,
        "Learning Rate": params.get("learning_rate"),
        "Max Depth": params.get("max_depth"),
        "Min Samples Leaf": params.get("min_samples_leaf"),
        "Estimators": params.get("n_estimators"),
        "Subsample": params.get("subsample"),
        "Train Gini": train_gini,
        "Test Gini": test_gini,
        "OOT Gini": oot_gini,
        "Train-OOT Gini Diff": abs(train_gini - oot_gini)
    })

# Convert results into DataFrame
results_df = pd.DataFrame(results)

# Step 5: Select Best Model (Highest Test Gini, Smallest Train-OOT Gini Difference)
best_model_row = results_df[results_df["Train-OOT Gini Diff"] < 0.05].nlargest(1, "Test Gini")

# Step 6: Train & Save the Best Model
best_params = best_model_row.iloc[0][["Learning Rate", "Max Depth", "Min Samples Leaf", "Estimators", "Subsample"]].to_dict()
final_model = GradientBoostingClassifier(
    learning_rate=best_params["Learning Rate"],
    max_depth=int(best_params["Max Depth"]),
    min_samples_leaf=int(best_params["Min Samples Leaf"]),
    n_estimators=int(best_params["Estimators"]),
    subsample=best_params["Subsample"],
    random_state=1992  # Keep the same random state
)

final_model.fit(X_train, y_train)
joblib.dump(final_model, "best_gradient_boosting.pkl")  # Save Model

# Generate Scores & Odds for all datasets
dev_scores = generate_scores_odds(final_model, X)
train_scores = generate_scores_odds(final_model, X_train)
test_scores = generate_scores_odds(final_model, X_test)
oot_scores = generate_scores_odds(final_model, X_oot_features)

# Step 7: Display Results Table
import ace_tools as tools
tools.display_dataframe_to_user(name="Gradient Boosting Hyperparameter Tuning Results", dataframe=results_df)
tools.display_dataframe_to_user(name="Dev Data Scores & Odds", dataframe=dev_scores)
tools.display_dataframe_to_user(name="Train Data Scores & Odds", dataframe=train_scores)
tools.display_dataframe_to_user(name="Test Data Scores & Odds", dataframe=test_scores)
tools.display_dataframe_to_user(name="OOT Data Scores & Odds", dataframe=oot_scores)