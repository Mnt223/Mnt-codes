Psi Table

import numpy as np
import pandas as pd

# Function to calculate PSI for each score band
def calculate_psi(expected, actual):
    expected = np.where(expected == 0, 0.0001, expected)  # Avoid zero division
    actual = np.where(actual == 0, 0.0001, actual)
    
    psi_values = (expected - actual) * np.log(expected / actual)
    psi_values = np.nan_to_num(psi_values)  # Handle NaN values safely
    
    return np.sum(psi_values)

# Function to create PSI table
def create_psi_table(train_scores, test_scores, oot_scores, bins=10):
    # Create score bands based on Train dataset
    bin_edges = np.percentile(train_scores, np.linspace(0, 100, bins + 1))
    bin_labels = [f"{int(bin_edges[i])}-{int(bin_edges[i+1])}" for i in range(len(bin_edges)-1)]
    
    # Assign each score to a band
    train_bins = pd.cut(train_scores, bins=bin_edges, labels=bin_labels, include_lowest=True)
    test_bins = pd.cut(test_scores, bins=bin_edges, labels=bin_labels, include_lowest=True)
    oot_bins = pd.cut(oot_scores, bins=bin_edges, labels=bin_labels, include_lowest=True)
    
    # Calculate distribution percentages
    train_dist = train_bins.value_counts(normalize=True).sort_index()
    test_dist = test_bins.value_counts(normalize=True).sort_index()
    oot_dist = oot_bins.value_counts(normalize=True).sort_index()
    
    # Compute PSI values
    psi_train_test = calculate_psi(train_dist.values, test_dist.values)
    psi_train_oot = calculate_psi(train_dist.values, oot_dist.values)
    
    # Create PSI table
    psi_df = pd.DataFrame({
        "Score Band": bin_labels,
        "Train %": train_dist.values * 100,
        "Test %": test_dist.values * 100,
        "OOT %": oot_dist.values * 100,
        "PSI (Train vs Test)": (train_dist - test_dist) * np.log(train_dist / test_dist),
        "PSI (Train vs OOT)": (train_dist - oot_dist) * np.log(train_dist / oot_dist)
    })
    
    # Replace NaN with 0 (to handle cases where no data is present in a bin)
    psi_df.fillna(0, inplace=True)
    
    # Add Total PSI row
    psi_df.loc["Total"] = ["Total", "-", "-", "-", psi_train_test, psi_train_oot]
    
    return psi_df

# Example Usage:
# Assuming `train_scores`, `test_scores`, and `oot_scores` are NumPy arrays or Pandas Series
train_scores = np.random.normal(600, 50, 1000)  # Example normal distribution
test_scores = np.random.normal(590, 55, 1000)
oot_scores = np.random.normal(580, 60, 1000)

# Generate PSI table
psi_table = create_psi_table(train_scores, test_scores, oot_scores, bins=10)

# Display PSI table
import ace_tools as tools
tools.display_dataframe_to_user(name="PSI Table - Train vs Test & Train vs OOT", dataframe=psi_table)











psi check 

import numpy as np
import pandas as pd

# Function to calculate PSI with zero handling
def calculate_psi(expected, actual, bins=10):
    expected_counts, expected_bins = np.histogram(expected, bins=bins)
    actual_counts, _ = np.histogram(actual, bins=expected_bins)

    # Convert to percentages
    expected_percents = expected_counts / np.sum(expected_counts)
    actual_percents = actual_counts / np.sum(actual_counts)

    # Avoid division by zero (replace 0 values with a small number)
    expected_percents = np.where(expected_percents == 0, 0.0001, expected_percents)
    actual_percents = np.where(actual_percents == 0, 0.0001, actual_percents)

    # Compute PSI
    psi_values = (expected_percents - actual_percents) * np.log(expected_percents / actual_percents)
    psi_values = np.nan_to_num(psi_values)  # Replace NaNs with zero

    return np.sum(psi_values)

# Identify features with high missing values or zero dominance
def identify_problematic_features(X_train, X_oot, threshold=0.95):
    missing_train = X_train.isna().mean()
    missing_oot = X_oot.isna().mean()
    zero_train = (X_train == 0).mean()
    zero_oot = (X_oot == 0).mean()

    # Find features with more than `threshold` % missing or zero values
    high_missing = missing_train[missing_train > threshold].index.tolist()
    high_missing_oot = missing_oot[missing_oot > threshold].index.tolist()
    high_zeros = zero_train[zero_train > threshold].index.tolist()
    high_zeros_oot = zero_oot[zero_oot > threshold].index.tolist()

    # Combine all problematic features
    problematic_features = set(high_missing + high_missing_oot + high_zeros + high_zeros_oot)

    return list(problematic_features)

# Compute PSI for Train vs Test and Train vs OOT
def compute_psi_for_features(X_train, X_test, X_oot):
    psi_train_test = {col: calculate_psi(X_train[col], X_test[col]) for col in X_train.columns}
    psi_train_oot = {col: calculate_psi(X_train[col], X_oot[col]) for col in X_train.columns}

    # Convert PSI results to DataFrame
    psi_df = pd.DataFrame({
        "Feature": list(psi_train_test.keys()),
        "PSI_Train_vs_Test": list(psi_train_test.values()),
        "PSI_Train_vs_OOT": list(psi_train_oot.values())
    })

    return psi_df

# Main function to clean datasets and filter features
def clean_and_filter_features(X_train, X_test, X_oot, psi_threshold=0.5):
    print("\nðŸ”¹ Identifying problematic features (high missing/zero values)...")
    problematic_features = identify_problematic_features(X_train, X_oot)

    print(f"ðŸ”¸ Features removed due to missing/zero dominance: {problematic_features}")

    # Drop problematic features before PSI calculation
    X_train_filtered = X_train.drop(columns=problematic_features, errors="ignore")
    X_test_filtered = X_test.drop(columns=problematic_features, errors="ignore")
    X_oot_filtered = X_oot.drop(columns=problematic_features, errors="ignore")

    print("\nðŸ”¹ Calculating PSI for remaining features...")
    psi_df = compute_psi_for_features(X_train_filtered, X_test_filtered, X_oot_filtered)

    # Identify features with high PSI (indicating data drift)
    high_psi_features = psi_df[psi_df["PSI_Train_vs_OOT"] > psi_threshold]["Feature"].tolist()
    
    print(f"ðŸ”¸ Features removed due to high PSI (> {psi_threshold}): {high_psi_features}")

    # Remove high-PSI features
    X_train_final = X_train_filtered.drop(columns=high_psi_features, errors="ignore")
    X_test_final = X_test_filtered.drop(columns=high_psi_features, errors="ignore")
    X_oot_final = X_oot_filtered.drop(columns=high_psi_features, errors="ignore")

    print("\nâœ… Data Cleaning Completed. Features removed:")
    print(f"â€¢ {len(problematic_features)} due to high missing/zero values")
    print(f"â€¢ {len(high_psi_features)} due to high PSI (data drift)")

    return X_train_final, X_test_final, X_oot_final, psi_df

# Run cleaning & filtering on the datasets
X_train_clean, X_test_clean, X_oot_clean, psi_results = clean_and_filter_features(X_train, X_test, X_oot_features)

# Display final PSI results
print("\nðŸ”¹ Final PSI Results After Cleaning:")
print(psi_results.sort_values(by="PSI_Train_vs_OOT", ascending=False).to_string(index=False))



















psi 

import numpy as np
import pandas as pd

# Function to calculate PSI (Population Stability Index)
def calculate_psi(expected, actual, bins=10):
    expected_counts, expected_bins = np.histogram(expected, bins=bins)
    actual_counts, _ = np.histogram(actual, bins=expected_bins)

    expected_percents = expected_counts / np.sum(expected_counts)
    actual_percents = actual_counts / np.sum(actual_counts)

    psi_values = (expected_percents - actual_percents) * np.log(expected_percents / actual_percents)
    psi_values = np.nan_to_num(psi_values)  # Handle zero values safely

    return np.sum(psi_values)

# Ensure `X_train`, `X_test`, `X_oot_features` are loaded before running
# Compute PSI for Train vs Test and Train vs OOT
psi_train_test = {col: calculate_psi(X_train[col], X_test[col]) for col in X_train.columns}
psi_train_oot = {col: calculate_psi(X_train[col], X_oot_features[col]) for col in X_train.columns}

# Convert PSI results to DataFrame
psi_df = pd.DataFrame({
    "Feature": list(psi_train_test.keys()),
    "PSI_Train_vs_Test": list(psi_train_test.values()),
    "PSI_Train_vs_OOT": list(psi_train_oot.values())
})

# Sort by highest PSI values
psi_df = psi_df.sort_values(by="PSI_Train_vs_OOT", ascending=False)

# Display PSI Table
print("\nðŸ”¹ PSI Train vs Test & Train vs OOT")
print(psi_df.to_string(index=False))



















shap & psi

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report
import shap

# Function to calculate Precision, Recall, and F1-Score
def evaluate_classification(model, X, y, dataset_name):
    y_pred = model.predict(X)
    y_proba = model.predict_proba(X)[:, 1]  # Probability of event

    precision = precision_score(y, y_pred)
    recall = recall_score(y, y_pred)
    f1 = f1_score(y, y_pred)

    print(f"\nðŸ”¹ Classification Report for {dataset_name}:")
    print(classification_report(y, y_pred))

    return {
        "Dataset": dataset_name,
        "Precision": precision,
        "Recall": recall,
        "F1-Score": f1
    }

# Evaluate Train, Test, and OOT
train_metrics = evaluate_classification(final_model, X_train, y_train, "Train")
test_metrics = evaluate_classification(final_model, X_test, y_test, "Test")
oot_metrics = evaluate_classification(final_model, X_oot_features, y_oot, "OOT")

# Create DataFrame for easy comparison
metrics_df = pd.DataFrame([train_metrics, test_metrics, oot_metrics])
print("\nðŸ”¹ Precision, Recall, and F1-Score Across Datasets:")
print(metrics_df)

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix"):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Non-Event", "Event"], yticklabels=["Non-Event", "Event"])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(title)
    plt.show()

# Plot Confusion Matrix for OOT
y_oot_pred = final_model.predict(X_oot_features)
plot_confusion_matrix(y_oot, y_oot_pred, title="Confusion Matrix - OOT")

# Function to calculate PSI
def calculate_psi(expected, actual, bins=10):
    expected_counts, expected_bins = np.histogram(expected, bins=bins)
    actual_counts, _ = np.histogram(actual, bins=expected_bins)

    expected_percents = expected_counts / np.sum(expected_counts)
    actual_percents = actual_counts / np.sum(actual_counts)

    psi_values = (expected_percents - actual_percents) * np.log(expected_percents / actual_percents)
    psi_values = np.nan_to_num(psi_values)

    return np.sum(psi_values)

# Calculate PSI between Train and OOT for key features
psi_results = {}
for col in X_train.columns:
    psi_results[col] = calculate_psi(X_train[col], X_oot_features[col])

# Convert PSI results to DataFrame
psi_df = pd.DataFrame(list(psi_results.items()), columns=["Feature", "PSI"])
psi_df = psi_df.sort_values(by="PSI", ascending=False)

# Display PSI results
print("\nðŸ”¹ Population Stability Index (PSI) Between Train & OOT:")
print(psi_df.head(10))  # Display top 10 features with highest PSI

# Function to plot SHAP feature importance
def plot_shap_importance(model, X):
    explainer = shap.Explainer(model, X)
    shap_values = explainer(X)

    # Summary Plot (Overall Feature Importance)
    shap.summary_plot(shap_values, X)

    # Bar Chart for Top 10 Features
    shap_values_mean = np.abs(shap_values.values).mean(axis=0)
    shap_importance = pd.DataFrame({"Feature": X.columns, "SHAP Importance": shap_values_mean})
    shap_importance = shap_importance.sort_values(by="SHAP Importance", ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(x="SHAP Importance", y="Feature", data=shap_importance[:10], palette="viridis")
    plt.title("Top 10 Most Important Features (SHAP)")
    plt.show()

# Generate SHAP Feature Importance Plots
plot_shap_importance(final_model, X_train)

# Extract feature importance from model
feature_importance = pd.DataFrame({
    "Feature": X_train.columns,
    "Importance": final_model.feature_importances_
}).sort_values(by="Importance", ascending=False)

# Display all feature importance values
print("\nðŸ”¹ Feature Importance for All Variables:")
print(feature_importance)

# Display top 10 features
print("\nðŸ”¹ Top 10 Most Important Features:")
print(feature_importance.head(10))

# Plot Top 10 Feature Importances
plt.figure(figsize=(10, 6))
sns.barplot(x="Importance", y="Feature", data=feature_importance[:10], palette="magma")
plt.title("Top 10 Most Important Features")
plt.show()


















Auc, charts

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score

# Function to create Gain Matrix with full event distribution analysis
def create_gain_matrix(model, X, y, dataset_name):
    # Get predicted probabilities
    y_pred_proba = model.predict_proba(X)[:, 1]  

    # Create DataFrame with Actual vs Predicted Probabilities
    df = pd.DataFrame({"Actual": y, "Predicted_Prob": y_pred_proba})
    
    # Create Deciles (10 bins based on predicted probability)
    df["Decile"] = pd.qcut(df["Predicted_Prob"], 10, labels=False, duplicates="drop")
    
    # Sort deciles from highest probability to lowest (Decile 1 = Highest Risk)
    df["Decile"] = 10 - df["Decile"]  

    # Compute Gain Matrix Metrics
    gain_matrix = df.groupby("Decile").agg(
        Total_Customers=("Actual", "count"),
        Events=("Actual", "sum")  # Count of positive class (event)
    ).reset_index()

    # Compute Non-Events
    gain_matrix["Non-Events"] = gain_matrix["Total_Customers"] - gain_matrix["Events"]
    
    # Cumulative Sums
    gain_matrix["Cumulative_Non_Events"] = gain_matrix["Non-Events"].cumsum()
    gain_matrix["Cumulative_Events"] = gain_matrix["Events"].cumsum()

    # Compute Percent Cumulative Events & Non-Events
    total_events = gain_matrix["Events"].sum()
    total_non_events = gain_matrix["Non-Events"].sum()

    gain_matrix["% Cumulative Non-Events"] = (gain_matrix["Cumulative_Non_Events"] / total_non_events) * 100
    gain_matrix["% Cumulative Events"] = (gain_matrix["Cumulative_Events"] / total_events) * 100

    # Compute Event Rate per Decile
    gain_matrix["Event Rate"] = gain_matrix["Events"] / gain_matrix["Total_Customers"]

    # Compute Cumulative Event Rate
    gain_matrix["Cumulative Event Rate"] = gain_matrix["Cumulative_Events"] / gain_matrix["Total_Customers"].sum()

    # Compute Separation (Difference in % Cumulative Events and Non-Events)
    gain_matrix["Separation"] = gain_matrix["% Cumulative Events"] - gain_matrix["% Cumulative Non-Events"]

    # Compute Lift (Cumulative Event Rate vs. Overall Event Rate)
    overall_event_rate = total_events / (total_events + total_non_events)
    gain_matrix["Lift"] = gain_matrix["Cumulative Event Rate"] / overall_event_rate

    # Add Dataset Name
    gain_matrix["Dataset"] = dataset_name

    return gain_matrix

# Function to calculate AUC
def calculate_auc(model, X, y):
    y_pred_proba = model.predict_proba(X)[:, 1]
    return roc_auc_score(y, y_pred_proba)

# Compute Gain Matrices
train_gain_matrix = create_gain_matrix(final_model, X_train, y_train, "Train")
test_gain_matrix = create_gain_matrix(final_model, X_test, y_test, "Test")
oot_gain_matrix = create_gain_matrix(final_model, X_oot_features, y_oot, "OOT")

# Compute AUC Scores
train_auc = calculate_auc(final_model, X_train, y_train)
test_auc = calculate_auc(final_model, X_test, y_test)
oot_auc = calculate_auc(final_model, X_oot_features, y_oot)

# Combine Gain Matrices for Display
gain_matrix_df = pd.concat([train_gain_matrix, test_gain_matrix, oot_gain_matrix])

# Display Gain Matrix Table
print("\n--- Gain Matrix (Decile Analysis) ---")
print(gain_matrix_df.to_string(index=False))

# Display AUC Scores
auc_scores_df = pd.DataFrame({
    "Dataset": ["Train", "Test", "OOT"],
    "AUC Score": [train_auc, test_auc, oot_auc]
})

print("\n--- AUC Scores ---")
print(auc_scores_df.to_string(index=False))

# Plot Cumulative Gain Chart
plt.figure(figsize=(8, 6))
plt.plot(train_gain_matrix["Decile"], train_gain_matrix["% Cumulative Events"], marker='o', label="Train")
plt.plot(test_gain_matrix["Decile"], test_gain_matrix["% Cumulative Events"], marker='s', label="Test")
plt.plot(oot_gain_matrix["Decile"], oot_gain_matrix["% Cumulative Events"], marker='^', label="OOT")

plt.xlabel("Decile (1 = Highest Risk)")
plt.ylabel("Cumulative % of Events")
plt.title("Cumulative Gain Chart Based on Deciles")
plt.legend()
plt.grid(True)
plt.show()

# Plot Lift Chart
plt.figure(figsize=(8, 6))
plt.plot(train_gain_matrix["Decile"], train_gain_matrix["Lift"], marker='o', label="Train")
plt.plot(test_gain_matrix["Decile"], test_gain_matrix["Lift"], marker='s', label="Test")
plt.plot(oot_gain_matrix["Decile"], oot_gain_matrix["Lift"], marker='^', label="OOT")

plt.xlabel("Decile (1 = Highest Risk)")
plt.ylabel("Lift")
plt.title("Lift Chart Based on Deciles")
plt.legend()
plt.grid(True)
plt.show()















Main Model

import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import roc_auc_score
import joblib

# Load Data (Ensure `Dev_data1` and `X_oot` are preloaded as DataFrames)
target_column = "mevent"  # Update with actual target variable name
ignore_features = ["cust_num", "samplingweight", "selection_time"]  # Features to ignore

# Remove ignored features from model input, but keep them in the dataset
X = Dev_data1.drop(columns=[target_column] + ignore_features)
y = Dev_data1[target_column]

# Ensure consistent Train-Test Split using random_state=1992
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1992)

# Prepare OOT dataset (Ensure features match Train/Test datasets)
y_oot = X_oot[target_column]
X_oot_features = X_oot.drop(columns=[target_column] + ignore_features)
X_oot_features = X_oot_features[X_train.columns]  # Ensure feature consistency

# Function to calculate Gini Score
def calculate_gini(model, X, y):
    y_pred = model.predict_proba(X)[:, 1]  # Get probabilities
    auc = roc_auc_score(y, y_pred)
    gini = 2 * auc - 1  # Convert AUC to Gini
    return gini

# Function to generate Scores and Odds
def generate_scores_odds(model, X):
    probs = model.predict_proba(X)[:, 1]  # Get probability of class 1
    scores = np.log(probs / (1 - probs))  # Log odds transformation
    return pd.DataFrame({"Score": scores, "Odds": probs / (1 - probs)})

# Define Hyperparameter Search Space (20 Iterations)
param_grid = {
    "learning_rate": np.linspace(0.01, 0.3, 10),
    "max_depth": np.arange(3, 10),
    "min_samples_leaf": np.arange(1, 50, 5),
    "n_estimators": np.arange(100, 1000, 100),
    "subsample": np.linspace(0.5, 1.0, 5)
}

# Gradient Boosting Classifier with fixed random_state
gb_model = GradientBoostingClassifier(random_state=1992)

# Use RandomizedSearchCV for Hyperparameter Tuning (20 Iterations) with fixed random_state
random_search = RandomizedSearchCV(
    estimator=gb_model,
    param_distributions=param_grid,
    n_iter=20,  # 20 Iterations
    scoring="roc_auc",
    cv=5,
    random_state=1992,  # Fixed random state for consistency
    verbose=1,
    n_jobs=-1
)

# Fit the Model
random_search.fit(X_train, y_train)

# Store Results for Each Iteration
results = []
for i, params in enumerate(random_search.cv_results_["params"]):
    best_model = GradientBoostingClassifier(**params, random_state=1992)  # Ensure consistent model training
    best_model.fit(X_train, y_train)

    # Compute Gini Scores
    train_gini = calculate_gini(best_model, X_train, y_train)
    test_gini = calculate_gini(best_model, X_test, y_test)
    oot_gini = calculate_gini(best_model, X_oot_features, y_oot)

    # Store iteration results
    results.append({
        "Iteration": i + 1,
        "Learning Rate": params.get("learning_rate"),
        "Max Depth": params.get("max_depth"),
        "Min Samples Leaf": params.get("min_samples_leaf"),
        "Estimators": params.get("n_estimators"),
        "Subsample": params.get("subsample"),
        "Train Gini": train_gini,
        "Test Gini": test_gini,
        "OOT Gini": oot_gini,
        "Train-OOT Gini Diff": abs(train_gini - oot_gini)
    })

# Convert results into DataFrame
results_df = pd.DataFrame(results)

# Step 5: Select Best Model (Highest Test Gini, Smallest Train-OOT Gini Difference)
best_model_row = results_df[results_df["Train-OOT Gini Diff"] < 0.05].nlargest(1, "Test Gini")

# Step 6: Train & Save the Best Model
best_params = best_model_row.iloc[0][["Learning Rate", "Max Depth", "Min Samples Leaf", "Estimators", "Subsample"]].to_dict()
final_model = GradientBoostingClassifier(
    learning_rate=best_params["Learning Rate"],
    max_depth=int(best_params["Max Depth"]),
    min_samples_leaf=int(best_params["Min Samples Leaf"]),
    n_estimators=int(best_params["Estimators"]),
    subsample=best_params["Subsample"],
    random_state=1992  # Keep the same random state
)

final_model.fit(X_train, y_train)
joblib.dump(final_model, "best_gradient_boosting.pkl")  # Save Model

# Generate Scores & Odds for all datasets
dev_scores = generate_scores_odds(final_model, X)
train_scores = generate_scores_odds(final_model, X_train)
test_scores = generate_scores_odds(final_model, X_test)
oot_scores = generate_scores_odds(final_model, X_oot_features)

# Step 7: Display Results Table
import ace_tools as tools
tools.display_dataframe_to_user(name="Gradient Boosting Hyperparameter Tuning Results", dataframe=results_df)
tools.display_dataframe_to_user(name="Dev Data Scores & Odds", dataframe=dev_scores)
tools.display_dataframe_to_user(name="Train Data Scores & Odds", dataframe=train_scores)
tools.display_dataframe_to_user(name="Test Data Scores & Odds", dataframe=test_scores)
tools.display_dataframe_to_user(name="OOT Data Scores & Odds", dataframe=oot_scores)