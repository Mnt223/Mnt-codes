import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
import random

# Set seed
SEED = 1992
np.random.seed(SEED)
random.seed(SEED)

# Step 1: Read & clean columns
raw.columns = [x.lower().strip() for x in raw.columns]
raw_OOT.columns = [x.lower().strip() for x in raw_OOT.columns]

# Step 2: Rename if needed
raw.rename(columns={'custid': 'cust_num'}, inplace=True)
raw_OOT.rename(columns={'custid': 'cust_num'}, inplace=True)

# Step 3: Define important columns
target_col = 'mevent'
weight_col = 'samplingweight'
selection_col = 'selection_month'

# Final 25 SHAP-selected features
selected_features = [
    'utilization_1m', 'cib_score_bucket', 'dsr_lending_ccpihml_1m', 'credit_limit_6m_max',
    'val_cc_totlmt_offus', 'max_offus_ln_open_mob', 'balance_active_1m', 'cnt_ln_offus',
    'cc_tran_3m_mean', 'cnt_enquiries_6m', 'reln_trb_max_final_3avg', 'billed_balance_6m_min',
    'pay_ratio_6m_mean', 'onus_unsec_bal_6m', 'sav_dep_ddp_cnt_lcy_3m_mean',
    'eop_casa_bal_avg_r13', 'payment_3m_max', 'cc_mb_tran_3m_sum', 'cc_ser_amt_1_6',
    'onus_max_pil_tenor', 'cc_ovs_amt_1m', 'utilization_3m_min', 'cc_or_amt_1_3',
    'cc_ent_tran_3m_mean', 'cc_cl_amt_6m_mean'
]

# Include target + utility columns
full_features = selected_features + [target_col, weight_col, selection_col, 'cust_num']

# Step 4: Filter raw data
df_main = raw[full_features].copy()
df_oot = raw_OOT[full_features].copy()

# Step 5: Handle categorical encoding (if needed)
for df in [df_main, df_oot]:
    for col in selected_features:
        if df[col].dtype == 'object':
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col].astype(str))

# Step 6: Impute missing values
imputer = SimpleImputer(strategy='median')
df_main[selected_features] = imputer.fit_transform(df_main[selected_features])
df_oot[selected_features] = imputer.transform(df_oot[selected_features])  # Use same stats

# Step 7: Train-Test Split (Stratified)
X = df_main[selected_features]
y = df_main[target_col]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=SEED
)

# Also extract matching metadata
train_index = X_train.index
test_index = X_test.index
df_train = df_main.loc[train_index].reset_index(drop=True)
df_test = df_main.loc[test_index].reset_index(drop=True)
df_oot = df_oot.reset_index(drop=True)

# Final Print
print(f"Train shape: {df_train.shape}, Event Ratio: {df_train[target_col].mean():.4f}")
print(f"Test shape : {df_test.shape}, Event Ratio: {df_test[target_col].mean():.4f}")
print(f"OOT shape  : {df_oot.shape}, Event Ratio: {df_oot[target_col].mean():.4f}")













def gain_matrix(input_gm):
    df = input_gm.copy()

    # Step 1: Remove rows with missing final score or mevent
    df = df[~df["final_score"].isna()]
    df = df[~df["mevent"].isna()]

    # Step 2: Bin into deciles (drop NaNs safely)
    try:
        df["deciles"] = pd.qcut(df["final_score"], 10, labels=False, duplicates='drop')
    except ValueError as e:
        print("Error in qcut: ", e)
        return None

    # Step 3: Weighted or unweighted aggregation
    if "samplingweight" in df.columns:
        gainsfreq = df.groupby(["deciles", "mevent"])["samplingweight"].sum().reset_index()
        gainsfreq = gainsfreq.rename(columns={"samplingweight": "count"})
    else:
        gainsfreq = df.groupby(["deciles", "mevent"]).size().reset_index(name="count")

    # Step 4: Pivot cleanly (only 2 mevent levels expected: 0, 1)
    pivot_df = gainsfreq.pivot(index="deciles", columns="mevent", values="count").fillna(0).reset_index()
    pivot_df.columns.name = None

    # Step 5: Rename columns based on actual content
    actual_cols = pivot_df.columns.tolist()
    if 0 in pivot_df.columns and 1 in pivot_df.columns:
        pivot_df.columns = ["Decile", "Non-Events", "Events"]
    else:
        pivot_df.columns = ["Decile"] + [str(c) for c in actual_cols[1:]]

    # Step 6: Sort in descending decile order
    pivot_df.sort_values(by="Decile", ascending=False, inplace=True)

    # Step 7: Cumulative and metrics
    pivot_df["Cum Non-Events"] = pivot_df["Non-Events"].cumsum()
    pivot_df["Cum Events"] = pivot_df["Events"].cumsum()

    total_non_events = pivot_df["Non-Events"].sum()
    total_events = pivot_df["Events"].sum()

    pivot_df["% Cum Non-Events"] = (pivot_df["Cum Non-Events"] / total_non_events) * 100
    pivot_df["% Cum Events"] = (pivot_df["Cum Events"] / total_events) * 100

    pivot_df["Event Rate"] = (pivot_df["Events"] / (pivot_df["Non-Events"] + pivot_df["Events"])) * 100
    pivot_df["Cum Event Rate"] = (
        pivot_df["Cum Events"] / (pivot_df["Cum Events"] + pivot_df["Cum Non-Events"])
    ) * 100

    pivot_df["Separation"] = pivot_df["% Cum Events"] - pivot_df["% Cum Non-Events"]

    return pivot_df













import pandas as pd
import numpy as np

# Step 1: Define your selected variables
selected_vars = [
    'utilization_3m_min', 'utilization_1m', 'billed_balance_6m_min', 'cc_cl_amt_6m_mean', 'cc_utilization_3m',
    'cc_ovs_amt_1m', 'cc_mb_tran_3m_sum', 'cc_ent_tran_3m_mean', 'cc_ser_amt_1_6', 'cc_or_amt_1_3',
    'cc_tran_3m_mean', 'payment_3m_max', 'eop_casa_bal_avg_r13', 'sav_dep_ddp_cr_bal_3m_mean',
    'sav_dep_ddp_bal_6m_mean', 'sav_dep_ddp_cnt_lcy_3m_mean', 'credit_limit_6m_max', 'val_cc_totlmt_offus',
    'onus_unsec_bal_6m', 'cnt_ln_offus', 'max_offus_ln_open_mob', 'dsr_lending_ccpilhml_1m',
    'onus_max_pil_tenor', 'cib_score_bucket', 'cnt_enquiries_6m', 'cust_mob_gdm', 'max_acct_mob_1m',
    'reln_trb_max_final_3avg', 'pay_ratio_6m_mean', 'balance_active_1m'
]

# Step 2: Subset the data
subset_df = raw[selected_vars].copy()

# Step 3: Compute correlation matrix
corr_matrix = subset_df.corr().abs()

# Step 4: Get upper triangle of the correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Step 5: Find features with correlation > 0.85
to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]

# Step 6: Final variable list after removing highly correlated ones
final_vars = [col for col in selected_vars if col not in to_drop]

# Step 7: Print final variable list as comma-separated string with quotes
print(', '.join([f"'{v}'" for v in final_vars]))