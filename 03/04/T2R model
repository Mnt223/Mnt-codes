import pandas as pd
import numpy as np

# Top 6 SHAP features
top_features = [
    'utilization_1m', 'cnt_ln_offus', 'balance_active_1m',
    'cib_score_bucket', 'cnt_enquiries_6m', 'dsr_lending_ccpihml_1m'
]

# Create score buckets
score_bins = [0, 175, 192, 208, 229, df_main['final_score'].max()]
score_labels = ['E. <= 175', 'D. 175 - 192', 'C. 192 - 208', 'B. 208 - 229', 'A. > 229']
df_main['score_band'] = pd.cut(df_main['final_score'], bins=score_bins, labels=score_labels, include_lowest=True)

# Dictionary to store final tables
feature_tables = {}

for feature in top_features:
    try:
        # Get quantile-based bins
        feat_bins = pd.qcut(df_main[feature], 5, retbins=True, duplicates='drop')[1]
        num_bins = len(feat_bins) - 1

        # Create labels
        label_base = ['A', 'B', 'C', 'D', 'E']
        feat_labels = []

        for i in range(num_bins):
            if i == 0:
                feat_labels.append(f"{label_base[i]}. > {round(feat_bins[-1], 2)}")
            elif i == num_bins - 1:
                feat_labels.append(f"{label_base[i]}. <= {round(feat_bins[1], 2)}")
            else:
                feat_labels.append(f"{label_base[i]}. {round(feat_bins[-(i+1)], 2)} - {round(feat_bins[-i], 2)}")

        # Apply binning
        df_main[f'{feature}_bucket'] = pd.cut(df_main[feature], bins=feat_bins, labels=feat_labels, include_lowest=True)

        # Create grouped table
        summary_table = df_main.groupby(['score_band', f'{feature}_bucket']).size().reset_index(name='freq')
        feature_tables[feature] = summary_table

    except Exception as e:
        feature_tables[feature] = f"Error for {feature}: {str(e)}"

feature_tables['utilization_1m']
feature_tables['cnt_ln_offus']
...



















import matplotlib.pyplot as plt

# Step 1: If not done, sort shap_importance from previous SHAP block
shap_top15 = shap_importance.sort_values(by='Mean_SHAP', ascending=True).tail(15)

# Step 2: Plot horizontal bar chart
plt.figure(figsize=(8, 6))
plt.barh(shap_top15['Feature'], shap_top15['Mean_SHAP'], color='darkcyan')
plt.xlabel('Mean |SHAP Value|', fontsize=12)
plt.title('Top 15 Features by SHAP Importance', fontsize=14, fontweight='bold')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()






# Top 6 SHAP features (replace with your actual list)
top_features = [
    'utilization_1m', 'cnt_ln_offus', 'balance_active_1m',
    'cib_score_bucket', 'cnt_enquiries_6m', 'dsr_lending_ccpihml_1m'
]

# Score band (fixed from Train-based score cutoffs)
score_bins = [0, 175, 192, 208, 229, df_main['final_score'].max()]
score_labels = ['E. <= 175', 'D. 175 - 192', 'C. 192 - 208', 'B. 208 - 229', 'A. > 229']
df_main['score_band'] = pd.cut(df_main['final_score'], bins=score_bins, labels=score_labels, include_lowest=True)

# Generate tables for each feature
feature_tables = {}

for feature in top_features:
    # Create quantile-based feature buckets
    feat_bins = pd.qcut(df_main[feature], 5, retbins=True, duplicates='drop')[1]
    feat_labels = [
        f"A. > {round(feat_bins[-1], 2)}",
        f"B. {round(feat_bins[-2], 2)} - {round(feat_bins[-1], 2)}",
        f"C. {round(feat_bins[-3], 2)} - {round(feat_bins[-2], 2)}",
        f"D. {round(feat_bins[-4], 2)} - {round(feat_bins[-3], 2)}",
        f"E. <= {round(feat_bins[1], 2)}"
    ]
    df_main[f'{feature}_bucket'] = pd.cut(df_main[feature], bins=feat_bins, labels=feat_labels, include_lowest=True)

    # Group and count
    temp = df_main.groupby(['score_band', f'{feature}_bucket']).size().reset_index(name='freq')
    feature_tables[feature] = temp
















import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve

# Step 1: KS calculator
def calculate_ks(df, score_col='final_score', target_col='mevent'):
    fpr, tpr, _ = roc_curve(df[target_col], df[score_col])
    return round(max(tpr - fpr), 4)

# Step 2: Calculate KS for Train, Test, OOT
ks_table = pd.DataFrame({
    'Dataset': ['Train', 'Test', 'Out-of-Time'],
    'KS Value': [
        calculate_ks(df_train),
        calculate_ks(df_test),
        calculate_ks(df_oot)
    ]
})

# Step 3: Show KS Table
print("KS Value Summary:")
display(ks_table)

# Step 4: Plot KS Bar Chart
plt.figure(figsize=(6, 4))
plt.bar(ks_table['Dataset'], ks_table['KS Value'], color=['steelblue', 'orange', 'green'])
plt.title("KS Value Comparison", fontsize=14, fontweight='bold')
plt.ylabel("KS Value")
plt.ylim(0, 1)
for i, v in enumerate(ks_table['KS Value']):
    plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontweight='bold')
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()












import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def compute_lift_per_decile(df, score_col='final_score', target_col='mevent', deciles=10):
    df = df[[score_col, target_col]].copy()
    
    # Sort descending to ensure decile 1 = highest score
    df['decile'] = pd.qcut(
        df[score_col].rank(method='first', ascending=False),
        deciles,
        labels=False,
        duplicates='drop'
    ) + 1
    
    gain_table = df.groupby('decile').agg(
        total=('decile', 'count'),
        events=(target_col, 'sum')
    ).sort_index()

    baseline_rate = df[target_col].mean()
    gain_table['event_rate'] = gain_table['events'] / gain_table['total']
    gain_table['lift'] = gain_table['event_rate'] / baseline_rate
    return gain_table['lift'].values

# Compute lift arrays
lift_train = compute_lift_per_decile(df_train)
lift_test = compute_lift_per_decile(df_test)
lift_oot = compute_lift_per_decile(df_oot)

# Plotting
plt.figure(figsize=(8, 5))
x = np.arange(1, 11)

plt.plot(x, lift_train, marker='o', label='Train')
plt.plot(x, lift_test, marker='o', label='Test')
plt.plot(x, lift_oot, marker='o', label='OOT')

plt.title("Lift Chart by Decile (Train vs Test vs OOT)", fontsize=14, fontweight='bold')
plt.xlabel("Decile (1 = Best Score)", fontsize=12)
plt.ylabel("Lift", fontsize=12)
plt.xticks(x)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()



import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve

def plot_lift_ks(df, score_col='final_score', target_col='mevent', dataset_label='Train'):
    df = df[[score_col, target_col]].copy()
    df['decile'] = pd.qcut(df[score_col].rank(method='first'), 10, labels=False, duplicates='drop')
    df = df.sort_values(by=score_col, ascending=False).reset_index(drop=True)

    # Lift Chart
    total_events = df[target_col].sum()
    df['cumulative_events'] = df[target_col].cumsum()
    df['population'] = np.arange(1, len(df) + 1)
    df['cumulative_event_rate'] = df['cumulative_events'] / total_events
    df['cumulative_population_rate'] = df['population'] / len(df)
    df['lift'] = df['cumulative_event_rate'] / df['cumulative_population_rate']

    # KS Curve
    fpr, tpr, thresholds = roc_curve(df[target_col], df[score_col])
    ks = max(tpr - fpr)

    # Plotting
    fig, axs = plt.subplots(1, 2, figsize=(12, 5))
    fig.suptitle(f"{dataset_label} - Lift and KS Curve", fontsize=14, fontweight='bold')

    # 1. Lift Chart
    axs[0].plot(df['cumulative_population_rate'], df['lift'], label='Lift', color='blue')
    axs[0].hlines(1, 0, 1, colors='gray', linestyles='dashed')
    axs[0].set_title('Lift Chart')
    axs[0].set_xlabel('% of Population')
    axs[0].set_ylabel('Lift')
    axs[0].grid(True)

    # 2. KS Chart
    axs[1].plot(fpr, label='False Positive Rate', color='red')
    axs[1].plot(tpr, label='True Positive Rate', color='green')
    axs[1].fill_between(np.arange(len(fpr)), tpr - fpr, color='lightgray', alpha=0.5)
    axs[1].set_title(f'KS Curve (KS = {ks:.3f})')
    axs[1].set_xlabel('Threshold Index')
    axs[1].set_ylabel('Rate')
    axs[1].legend()
    axs[1].grid(True)

    plt.tight_layout()
    plt.show()

plot_lift_ks(df_train, dataset_label='Train')
plot_lift_ks(df_test, dataset_label='Test')
plot_lift_ks(df_oot, dataset_label='OOT')














def calculate_csi_for_feature(feature, base_df, target_df, bins=5):
    base = base_df[[feature]].copy()
    target = target_df[[feature]].copy()
    
    # Handle categoricals separately
    if str(base[feature].dtype) == 'object' or base[feature].nunique() <= 10:
        base[feature] = base[feature].astype(str)
        target[feature] = target[feature].astype(str)
        all_values = list(set(base[feature].unique()).union(set(target[feature].unique())))
        base_dist = base[feature].value_counts(normalize=True).reindex(all_values, fill_value=0.0001)
        target_dist = target[feature].value_counts(normalize=True).reindex(all_values, fill_value=0.0001)
    else:
        edges = pd.qcut(base[feature], bins, duplicates='drop', retbins=True)[1]
        base['bucket'] = pd.cut(base[feature], bins=edges, include_lowest=True)
        target['bucket'] = pd.cut(target[feature], bins=edges, include_lowest=True)
        all_values = base['bucket'].cat.categories
        base_dist = base['bucket'].value_counts(normalize=True).reindex(all_values, fill_value=0.0001)
        target_dist = target['bucket'].value_counts(normalize=True).reindex(all_values, fill_value=0.0001)

    csi = ((base_dist - target_dist) * np.log(base_dist / target_dist)).sum()
    return round(csi, 4)

# Example: Top 10 features (replace with your actual list)
top_features = [
    'utilization_1m', 'cib_score_bucket', 'dsr_lending_ccpihml_1m', 'credit_limit_6m_max',
    'val_cc_totlmt_offus', 'max_offus_ln_open_mob', 'balance_active_1m',
    'cnt_ln_offus', 'cc_tran_3m_mean', 'cnt_enquiries_6m'
]

# Run CSI for each
csi_results = []
for feat in top_features:
    csi = calculate_csi_for_feature(feat, df_train, df_oot)
    csi_results.append({'Feature': feat, 'CSI': csi})

# Create DataFrame
csi_df = pd.DataFrame(csi_results).sort_values(by='CSI', ascending=False).reset_index(drop=True)
print("\nCharacteristic Stability Index (CSI):")
display(csi_df)

















import shap

# Step 1: Initialize SHAP explainer for CatBoost
explainer = shap.TreeExplainer(final_model)

# Step 2: Calculate SHAP values on train data
shap_values = explainer.shap_values(df_train[selected_features])

# Step 3: Convert SHAP values to importance DataFrame
shap_df = pd.DataFrame(shap_values, columns=selected_features)
shap_importance = shap_df.abs().mean().sort_values(ascending=False).reset_index()
shap_importance.columns = ['Feature', 'Mean_SHAP']

# Display top 10
top_10_features = shap_importance.head(10)
print("Top 10 Features by Mean SHAP Value:")
display(top_10_features)

shap.summary_plot(shap_values, df_train[selected_features], max_display=10)











import numpy as np
import pandas as pd

# Step 1: Define fixed score bins from train
score_bins = pd.qcut(df_train['final_score'], 5, retbins=True, duplicates='drop')[1]
score_labels = [
    f"{round(score_bins[i], 1)}–{round(score_bins[i+1], 1)}"
    for i in range(len(score_bins)-1)
]

# Step 2: Function to generate formatted PSI table
def generate_excel_style_psi(base_df, target_df, score_col='final_score', bins=score_bins, labels=score_labels):
    base_df = base_df[[score_col]].copy()
    target_df = target_df[[score_col]].copy()

    base_df['bucket'] = pd.cut(base_df[score_col], bins=bins, labels=labels, include_lowest=True)
    target_df['bucket'] = pd.cut(target_df[score_col], bins=bins, labels=labels, include_lowest=True)

    dist_base = base_df['bucket'].value_counts(normalize=True).sort_index()
    dist_target = target_df['bucket'].value_counts(normalize=True).sort_index()

    df = pd.DataFrame({
        'Score_Band': dist_base.index,
        'Train_%': (dist_base * 100).round(4),
        'Target_%': (dist_target * 100).round(4)
    })

    # Avoid division by zero
    df['Train_%'] = df['Train_%'].replace(0, 0.0001)
    df['Target_%'] = df['Target_%'].replace(0, 0.0001)

    df['A - B'] = (df['Train_%'] - df['Target_%']).round(4)
    df['Ln(A/B)'] = np.log(df['Train_%'] / df['Target_%']).round(4)
    df['PSI'] = ((df['Train_%'] - df['Target_%']) * df['Ln(A/B)'] / 100).round(6)

    return df, df['PSI'].sum().round(4)

psi_excel_test, psi_val_test = generate_excel_style_psi(df_train, df_test)
psi_excel_oot, psi_val_oot = generate_excel_style_psi(df_train, df_oot)

print(f"Train vs Test PSI: {psi_val_test}")
display(psi_excel_test)

print(f"\nTrain vs OOT PSI: {psi_val_oot}")
display(psi_excel_oot)

















def calculate_psi(base, target, score_col='final_score', buckets=10, bucket_type='quantile'):
    base = base[[score_col]].copy()
    target = target[[score_col]].copy()
    
    if bucket_type == 'quantile':
        bins = pd.qcut(base[score_col], q=buckets, duplicates='drop', retbins=True)[1]
    elif bucket_type == 'fixed':
        bins = np.linspace(base[score_col].min(), base[score_col].max(), buckets + 1)
    else:
        raise ValueError("bucket_type must be 'quantile' or 'fixed'")

    base['bucket'] = pd.cut(base[score_col], bins=bins, include_lowest=True)
    target['bucket'] = pd.cut(target[score_col], bins=bins, include_lowest=True)

    dist_base = base['bucket'].value_counts(normalize=True).sort_index()
    dist_target = target['bucket'].value_counts(normalize=True).sort_index()

    psi_df = pd.DataFrame({
        'Score_Bucket': dist_base.index.astype(str),
        'Base_Dist': dist_base.values,
        'Target_Dist': dist_target.values
    })

    psi_df['Base_Dist'] = psi_df['Base_Dist'].replace(0, 0.0001)
    psi_df['Target_Dist'] = psi_df['Target_Dist'].replace(0, 0.0001)

    psi_df['PSI'] = (psi_df['Base_Dist'] - psi_df['Target_Dist']) * np.log(psi_df['Base_Dist'] / psi_df['Target_Dist'])
    total_psi = psi_df['PSI'].sum()

    return total_psi, psi_df

psi_test_val, psi_test_df = calculate_psi(df_train, df_test)
psi_oot_val, psi_oot_df = calculate_psi(df_train, df_oot)

print(f"\nPSI - Train vs Test: {round(psi_test_val, 4)}")
display(psi_test_df)

print(f"\nPSI - Train vs OOT : {round(psi_oot_val, 4)}")
display(psi_oot_df)














# Step 1: Create decile bins using actual final_score
decile_edges = pd.qcut(df_train['final_score'], 10, retbins=True, duplicates='drop')[1]

# Step 2: Apply consistent deciles across all datasets
def assign_deciles(df, score_col='final_score', bins=decile_edges):
    return pd.cut(df[score_col], bins=bins, labels=list(range(10, 0, -1)), include_lowest=True)

# Step 3: Apply and safely convert to integer using .cat.codes + 1
for df in [df_train, df_test, df_oot, df_main]:
    df['decile'] = assign_deciles(df)
    df['decile'] = df['decile'].cat.codes + 1  # Handles NaNs gracefully











def generate_gain_table_from_decile(df, target_col='mevent', decile_col='decile'):
    gain_table = df.groupby(decile_col).agg(
        Total_Customers=(target_col, 'count'),
        Events=(target_col, 'sum')
    ).sort_index(ascending=True).reset_index()

    gain_table['Non_Events'] = gain_table['Total_Customers'] - gain_table['Events']
    gain_table['Event_Rate'] = gain_table['Events'] / gain_table['Total_Customers']
    gain_table['Cumulative_Events'] = gain_table['Events'].cumsum()
    gain_table['Cumulative_Customers'] = gain_table['Total_Customers'].cumsum()
    gain_table['Cumulative_Event_Rate'] = gain_table['Cumulative_Events'] / gain_table['Cumulative_Customers']
    total_events = gain_table['Events'].sum()
    total_customers = gain_table['Total_Customers'].sum()
    baseline_event_rate = total_events / total_customers
    gain_table['Lift'] = gain_table['Event_Rate'] / baseline_event_rate
    gain_table['Cumulative_Perc_Events'] = gain_table['Cumulative_Events'] / total_events
    gain_table['Cumulative_Perc_Customers'] = gain_table['Cumulative_Customers'] / total_customers

    return gain_table

gain_train = generate_gain_table(df_train)
gain_test = generate_gain_table(df_test)
gain_oot = generate_gain_table(df_oot)

print("Train Gain Table:")
display(gain_train)

print("Test Gain Table:")
display(gain_test)

print("OOT Gain Table:")
display(gain_oot)












import numpy as np

# Constants (can be adjusted)
BASE_SCORE = 200
FACTOR = 28.8539

# Convert probability score to log-odds score
def convert_to_score(prob_series):
    odds = prob_series / (1 - prob_series)
    return BASE_SCORE + FACTOR * np.log(odds)

# Apply to all datasets
df_train['final_score'] = convert_to_score(df_train['score_final'])
df_test['final_score'] = convert_to_score(df_test['score_final'])
df_oot['final_score'] = convert_to_score(df_oot['score_final'])
df_main['final_score'] = convert_to_score(df_main['score_final'])

# Optional: Check score distribution
print("Train Score Distribution:", df_train['final_score'].describe())
print("Test Score Distribution :", df_test['final_score'].describe())
print("OOT Score Distribution  :", df_oot['final_score'].describe())

























3rd trial: final used

import optuna
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score
import pandas as pd
import numpy as np

SEED = 1992
np.random.seed(SEED)

# Function to calculate Gini
def gini_score(y_true, y_prob):
    return 2 * roc_auc_score(y_true, y_prob) - 1

# Store all trial results
trial_results = []

# Define Optuna objective
def objective(trial):
    params = {
        'depth': trial.suggest_int('depth', 3, 7),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 7),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'iterations': 300,
        'random_seed': SEED,
        'verbose': 0
    }

    model = CatBoostClassifier(**params)
    model.fit(df_train[selected_features], df_train['mevent'])

    # Predictions
    train_prob = model.predict_proba(df_train[selected_features])[:, 1]
    test_prob = model.predict_proba(df_test[selected_features])[:, 1]
    oot_prob = model.predict_proba(df_oot[selected_features])[:, 1]

    # Gini scores
    gini_train = gini_score(df_train['mevent'], train_prob)
    gini_test = gini_score(df_test['mevent'], test_prob)
    gini_oot = gini_score(df_oot['mevent'], oot_prob)
    gini_gap = abs(gini_train - gini_oot)

    # Save to list
    trial_results.append({
        'Iteration': len(trial_results) + 1,
        'depth': params['depth'],
        'learning_rate': round(params['learning_rate'], 4),
        'l2_leaf_reg': params['l2_leaf_reg'],
        'subsample': round(params['subsample'], 2),
        'Train_Gini': round(gini_train, 4),
        'Test_Gini': round(gini_test, 4),
        'OOT_Gini': round(gini_oot, 4),
        'Gini_Gap': round(gini_gap, 4)
    })

    return gini_test

# Run Optuna with 100 trials
study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=SEED))
study.optimize(objective, n_trials=100, show_progress_bar=True)

# Step 1: Create full table from all 100 trials
gini_table_all = pd.DataFrame(trial_results)

# Step 2: Select top 20 by Test Gini
gini_table_top20 = gini_table_all.sort_values(by='Test_Gini', ascending=False).head(20).reset_index(drop=True)
print("\nTop 20 Models by Test Gini:")
print(gini_table_top20)

# Step 3: From top 20, select best model with minimal Train–OOT Gini gap
best_row = gini_table_top20.sort_values(by='Gini_Gap', ascending=True).iloc[0]
print("\nSelected Best Model:")
print(best_row)

# Step 4: Retrain final best model
final_model = CatBoostClassifier(
    depth=int(best_row['depth']),
    learning_rate=best_row['learning_rate'],
    l2_leaf_reg=int(best_row['l2_leaf_reg']),
    subsample=best_row['subsample'],
    iterations=300,
    random_seed=SEED,
    verbose=0
)
final_model.fit(df_train[selected_features], df_train['mevent'])

# Step 5: Score all datasets
df_train['score_final'] = final_model.predict_proba(df_train[selected_features])[:, 1]
df_test['score_final'] = final_model.predict_proba(df_test[selected_features])[:, 1]
df_oot['score_final'] = final_model.predict_proba(df_oot[selected_features])[:, 1]
df_main['score_final'] = final_model.predict_proba(df_main[selected_features])[:, 1]

# Final Gini print
def show_ginis():
    print("\nFinal Gini Scores:")
    print(f"Train Gini: {gini_score(df_train['mevent'], df_train['score_final']):.4f}")
    print(f"Test  Gini: {gini_score(df_test['mevent'], df_test['score_final']):.4f}")
    print(f"OOT   Gini: {gini_score(df_oot['mevent'], df_oot['score_final']):.4f}")

show_ginis()




















2nd trial: 
import optuna
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score
import numpy as np
import pandas as pd

SEED = 1992
np.random.seed(SEED)

# Objective function for Optuna
def objective(trial):
    params = {
        'depth': trial.suggest_int('depth', 3, 7),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 7),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'iterations': 300,
        'random_seed': SEED,
        'verbose': 0
    }

    model = CatBoostClassifier(**params)
    model.fit(df_train[selected_features], df_train['mevent'])

    test_probs = model.predict_proba(df_test[selected_features])[:, 1]
    test_auc = roc_auc_score(df_test['mevent'], test_probs)
    test_gini = 2 * test_auc - 1

    return test_gini  # Maximize Gini

# Run Optuna study
study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=SEED))
study.optimize(objective, n_trials=30, show_progress_bar=True)

# Show best hyperparameters
print("\nBest hyperparameters:")
print(study.best_trial.params)
print(f"Best Test Gini: {round(study.best_value, 4)}")

# Retrain best model on df_train
best_params = study.best_trial.params
best_params.update({'iterations': 300, 'random_seed': SEED, 'verbose': 0})

final_model = CatBoostClassifier(**best_params)
final_model.fit(df_train[selected_features], df_train['mevent'])

# Score all datasets
df_train['score_final'] = final_model.predict_proba(df_train[selected_features])[:, 1]
df_test['score_final'] = final_model.predict_proba(df_test[selected_features])[:, 1]
df_oot['score_final'] = final_model.predict_proba(df_oot[selected_features])[:, 1]
df_main['score_final'] = final_model.predict_proba(df_main[selected_features])[:, 1]

# Final Gini calculation
def gini(y_true, y_score):
    return round(2 * roc_auc_score(y_true, y_score) - 1, 4)

print("\nFinal Gini Scores:")
print(f"Train Gini: {gini(df_train['mevent'], df_train['score_final'])}")
print(f"Test  Gini: {gini(df_test['mevent'], df_test['score_final'])}")
print(f"OOT   Gini: {gini(df_oot['mevent'], df_oot['score_final'])}")
















1st trial:
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score
import pandas as pd
import numpy as np
import random

SEED = 1992
random.seed(SEED)
np.random.seed(SEED)

features = selected_features
target = 'mevent'

results = []

# Step 1: Run 20 iterations
for i in range(1, 21):
    params = {
        'depth': random.choice([3, 4, 5, 6, 7]),
        'learning_rate': random.choice([0.01, 0.05, 0.1]),
        'l2_leaf_reg': random.choice([1, 3, 5]),
        'iterations': 300,
        'verbose': 0,
        'random_seed': SEED
    }

    model = CatBoostClassifier(**params)
    model.fit(df_train[features], df_train[target])

    # Predict probs
    train_score = model.predict_proba(df_train[features])[:, 1]
    test_score = model.predict_proba(df_test[features])[:, 1]
    oot_score = model.predict_proba(df_oot[features])[:, 1]

    # Compute AUC & Gini
    auc_train = roc_auc_score(df_train[target], train_score)
    auc_test = roc_auc_score(df_test[target], test_score)
    auc_oot = roc_auc_score(df_oot[target], oot_score)

    gini_train = 2 * auc_train - 1
    gini_test = 2 * auc_test - 1
    gini_oot = 2 * auc_oot - 1
    gini_gap = abs(gini_train - gini_oot)

    results.append({
        'Iteration': i,
        'depth': params['depth'],
        'learning_rate': params['learning_rate'],
        'l2_leaf_reg': params['l2_leaf_reg'],
        'Train_Gini': round(gini_train, 4),
        'Test_Gini': round(gini_test, 4),
        'OOT_Gini': round(gini_oot, 4),
        'Gini_Gap': round(gini_gap, 4)
    })

# Step 2: Build final table
gini_table = pd.DataFrame(results)

# Step 3: Select best iteration (highest Test_Gini with lowest Gini_Gap)
best_row = gini_table.sort_values(by=['Test_Gini', 'Gini_Gap'], ascending=[False, True]).iloc[0]
print("\nBest Iteration Chosen:")
print(best_row)

# Step 4: Retrain best model
best_model = CatBoostClassifier(
    depth=int(best_row['depth']),
    learning_rate=best_row['learning_rate'],
    l2_leaf_reg=best_row['l2_leaf_reg'],
    iterations=300,
    verbose=0,
    random_seed=SEED
)
best_model.fit(df_train[features], df_train[target])

# Step 5: Predict final scores on all datasets
df_train['score_final'] = best_model.predict_proba(df_train[features])[:, 1]
df_test['score_final'] = best_model.predict_proba(df_test[features])[:, 1]
df_oot['score_final'] = best_model.predict_proba(df_oot[features])[:, 1]
df_main['score_final'] = best_model.predict_proba(df_main[features])[:, 1]

# Final Output
print("\nFinal Scoring Completed on: Train, Test, OOT, and Full Base (df_main)")

























from pycaret.classification import setup, compare_models

# Step 1: Prepare data for PyCaret
df_pycaret = df_train[selected_features + ['mevent']].copy()

# Step 2: PyCaret setup
pycaret_setup = setup(
    data=df_pycaret,
    target='mevent',
    session_id=1992,
    normalize=False,        # No scaling for tree models
    silent=True,
    verbose=False,
    fold=5,
    use_gpu=False,
    train_size=1.0          # Use full data for comparing models
)

# Step 3: Compare models sorted by AUC
top_model = compare_models(sort='AUC')












import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
import random

# Set seed
SEED = 1992
np.random.seed(SEED)
random.seed(SEED)

# Step 1: Read & clean columns
raw.columns = [x.lower().strip() for x in raw.columns]
raw_OOT.columns = [x.lower().strip() for x in raw_OOT.columns]

# Step 2: Rename if needed
raw.rename(columns={'custid': 'cust_num'}, inplace=True)
raw_OOT.rename(columns={'custid': 'cust_num'}, inplace=True)

# Step 3: Define important columns
target_col = 'mevent'
weight_col = 'samplingweight'
selection_col = 'selection_month'

# Final 25 SHAP-selected features
selected_features = [
    'utilization_1m', 'cib_score_bucket', 'dsr_lending_ccpihml_1m', 'credit_limit_6m_max',
    'val_cc_totlmt_offus', 'max_offus_ln_open_mob', 'balance_active_1m', 'cnt_ln_offus',
    'cc_tran_3m_mean', 'cnt_enquiries_6m', 'reln_trb_max_final_3avg', 'billed_balance_6m_min',
    'pay_ratio_6m_mean', 'onus_unsec_bal_6m', 'sav_dep_ddp_cnt_lcy_3m_mean',
    'eop_casa_bal_avg_r13', 'payment_3m_max', 'cc_mb_tran_3m_sum', 'cc_ser_amt_1_6',
    'onus_max_pil_tenor', 'cc_ovs_amt_1m', 'utilization_3m_min', 'cc_or_amt_1_3',
    'cc_ent_tran_3m_mean', 'cc_cl_amt_6m_mean'
]

# Include target + utility columns
full_features = selected_features + [target_col, weight_col, selection_col, 'cust_num']

# Step 4: Filter raw data
df_main = raw[full_features].copy()
df_oot = raw_OOT[full_features].copy()

# Step 5: Handle categorical encoding (if needed)
for df in [df_main, df_oot]:
    for col in selected_features:
        if df[col].dtype == 'object':
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col].astype(str))

# Step 6: Impute missing values
imputer = SimpleImputer(strategy='median')
df_main[selected_features] = imputer.fit_transform(df_main[selected_features])
df_oot[selected_features] = imputer.transform(df_oot[selected_features])  # Use same stats

# Step 7: Train-Test Split (Stratified)
X = df_main[selected_features]
y = df_main[target_col]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=SEED
)

# Also extract matching metadata
train_index = X_train.index
test_index = X_test.index
df_train = df_main.loc[train_index].reset_index(drop=True)
df_test = df_main.loc[test_index].reset_index(drop=True)
df_oot = df_oot.reset_index(drop=True)

# Final Print
print(f"Train shape: {df_train.shape}, Event Ratio: {df_train[target_col].mean():.4f}")
print(f"Test shape : {df_test.shape}, Event Ratio: {df_test[target_col].mean():.4f}")
print(f"OOT shape  : {df_oot.shape}, Event Ratio: {df_oot[target_col].mean():.4f}")













def gain_matrix(input_gm):
    df = input_gm.copy()

    # Step 1: Remove rows with missing final score or mevent
    df = df[~df["final_score"].isna()]
    df = df[~df["mevent"].isna()]

    # Step 2: Bin into deciles (drop NaNs safely)
    try:
        df["deciles"] = pd.qcut(df["final_score"], 10, labels=False, duplicates='drop')
    except ValueError as e:
        print("Error in qcut: ", e)
        return None

    # Step 3: Weighted or unweighted aggregation
    if "samplingweight" in df.columns:
        gainsfreq = df.groupby(["deciles", "mevent"])["samplingweight"].sum().reset_index()
        gainsfreq = gainsfreq.rename(columns={"samplingweight": "count"})
    else:
        gainsfreq = df.groupby(["deciles", "mevent"]).size().reset_index(name="count")

    # Step 4: Pivot cleanly (only 2 mevent levels expected: 0, 1)
    pivot_df = gainsfreq.pivot(index="deciles", columns="mevent", values="count").fillna(0).reset_index()
    pivot_df.columns.name = None

    # Step 5: Rename columns based on actual content
    actual_cols = pivot_df.columns.tolist()
    if 0 in pivot_df.columns and 1 in pivot_df.columns:
        pivot_df.columns = ["Decile", "Non-Events", "Events"]
    else:
        pivot_df.columns = ["Decile"] + [str(c) for c in actual_cols[1:]]

    # Step 6: Sort in descending decile order
    pivot_df.sort_values(by="Decile", ascending=False, inplace=True)

    # Step 7: Cumulative and metrics
    pivot_df["Cum Non-Events"] = pivot_df["Non-Events"].cumsum()
    pivot_df["Cum Events"] = pivot_df["Events"].cumsum()

    total_non_events = pivot_df["Non-Events"].sum()
    total_events = pivot_df["Events"].sum()

    pivot_df["% Cum Non-Events"] = (pivot_df["Cum Non-Events"] / total_non_events) * 100
    pivot_df["% Cum Events"] = (pivot_df["Cum Events"] / total_events) * 100

    pivot_df["Event Rate"] = (pivot_df["Events"] / (pivot_df["Non-Events"] + pivot_df["Events"])) * 100
    pivot_df["Cum Event Rate"] = (
        pivot_df["Cum Events"] / (pivot_df["Cum Events"] + pivot_df["Cum Non-Events"])
    ) * 100

    pivot_df["Separation"] = pivot_df["% Cum Events"] - pivot_df["% Cum Non-Events"]

    return pivot_df













import pandas as pd
import numpy as np

# Step 1: Define your selected variables
selected_vars = [
    'utilization_3m_min', 'utilization_1m', 'billed_balance_6m_min', 'cc_cl_amt_6m_mean', 'cc_utilization_3m',
    'cc_ovs_amt_1m', 'cc_mb_tran_3m_sum', 'cc_ent_tran_3m_mean', 'cc_ser_amt_1_6', 'cc_or_amt_1_3',
    'cc_tran_3m_mean', 'payment_3m_max', 'eop_casa_bal_avg_r13', 'sav_dep_ddp_cr_bal_3m_mean',
    'sav_dep_ddp_bal_6m_mean', 'sav_dep_ddp_cnt_lcy_3m_mean', 'credit_limit_6m_max', 'val_cc_totlmt_offus',
    'onus_unsec_bal_6m', 'cnt_ln_offus', 'max_offus_ln_open_mob', 'dsr_lending_ccpilhml_1m',
    'onus_max_pil_tenor', 'cib_score_bucket', 'cnt_enquiries_6m', 'cust_mob_gdm', 'max_acct_mob_1m',
    'reln_trb_max_final_3avg', 'pay_ratio_6m_mean', 'balance_active_1m'
]

# Step 2: Subset the data
subset_df = raw[selected_vars].copy()

# Step 3: Compute correlation matrix
corr_matrix = subset_df.corr().abs()

# Step 4: Get upper triangle of the correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Step 5: Find features with correlation > 0.85
to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]

# Step 6: Final variable list after removing highly correlated ones
final_vars = [col for col in selected_vars if col not in to_drop]

# Step 7: Print final variable list as comma-separated string with quotes
print(', '.join([f"'{v}'" for v in final_vars]))