from polyfuzz import PolyFuzz
from polyfuzz.models import TFIDF, RapidFuzz, EditDistance
import pandas as pd
import re
from jellyfish import jaro_winkler_similarity
from tqdm import tqdm
import matplotlib.pyplot as plt


# Function for preprocessing
def preprocess(df, column_name, list_space_cmp):
    # Convert to uppercase
    df[column_name] = df[column_name].apply(lambda x: str(x).upper())
    # Remove suffixes
    for cmp in list_space_cmp:
        df[column_name] = df[column_name].str.replace(cmp, " ", regex=False)
    # Remove special characters and normalize spaces
    df[column_name] = df[column_name].apply(lambda x: re.sub(r"[^A-Z0-9]", " ", x))
    df[column_name] = df[column_name].apply(lambda x: re.sub(r" +", " ", x).strip())
    return df


# Function to process data in chunks with a progress bar
def match_in_chunks(from_list, to_list, model, chunk_size=5000, algorithm_name="Algorithm"):
    results = []
    print(f"Running {algorithm_name} matching...")
    for i in tqdm(range(0, len(from_list), chunk_size), desc=f"{algorithm_name} Progress"):
        from_chunk = from_list[i:i + chunk_size]
        model.match(from_chunk, to_list)
        chunk_results = model.get_matches()
        results.append(chunk_results)
    return pd.concat(results, ignore_index=True)


# Function to save precision-recall plots
def save_precision_recall_plot(model, file_name):
    print(f"Generating precision-recall plot for {file_name.split('_')[0]}...")
    model.visualize_precision_recall()
    plt.savefig(file_name)
    print(f"Precision-recall plot saved as '{file_name}'.")


# Load data
df = pd.read_csv('file_company.csv')  # Unstructured employer names
df_master = pd.read_csv('file_master.csv')  # Structured employer names

# Drop missing values
df = df.dropna()
df_master = df_master.dropna()

# List of suffixes to clean
list_space_cmp = [
    'SOLUTIONS', 'SOLUTION', 'SERVICES', 'SERVICE', 'PVTLTD',
    'PVT LTD', 'PRIVATELIMITED', 'PRIVATE', 'PRIVAT', 'LTD',
    'PVT', 'LLP', 'INTL', 'LIMTED', 'LIMITED', 'LIMITE',
    'CORPORATE', 'INC', 'GROUP', 'ENTERPRISE', 'CORPORATION',
    'CORP', 'COMPANY'
]

# Preprocess columns
df = preprocess(df, 'company_check', list_space_cmp)
df_master = preprocess(df_master, 'company_List', list_space_cmp)

# Create lists for matching
from_list = df['company_check'].tolist()
to_list = df_master['company_List'].tolist()

# TF-IDF Matching
tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0)
model_tfidf = PolyFuzz(tfidf)
tfidf_results = match_in_chunks(from_list, to_list, model_tfidf, chunk_size=5000, algorithm_name="TF-IDF")
tfidf_results[['From', 'To', 'Similarity']].to_csv('matched_employers_tfidf.csv', index=False, sep='|')
save_precision_recall_plot(model_tfidf, "tfidf_precision_recall.png")

# RapidFuzz Matching
rapidfuzz_matcher = RapidFuzz(n_jobs=1)
model_rapidfuzz = PolyFuzz(rapidfuzz_matcher)
rapidfuzz_results = match_in_chunks(from_list, to_list, model_rapidfuzz, chunk_size=5000, algorithm_name="RapidFuzz")
rapidfuzz_results[['From', 'To', 'Similarity']].to_csv('matched_employers_rapidfuzz.csv', index=False, sep='|')
save_precision_recall_plot(model_rapidfuzz, "rapidfuzz_precision_recall.png")

# Jellyfish Matching (Jaro-Winkler)
jellyfish_matcher = EditDistance(n_jobs=1, scorer=jaro_winkler_similarity)
model_jellyfish = PolyFuzz(jellyfish_matcher)
jellyfish_results = match_in_chunks(from_list, to_list, model_jellyfish, chunk_size=5000, algorithm_name="Jellyfish")
jellyfish_results[['From', 'To', 'Similarity']].to_csv('matched_employers_jellyfish.csv', index=False, sep='|')
save_precision_recall_plot(model_jellyfish, "jellyfish_precision_recall.png")








from polyfuzz import PolyFuzz
from polyfuzz.models import TFIDF, RapidFuzz, EditDistance
import pandas as pd
import re
from jellyfish import jaro_winkler_similarity
from tqdm import tqdm

# Load data
df = pd.read_csv('file_company.csv')  # Unstructured employer names
df_master = pd.read_csv('file_master.csv')  # Structured employer names

# Drop missing values
df = df.dropna()
df_master = df_master.dropna()

# Convert text to uppercase
df['company_check'] = df['company_check'].apply(lambda x: str(x).upper())
df_master['company_List'] = df_master['company_List'].apply(lambda x: str(x).upper())

# List of suffixes to clean
list_space_cmp = [
    'SOLUTIONS', 'SOLUTION', 'SERVICES', 'SERVICE', 'PVTLTD',
    'PVT LTD', 'PRIVATELIMITED', 'PRIVATE', 'PRIVAT', 'LTD',
    'PVT', 'LLP', 'INTL', 'LIMTED', 'LIMITED', 'LIMITE',
    'CORPORATE', 'INC', 'GROUP', 'ENTERPRISE', 'CORPORATION',
    'CORP', 'COMPANY'
]

# Replace unwanted suffixes
def replace_char(df_column, old_value):
    return df_column.str.replace(old_value, ' ', regex=False)

for cmp in list_space_cmp:
    df['company_check'] = replace_char(df['company_check'], cmp)
    df_master['company_List'] = replace_char(df_master['company_List'], cmp)

# Remove non-alphanumeric characters and normalize spaces
df['company_check_cln'] = df['company_check'].apply(lambda x: re.sub(r"[^a-zA-Z0-9]", " ", x))
df['company_check_cln'] = df['company_check_cln'].apply(lambda x: re.sub(r" +", " ", x).strip())

df_master['company_List_cln'] = df_master['company_List'].apply(lambda x: re.sub(r"[^a-zA-Z0-9]", " ", x))
df_master['company_List_cln'] = df_master['company_List_cln'].apply(lambda x: re.sub(r" +", " ", x).strip())

# Create lists for matching
from_list = df['company_check_cln'].tolist()
to_list = df_master['company_List_cln'].tolist()

# Chunking for large datasets
chunk_size = 5000  # Adjust chunk size based on system memory

# TF-IDF Matching with Precision-Recall Visualization
results = []
tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0)
model = PolyFuzz(tfidf)

for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model.match(from_chunk, to_list)
    chunk_results = model.get_matches()
    results.append(chunk_results)

final_results = pd.concat(results, ignore_index=True)
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers_tfidf.csv', index=False, sep='|')
model.visualize_precision_recall(save_as="tfidf_precision_recall.html")
print("TF-IDF Matching completed and saved to 'matched_employers_tfidf.csv'.")

# RapidFuzz Matching with Precision-Recall Visualization
results = []
rapidfuzz_matcher = RapidFuzz(n_jobs=1)
model2 = PolyFuzz(rapidfuzz_matcher)

for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model2.match(from_chunk, to_list)
    chunk_results = model2.get_matches()
    results.append(chunk_results)

final_results = pd.concat(results, ignore_index=True)
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers_rapidfuzz.csv', index=False, sep='|')
model2.visualize_precision_recall(save_as="rapidfuzz_precision_recall.html")
print("RapidFuzz Matching completed and saved to 'matched_employers_rapidfuzz.csv'.")

# Jellyfish Matching with Precision-Recall Visualization
results = []
jellyfish_matcher = EditDistance(n_jobs=1, scorer=jaro_winkler_similarity)
model3 = PolyFuzz(jellyfish_matcher)

for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model3.match(from_chunk, to_list)
    chunk_results = model3.get_matches()
    results.append(chunk_results)

final_results = pd.concat(results, ignore_index=True)
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers_jellyfish.csv', index=False, sep='|')
model3.visualize_precision_recall(save_as="jellyfish_precision_recall.html")
print("Jellyfish (Jaro-Winkler) Matching completed and saved to 'matched_employers_jellyfish.csv'.")







from polyfuzz import PolyFuzz
from polyfuzz.models import TFIDF, RapidFuzz, EditDistance
import pandas as pd
import re
from jellyfish import jaro_winkler_similarity
from tqdm import tqdm

# Load data
df = pd.read_csv('file_company.csv')  # Unstructured employer names
df_master = pd.read_csv('file_master.csv')  # Structured employer names

# Drop missing values
df = df.dropna()
df_master = df_master.dropna()

# Convert text to uppercase
df['company_check'] = df['company_check'].apply(lambda x: str(x).upper())
df_master['company_List'] = df_master['company_List'].apply(lambda x: str(x).upper())

# List of suffixes to clean
list_space_cmp = [
    'SOLUTIONS', 'SOLUTION', 'SERVICES', 'SERVICE', 'PVTLTD',
    'PVT LTD', 'PRIVATELIMITED', 'PRIVATE', 'PRIVAT', 'LTD',
    'PVT', 'LLP', 'INTL', 'LIMTED', 'LIMITED', 'LIMITE',
    'CORPORATE', 'INC', 'GROUP', 'ENTERPRISE', 'CORPORATION',
    'CORP', 'COMPANY'
]

# Replace unwanted suffixes
def replace_char(df_column, old_value):
    return df_column.str.replace(old_value, ' ', regex=False)

for cmp in list_space_cmp:
    df['company_check'] = replace_char(df['company_check'], cmp)
    df_master['company_List'] = replace_char(df_master['company_List'], cmp)

# Remove non-alphanumeric characters and normalize spaces
df['company_check_cln'] = df['company_check'].apply(lambda x: re.sub(r"[^a-zA-Z0-9]", " ", x))
df['company_check_cln'] = df['company_check_cln'].apply(lambda x: re.sub(r" +", " ", x).strip())

df_master['company_List_cln'] = df_master['company_List'].apply(lambda x: re.sub(r"[^a-zA-Z0-9]", " ", x))
df_master['company_List_cln'] = df_master['company_List_cln'].apply(lambda x: re.sub(r" +", " ", x).strip())

# Create lists for matching
from_list = df['company_check_cln'].tolist()
to_list = df_master['company_List_cln'].tolist()

# Chunking for large datasets
chunk_size = 5000  # Adjust chunk size based on system memory
results = []

# TF-IDF Matching
tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0)
model = PolyFuzz(tfidf)

for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model.match(from_chunk, to_list)
    chunk_results = model.get_matches()
    results.append(chunk_results)

# Combine and save TF-IDF results
final_results = pd.concat(results, ignore_index=True)
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers_tfidf.csv', index=False, sep='|')
print("TF-IDF Matching completed and saved to 'matched_employers_tfidf.csv'.")

# RapidFuzz Matching
results = []
rapidfuzz_matcher = RapidFuzz(n_jobs=1)
model2 = PolyFuzz(rapidfuzz_matcher)

for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model2.match(from_chunk, to_list)
    chunk_results = model2.get_matches()
    results.append(chunk_results)

final_results = pd.concat(results, ignore_index=True)
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers_rapidfuzz.csv', index=False, sep='|')
print("RapidFuzz Matching completed and saved to 'matched_employers_rapidfuzz.csv'.")

# Jellyfish Matching (Jaro-Winkler)
results = []
jellyfish_matcher = EditDistance(n_jobs=1, scorer=jaro_winkler_similarity)
model3 = PolyFuzz(jellyfish_matcher)

for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model3.match(from_chunk, to_list)
    chunk_results = model3.get_matches()
    results.append(chunk_results)

final_results = pd.concat(results, ignore_index=True)
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers_jellyfish.csv', index=False, sep='|')
print("Jellyfish (Jaro-Winkler) Matching completed and saved to 'matched_employers_jellyfish.csv'.")







from polyfuzz import PolyFuzz
from polyfuzz.models import TFIDF, RapidFuzz, EditDistance
import pandas as pd
import re
from tqdm import tqdm

# Load data
df = pd.read_csv('file_company.csv')  # Unstructured employer names
df_master = pd.read_csv('file_master.csv')  # Structured employer names

# Drop missing values
df = df.dropna()
df_master = df_master.dropna()

# Convert text to uppercase
df['company_check'] = df['company_check'].apply(lambda x: str(x).upper())
df_master['company_List'] = df_master['company_List'].apply(lambda x: str(x).upper())

# List of suffixes to clean
list_space_cmp = [
    'SOLUTIONS', 'SOLUTION', 'SERVICES', 'SERVICE', 'PVTLTD',
    'PVT LTD', 'PRIVATELIMITED', 'PRIVATE', 'PRIVAT', 'LTD',
    'PVT', 'LLP', 'INTL', 'LIMTED', 'LIMITED', 'LIMITE',
    'CORPORATE', 'INC', 'GROUP', 'ENTERPRISE', 'CORPORATION',
    'CORP', 'COMPANY'
]

# Replace unwanted suffixes
def replace_char(df_column, old_value):
    return df_column.str.replace(old_value, ' ', regex=False)

for cmp in list_space_cmp:
    df['company_check'] = replace_char(df['company_check'], cmp)
    df_master['company_List'] = replace_char(df_master['company_List'], cmp)

# Remove non-alphanumeric characters and normalize spaces
df['company_check_cln'] = df['company_check'].apply(lambda x: re.sub(r"[^a-zA-Z0-9]", " ", x))
df['company_check_cln'] = df['company_check_cln'].apply(lambda x: re.sub(r" +", " ", x).strip())

df_master['company_List_cln'] = df_master['company_List'].apply(lambda x: re.sub(r"[^a-zA-Z0-9]", " ", x))
df_master['company_List_cln'] = df_master['company_List_cln'].apply(lambda x: re.sub(r" +", " ", x).strip())

# Create lists for matching
from_list = df['company_check_cln'].tolist()
to_list = df_master['company_List_cln'].tolist()

# Initialize PolyFuzz model with TF-IDF
tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0)
model = PolyFuzz(tfidf)

# Chunking for large datasets
chunk_size = 5000  # Adjust chunk size based on system memory
results = []

for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model.match(from_chunk, to_list)
    chunk_results = model.get_matches()
    results.append(chunk_results)

# Combine all results into a single DataFrame
final_results = pd.concat(results, ignore_index=True)

# Save results to a CSV file
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers_tfidf.csv', index=False, sep='|')

print("TF-IDF Matching completed and saved to 'matched_employers_tfidf.csv'.")

# Perform matching using RapidFuzz
rapidfuzz_matcher = RapidFuzz(n_jobs=1)
model2 = PolyFuzz(rapidfuzz_matcher)

results = []
for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model2.match(from_chunk, to_list)
    chunk_results = model2.get_matches()
    results.append(chunk_results)

final_results = pd.concat(results, ignore_index=True)
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers_rapidfuzz.csv', index=False, sep='|')

print("RapidFuzz Matching completed and saved to 'matched_employers_rapidfuzz.csv'.")










from polyfuzz import PolyFuzz
from polyfuzz.models import TFIDF, RapidFuzz, EditDistance
import pandas as pd
import re
from tqdm import tqdm

# Load data
df = pd.read_csv('file_company.csv')  # Unstructured employer names
df_master = pd.read_csv('file_master.csv')  # Structured employer names

# Drop missing values
df = df.dropna()
df_master = df_master.dropna()

# Convert text to uppercase
df['company_check'] = df['company_check'].apply(lambda x: str(x).upper())
df_master['company_List'] = df_master['company_List'].apply(lambda x: str(x).upper())

# List of suffixes to clean
list_space_cmp = [
    'SOLUTIONS', 'SOLUTION', 'SERVICES', 'SERVICE', 'PVTLTD',
    'PVT LTD', 'PRIVATELIMITED', 'PRIVATE', 'PRIVAT', 'LTD',
    'PVT', 'LLP', 'INTL', 'LIMTED', 'LIMITED', 'LIMITE',
    'CORPORATE', 'INC', 'GROUP', 'ENTERPRISE', 'CORPORATION',
    'CORP', 'COMPANY'
]

# Replace unwanted suffixes
def replace_char(df_column, old_value):
    return df_column.str.replace(old_value, ' ', regex=False)

for cmp in list_space_cmp:
    df['company_check'] = replace_char(df['company_check'], cmp)
    df_master['company_List'] = replace_char(df_master['company_List'], cmp)

# Remove non-alphanumeric characters and normalize spaces
df['company_check_cln'] = df['company_check'].apply(lambda x: re.sub(r"[^a-zA-Z0-9]", " ", x))
df['company_check_cln'] = df['company_check_cln'].apply(lambda x: re.sub(r" +", " ", x).strip())

df_master['company_List_cln'] = df_master['company_List'].apply(lambda x: re.sub(r"[^a-zA-Z0-9]", " ", x))
df_master['company_List_cln'] = df_master['company_List_cln'].apply(lambda x: re.sub(r" +", " ", x).strip())

# Create lists for matching
from_list = df['company_check_cln'].tolist()
to_list = df_master['company_List_cln'].tolist()

# Initialize PolyFuzz model with TF-IDF
tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0)
model = PolyFuzz(tfidf)

# Chunking for large datasets
chunk_size = 5000  # Adjust chunk size based on system memory
results = []

for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model.match(from_chunk, to_list)
    chunk_results = model.get_matches()
    results.append(chunk_results)

# Combine all results into a single DataFrame
final_results = pd.concat(results, ignore_index=True)

# Save results to a CSV file
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers_tfidf.csv', index=False, sep='|')

print("TF-IDF Matching completed and saved to 'matched_employers_tfidf.csv'.")

# Perform matching using RapidFuzz
rapidfuzz_matcher = RapidFuzz(n_jobs=1)
model2 = PolyFuzz(rapidfuzz_matcher)

results = []
for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model2.match(from_chunk, to_list)
    chunk_results = model2.get_matches()
    results.append(chunk_results)

final_results = pd.concat(results, ignore_index=True)
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers_rapidfuzz.csv', index=False, sep='|')

print("RapidFuzz Matching completed and saved to 'matched_employers_rapidfuzz.csv'.")





from polyfuzz import PolyFuzz
from polyfuzz.models import TFIDF
import pandas as pd
import re

# Function to replace synonyms in text
def replace_synonyms(text, synonym_map):
    if not isinstance(text, str):  # Handle non-string values
        return text
    for key, value in synonym_map.items():
        text = text.replace(key, value)
    return text

# Function to preprocess the columns
def preprocess_column(data, column_name, list_space_cmp, synonym_map):
    # Ensure all values are strings and handle NaN values
    data[column_name] = data[column_name].astype(str).fillna("").str.upper()  # Convert to uppercase
    # Replace synonyms
    data[column_name] = data[column_name].apply(lambda x: replace_synonyms(x, synonym_map))
    # Remove special characters
    data[column_name] = data[column_name].apply(lambda x: re.sub(r"[^A-Z0-9]", " ", x))
    # Remove common suffixes
    for cmp in list_space_cmp:
        data[column_name] = data[column_name].str.replace(cmp, " ", regex=False)
    # Normalize spaces
    data[column_name] = data[column_name].str.replace(r" +", " ", regex=True).str.strip()
    return data

# Synonym mapping for replacement
synonym_map = {
    "PVT LTD": "PRIVATE LIMITED",
    "LTD": "LIMITED",
    "CORP": "CORPORATION",
    "INTL": "INTERNATIONAL",
    "INC": "INCORPORATED",
    "SOLUTIONS": "SOLUTION",
    "SERVICES": "SERVICE"
}

# List of suffixes to remove
list_space_cmp = [
    "SOLUTIONS", "SOLUTION", "SERVICES", "SERVICE", "PVTLTD", "PVT LTD",
    "PRIVATELIMITED", "PRIVATE", "PRIVAT", "LTD", "PVT", "LLP", "INTL", 
    "LIMITED", "LIMTED", "LIMITE", "CORPORATE", "INC", "GROUP", "ENTERPRISE",
    "CORPORATION", "CORP", "COMPANY"
]

# Load datasets
df = pd.read_csv('file_company.csv')  # Unstructured employer names
df_master = pd.read_csv('file_master.csv')  # Structured employer names

# Drop missing values
df = df.dropna(subset=['company_check'])  # Ensure no missing values in relevant columns
df_master = df_master.dropna(subset=['company_List'])

# Preprocess columns
df = preprocess_column(df, 'company_check', list_space_cmp, synonym_map)
df_master = preprocess_column(df_master, 'company_List', list_space_cmp, synonym_map)

# Initialize PolyFuzz model with TF-IDF
tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0.7)
model = PolyFuzz(tfidf)

# Perform matching
from_list = df['company_check'].tolist()
to_list = df_master['company_List'].tolist()
model.match(from_list, to_list)

# Get results and save to CSV
result = model.get_matches()
result[['From', 'To', 'Similarity']].to_csv('matched_employers.csv', index=False, sep='|')

print("Matching completed! Results saved to 'matched_employers.csv'.")












from polyfuzz import PolyFuzz
from polyfuzz.models import TFIDF, RapidFuzz, EditDistance
import pandas as pd
import re

# Load the data
df = pd.read_csv('file_company.csv')  # Unstructured employer names
df_master = pd.read_csv('file_master.csv')  # Structured employer names

# Drop missing values
df = df.dropna()
df_master = df_master.dropna()

# Preprocess function to clean text
def preprocess_column(data, column_name, list_space_cmp):
    data[column_name] = data[column_name].str.upper()  # Convert to uppercase
    data[column_name] = data[column_name].apply(lambda x: re.sub(r"[^A-Z0-9]", " ", str(x)))  # Remove special characters
    for cmp in list_space_cmp:  # Replace suffixes
        data[column_name] = data[column_name].str.replace(cmp, " ", regex=False)
    data[column_name] = data[column_name].str.replace(r" +", " ", regex=True).str.strip()  # Remove extra spaces
    return data

# List of suffixes and unnecessary components
list_space_cmp = [
    "SOLUTIONS", "SOLUTION", "SERVICES", "SERVICE", "PVTLTD", "PVT LTD",
    "PRIVATELIMITED", "PRIVATE", "PRIVAT", "LTD", "PVT", "LLP", "INTL", 
    "LIMITED", "LIMTED", "LIMITE", "CORPORATE", "INC", "GROUP", "ENTERPRISE",
    "CORPORATION", "CORP", "COMPANY"
]

# Preprocess the columns
df = preprocess_column(df, 'company_check', list_space_cmp)
df_master = preprocess_column(df_master, 'company_List', list_space_cmp)

# Chunking for large datasets
chunk_size = 5000  # Adjust this based on your system's memory
results = []

# Match data in chunks
from_list = df['company_check'].tolist()
to_list = df_master['company_List'].tolist()

# Initialize the PolyFuzz model with TF-IDF
tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0.7)  # Minimum similarity set to 70%
model = PolyFuzz(tfidf)

# Process data in chunks
for i in range(0, len(from_list), chunk_size):
    from_chunk = from_list[i:i + chunk_size]
    model.match(from_chunk, to_list)
    matches = model.get_matches()
    results.append(matches)

# Combine all results
final_results = pd.concat(results, ignore_index=True)

# Save results to a CSV file
final_results[['From', 'To', 'Similarity']].to_csv('matched_employers.csv', index=False, sep='|')

print("Matching completed! Results saved to 'matched_employers.csv'.")









import pandas as pd
import re
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from rapidfuzz import process, fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
import numpy as np

# Load datasets
df_master = pd.read_csv('master_file.csv')
df = pd.read_csv('unstructured_file.csv')

# Define synonym mapping and suffixes
synonym_map = {
    "pvt ltd": "private limited",
    "ltd": "limited",
    "corp": "corporation",
    "intl": "international",
    "inc": "incorporated",
    "solutions": "solution",
    "services": "service"
}
suffixes = ['limited', 'private', 'corporation', 'group', 'company', 'solution', 'service']

# Replace synonyms in text
def replace_synonyms(text, synonym_map):
    if not isinstance(text, str):  # Handle non-string values
        return text
    for key, value in synonym_map.items():
        text = text.replace(key, value)
    return text

# Remove suffixes from text
def remove_suffixes(text, suffixes):
    if not isinstance(text, str):  # Handle non-string values
        return text
    for suffix in suffixes:
        text = re.sub(rf"\b{suffix}\b", "", text)
    return text.strip()

# Preprocessing function
def preprocess(data, column_name):
    data[column_name] = data[column_name].astype(str).fillna("").str.lower()  # Convert to lowercase
    data[column_name] = data[column_name].apply(lambda x: replace_synonyms(x, synonym_map))  # Replace synonyms
    data[column_name] = data[column_name].apply(lambda x: remove_suffixes(x, suffixes))  # Remove suffixes
    data[column_name] = data[column_name].str.replace(r"[^a-z0-9 ]", " ", regex=True)  # Remove special chars
    data[column_name] = data[column_name].str.replace(r" +", " ", regex=True).str.strip()  # Normalize spaces
    return data

# Preprocess both datasets
df = preprocess(df, 'company_check')
df_master = preprocess(df_master, 'company_List')

# Step 1: Semantic Similarity using Sentence Transformers
model = SentenceTransformer('all-MiniLM-L6-v2')
master_embeddings = model.encode(df_master['company_List'].tolist(), show_progress_bar=True)
unstructured_embeddings = model.encode(df['company_check'].tolist(), show_progress_bar=True)

def get_best_matches(similarity_matrix, unstructured_list, master_list, threshold=0.8):
    matches = []
    for i, row in enumerate(similarity_matrix):
        best_idx = np.argmax(row)
        best_score = row[best_idx]
        matches.append({
            "Unstructured employer": unstructured_list[i],
            "Structured employer": master_list[best_idx] if best_score >= threshold else None,
            "Confidence": best_score
        })
    return matches

similarity_matrix = cosine_similarity(unstructured_embeddings, master_embeddings)
semantic_matches = get_best_matches(similarity_matrix, df['company_check'].tolist(), df_master['company_List'].tolist())

# Step 2: Hybrid Matching with TF-IDF and Semantic Embeddings
def hybrid_match(unstructured_name, master_list, master_embeddings, threshold=0.8):
    tfidf_match, tfidf_score = process.extractOne(unstructured_name, master_list, scorer=fuzz.ratio)
    embedding_score = cosine_similarity(model.encode([unstructured_name]), master_embeddings).max()
    combined_score = 0.6 * tfidf_score + 0.4 * embedding_score
    return (tfidf_match if combined_score >= threshold else None, combined_score)

hybrid_results = []
for name in df['company_check']:
    match, score = hybrid_match(name, df_master['company_List'].tolist(), master_embeddings)
    hybrid_results.append({
        "Unstructured employer": name,
        "Structured employer": match,
        "Confidence": score
    })

# Combine results
semantic_df = pd.DataFrame(semantic_matches)
hybrid_df = pd.DataFrame(hybrid_results)
final_results = pd.concat([semantic_df, hybrid_df], ignore_index=True).drop_duplicates()

# Export results
final_results.to_csv('enhanced_matched_employers.csv', index=False, sep='|')




import pandas as pd
import re
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from rapidfuzz import process, fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
import numpy as np

# Load datasets
df_master = pd.read_csv('master_file.csv')
df = pd.read_csv('unstructured_file.csv')

# Define preprocessing helpers
synonym_map = {
    "pvt ltd": "private limited",
    "ltd": "limited",
    "corp": "corporation",
    "intl": "international",
    "inc": "incorporated",
    "solutions": "solution",
    "services": "service"
}
suffixes = ['limited', 'private', 'corporation', 'group', 'company', 'solution', 'service']

def preprocess(data, column_name):
    data[column_name] = data[column_name].str.lower()  # Convert to lowercase
    data[column_name] = data[column_name].str.replace(r"[^a-z0-9 ]", " ", regex=True)  # Remove special chars
    data[column_name] = data[column_name].apply(lambda x: replace_synonyms(x, synonym_map))  # Replace synonyms
    data[column_name] = data[column_name].apply(lambda x: remove_suffixes(x, suffixes))  # Remove suffixes
    data[column_name] = data[column_name].str.replace(r" +", " ", regex=True).str.strip()  # Normalize spaces
    return data

def replace_synonyms(text, synonym_map):
    for key, value in synonym_map.items():
        text = text.replace(key, value)
    return text

def remove_suffixes(text, suffixes):
    for suffix in suffixes:
        text = re.sub(rf"\b{suffix}\b", "", text)
    return text.strip()

# Preprocess data
df = preprocess(df, 'company_check')
df_master = preprocess(df_master, 'company_List')

# Step 1: Semantic Similarity (Sentence Transformers)
model = SentenceTransformer('all-MiniLM-L6-v2')
master_embeddings = model.encode(df_master['company_List'].tolist(), show_progress_bar=True)
unstructured_embeddings = model.encode(df['company_check'].tolist(), show_progress_bar=True)

def get_best_matches(similarity_matrix, unstructured_list, master_list, threshold=0.8):
    matches = []
    for i, row in enumerate(similarity_matrix):
        best_idx = np.argmax(row)
        best_score = row[best_idx]
        matches.append({
            "Unstructured employer": unstructured_list[i],
            "Structured employer": master_list[best_idx] if best_score >= threshold else None,
            "Confidence": best_score
        })
    return matches

similarity_matrix = cosine_similarity(unstructured_embeddings, master_embeddings)
semantic_matches = get_best_matches(similarity_matrix, df['company_check'].tolist(), df_master['company_List'].tolist())

# Step 2: Clustering (DBSCAN)
all_names = df_master['company_List'].tolist() + df['company_check'].tolist()
vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 3))
X = vectorizer.fit_transform(all_names)
clustering = DBSCAN(eps=0.5, min_samples=2, metric='cosine').fit(X)
clustered_df = pd.DataFrame({'Name': all_names, 'Cluster': clustering.labels_})

# Step 3: Hybrid Matching
def hybrid_match(unstructured_name, master_list, master_embeddings, threshold=0.8):
    tfidf_match, tfidf_score = process.extractOne(unstructured_name, master_list, scorer=fuzz.ratio)
    embedding_score = cosine_similarity(model.encode([unstructured_name]), master_embeddings).max()
    combined_score = 0.6 * tfidf_score + 0.4 * embedding_score
    return (tfidf_match if combined_score >= threshold else None, combined_score)

hybrid_results = []
for name in df['company_check']:
    match, score = hybrid_match(name, df_master['company_List'].tolist(), master_embeddings)
    hybrid_results.append({
        "Unstructured employer": name,
        "Structured employer": match,
        "Confidence": score
    })

# Combine all results
semantic_df = pd.DataFrame(semantic_matches)
hybrid_df = pd.DataFrame(hybrid_results)
final_results = pd.concat([semantic_df, hybrid_df], ignore_index=True).drop_duplicates()

# Export results
final_results.to_csv('enhanced_matched_employers.csv', index=False, sep='|')
