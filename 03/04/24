import pandas as pd
import plotly.graph_objects as go

# Sample data setup
data = {
    'cusid': range(1, 11),
    'nov-bucket': ['101-600', '601-650', '651-700', '701-750', '751-800', '801-850', '>850', '0-100', '751-800', '701-750'],
    'jan-bucket': ['101-600', '601-650', '651-700', '701-750', '751-800', '801-850', '>850', '0-100', '751-800', '701-750'],
    'feb-bucket': ['101-600', '601-650', '651-700', '701-750', '751-800', '801-850', '>850', '0-100', '751-800', '701-750'],
    'apr-bucket': ['101-600', '601-650', '651-700', '701-750', '751-800', '801-850', '>850', '0-100', '751-800', '701-750']
}
df = pd.DataFrame(data)

# Define the buckets in the proper order
bucket_order = ['0-100', '101-600', '601-650', '651-700', '701-750', '751-800', '801-850', '>850']

# Define a color map corresponding to the new bucket ranges
color_map = {
    '0-100': '#FF6692', '101-600': '#19D3F3', '601-650': '#FFA15A',
    '651-700': '#AB63FA', '701-750': '#00CC96', '751-800': '#EF553B',
    '801-850': '#636EFA', '>850': '#FFCE56'
}

# Adjust the bucket labels to include the month for uniqueness and preserve order
months = ['nov', 'jan', 'feb', 'apr']
for month in months:
    df[f'{month}-bucket'] = df[f'{month}-bucket'].apply(lambda x: f"{x} ({month})")

# Generate the source, target, value lists, and colors
sources = []
targets = []
values = []
colors = []

# Create a map from bucket to index based on ordered_unique_buckets
ordered_unique_buckets = [f"{b} ({m})" for b in bucket_order for m in months]
bucket_index = {bucket: i for i, bucket in enumerate(ordered_unique_buckets)}

# Calculate transitions and maintain order
for i in range(len(months) - 1):
    transitions = df.groupby([f'{months[i]}-bucket', f'{months[i+1]}-bucket']).size().reset_index(name='count')
    for _, row in transitions.iterrows():
        source_bucket = row[f'{months[i]}-bucket']
        target_bucket = row[f'{months[i+1]}-bucket']
        if source_bucket in bucket_index and target_bucket in bucket_index:
            sources.append(bucket_index[source_bucket])
            targets.append(bucket_index[target_bucket])
            values.append(row['count'])

            # Extract the base bucket score range to apply the appropriate color
            base_color_key = source_bucket.split()[0] if '-' in source_bucket else source_bucket.split()[0] + '-' + source_bucket.split()[1]
            if base_color_key in color_map:
                colors.append(color_map[base_color_key])
            else:
                print(f"Key Error: {base_color_key} not found in color_map!")

# Create the Sankey diagram
fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=ordered_unique_buckets,
        color="blue"
    ),
    link=dict(
        source=sources,
        target=targets,
        value=values,
        color=colors  # Set link colors
    ))])

fig.update_layout(title_text="Customer Movements Across CIBIL Score Buckets Over Months", font_size=10)
fig.show()










import pandas as pd
import plotly.graph_objects as go

# Sample data setup
data = {
    'cusid': range(1, 11),
    'nov-bucket': ['600-650', '650-700', '700-750', '750-800', '600-650', '700-750', '700-750', '750-800', '800-850', '850-900'],
    'jan-bucket': ['600-650', '650-700', '700-750', '750-800', '650-700', '700-750', '700-750', '750-800', '850-900', '850-900'],
    'feb-bucket': ['600-650', '700-750', '700-750', '800-850', '700-750', '700-750', '700-750', '850-900', '<600', '850-900'],
    'apr-bucket': ['600-650', '800-850', '700-750', '<600', '750-800', '700-750', '700-750', '<600', '600-650', '650-700']
}
df = pd.DataFrame(data)

# Define the buckets in decreasing order of CIBIL score ranges
bucket_order = ['850-900', '800-850', '750-800', '700-750', '650-700', '600-650', '<600']

# Color map
color_map = {
    '850-900': '#636EFA', '800-850': '#EF553B', '750-800': '#00CC96',
    '700-750': '#AB63FA', '650-700': '#FFA15A', '600-650': '#19D3F3',
    '<600': '#FF6692'
}

# Adjust the bucket labels to include the month for uniqueness and preserve order
months = ['nov', 'jan', 'feb', 'apr']
for month in months:
    df[f'{month}-bucket'] = df[f'{month}-bucket'].apply(lambda x: f"{x} ({month})")

# Generate the source, target, value lists, and colors
sources = []
targets = []
values = []
colors = []

# Create a map from bucket to index based on ordered_unique_buckets
ordered_unique_buckets = [f"{b} ({m})" for b in bucket_order for m in months]
bucket_index = {bucket: i for i, bucket in enumerate(ordered_unique_buckets)}

# Calculate transitions and maintain order
for i in range(len(months) - 1):
    transitions = df.groupby([f'{months[i]}-bucket', f'{months[i+1]}-bucket']).size().reset_index(name='count')
    for _, row in transitions.iterrows():
        source_bucket = row[f'{months[i]}-bucket']
        target_bucket = row[f'{months[i+1]}-bucket']
        if source_bucket in bucket_index and target_bucket in bucket_index:  # Ensure both buckets are in the defined order
            sources.append(bucket_index[source_bucket])
            targets.append(bucket_index[target_bucket])
            values.append(row['count'])
            # Assign color based on source bucket score range
            base_color = source_bucket.split()[0].replace('(', '')
            colors.append(color_map[base_color.split('-')[0]])

# Create the Sankey diagram
fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=ordered_unique_buckets,
        color="blue"
    ),
    link=dict(
        source=sources,
        target=targets,
        value=values,
        color=colors  # Set link colors
    ))])

fig.update_layout(title_text="Customer Movements Across CIBIL Score Buckets Over Months", font_size=10)
fig.show()









import pandas as pd
import plotly.graph_objects as go

# Sample data setup
data = {
    'cusid': range(1, 11),
    'nov-bucket': ['600-650', '650-700', '700-750', '750-800', '600-650', '700-750', '700-750', '750-800', '800-850', '850-900'],
    'jan-bucket': ['600-650', '650-700', '700-750', '750-800', '650-700', '700-750', '700-750', '750-800', '850-900', '850-900'],
    'feb-bucket': ['600-650', '700-750', '700-750', '800-850', '700-750', '700-750', '700-750', '850-900', '<600', '850-900'],
    'apr-bucket': ['600-650', '800-850', '700-750', '<600', '750-800', '700-750', '700-750', '<600', '600-650', '650-700']
}

df = pd.DataFrame(data)

# Define the buckets in decreasing order of CIBIL score ranges
bucket_order = ['850-900', '800-850', '750-800', '700-750', '650-700', '600-650', '<600']

# Adjust the bucket labels to include the month for uniqueness and preserve order
months = ['nov', 'jan', 'feb', 'apr']
for month in months:
    df[f'{month}-bucket'] = df[f'{month}-bucket'].apply(lambda x: f"{x} ({month})")

# Create a unique, ordered list of buckets
ordered_unique_buckets = [f"{b} ({m})" for b in bucket_order for m in months]

# Generate the source, target, and value lists
sources = []
targets = []
values = []

# Create a map from bucket to index based on ordered_unique_buckets
bucket_index = {bucket: i for i, bucket in enumerate(ordered_unique_buckets)}

# Calculate transitions and maintain order
for i in range(len(months) - 1):
    transitions = df.groupby([f'{months[i]}-bucket', f'{months[i+1]}-bucket']).size().reset_index(name='count')
    for _, row in transitions.iterrows():
        source_bucket = row[f'{months[i]}-bucket']
        target_bucket = row[f'{months[i+1]}-bucket']
        if source_bucket in bucket_index and target_bucket in bucket_index:  # Ensure both buckets are in the defined order
            sources.append(bucket_index[source_bucket])
            targets.append(bucket_index[target_bucket])
            values.append(row['count'])

# Create the Sankey diagram
fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=ordered_unique_buckets,
        color="blue"
    ),
    link=dict(
        source=sources,
        target=targets,
        value=values
    ))])

fig.update_layout(title_text="Customer Movements Across CIBIL Score Buckets Over Months", font_size=10)
fig.show()






import pandas as pd
import plotly.graph_objects as go

# Sample data setup
data = {
    'cusid': range(1, 11),
    'nov-bucket': ['600-650', '650-700', '700-750', '750-800', '600-650', '700-750', '700-750', '750-800', '800-850', '850-900'],
    'jan-bucket': ['600-650', '650-700', '700-750', '750-800', '650-700', '700-750', '700-750', '750-800', '850-900', '850-900'],
    'feb-bucket': ['600-650', '700-750', '700-750', '800-850', '700-750', '700-750', '700-750', '850-900', '<600', '850-900'],
    'apr-bucket': ['600-650', '800-850', '700-750', '<600', '750-800', '700-750', '700-750', '<600', '600-650', '650-700']
}

df = pd.DataFrame(data)

# Adjust the bucket labels to include the month for uniqueness
months = ['nov', 'jan', 'feb', 'apr']
for month in months:
    df[f'{month}-bucket'] = df[f'{month}-bucket'].apply(lambda x: f"{x} ({month})")

# Prepare lists to hold source, target, and counts
sources = []
targets = []
values = []

# Unique bucket names for mapping
unique_buckets = set()
for col in df.columns[1:]:
    unique_buckets.update(df[col].unique())
unique_buckets = list(unique_buckets)
bucket_index = {bucket: i for i, bucket in enumerate(unique_buckets)}

# Generate the source, target, and value lists
for i in range(len(months) - 1):
    transitions = df.groupby([f'{months[i]}-bucket', f'{months[i+1]}-bucket']).size().reset_index(name='count')
    for _, row in transitions.iterrows():
        sources.append(bucket_index[row[f'{months[i]}-bucket']])
        targets.append(bucket_index[row[f'{months[i+1]}-bucket']])
        values.append(row['count'])

# Create the Sankey diagram
fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=unique_buckets,
        color="blue"
    ),
    link=dict(
        source=sources,  # indices correspond to labels
        target=targets,  # indices correspond to labels
        value=values
    ))])

fig.update_layout(title_text="Customer Movements Across CIBIL Score Buckets Over Months", font_size=10)
fig.show()







import pandas as pd
import plotly.graph_objects as go

# Calculate transitions
def get_transitions(df, start_month, end_month):
    transitions = df.groupby([start_month, end_month]).size().reset_index(name='count')
    return transitions

# List of months and an empty list for storing transitions
months = ['nov-bucket', 'jan-bucket', 'feb-bucket', 'apr-bucket']
transitions_list = []

# Calculate transitions for consecutive months
for i in range(len(months) - 1):
    month_data = get_transitions(df, months[i], months[i + 1])
    transitions_list.append(month_data)

# Prepare the source, target, and values for the Sankey diagram
sources = []
targets = []
values = []
label_list = []

# Function to add indices and get unique labels
def add_indices(data, source_col, target_col, value_col, label_list):
    for index, row in data.iterrows():
        if row[source_col] not in label_list:
            label_list.append(row[source_col])
        if row[target_col] not in label_list:
            label_list.append(row[target_col])
        sources.append(label_list.index(row[source_col]))
        targets.append(label_list.index(row[target_col]))
        values.append(row[value_col])

# Process each transition
for transition in transitions_list:
    add_indices(transition, 0, 1, 'count', label_list)

# Create Sankey diagram
fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color="black", width=0.5),
        label=label_list,
        color="blue"
    ),
    link=dict(
        source=sources,
        target=targets,
        value=values
    ))])

fig.update_layout(title_text="Customer Movements Across CIBIL Score Buckets Over Months", font_size=10)
fig.show()








import pandas as pd
import plotly.graph_objects as go

# Sample data
data = {
    'cusid': [1, 2, 3, 4, 5],
    'nov-bucket': ['600-650', '650-700', '600-650', '700-750', '650-700'],
    'jan-bucket': ['650-700', '700-750', '600-650', '750-800', '700-750'],
    'feb-bucket': ['700-750', '750-800', '650-700', '800-850', '750-800'],
    'apr-bucket': ['750-800', '800-850', '700-750', '850-900', '800-850']
}
df = pd.DataFrame(data)

# Prepare lists to hold source, target, and counts
sources = []
targets = []
values = []

# Unique bucket names for mapping
unique_buckets = set()
for col in df.columns[1:]:
    unique_buckets.update(df[col].unique())
unique_buckets = list(unique_buckets)
bucket_index = {bucket: i for i, bucket in enumerate(unique_buckets)}

# Generate the source, target, and value lists
for i in range(1, len(df.columns) - 1):
    month_data = df.groupby([df.columns[i], df.columns[i+1]]).size().reset_index(name='count')
    for _, row in month_data.iterrows():
        sources.append(bucket_index[row[df.columns[i]]])
        targets.append(bucket_index[row[df.columns[i+1]]])
        values.append(row['count'])

# Create the Sankey diagram
fig = go.Figure(data=[go.Sankey(
    node = dict(
      pad = 15,
      thickness = 20,
      line = dict(color = "black", width = 0.5),
      label = unique_buckets,
      color = "blue"
    ),
    link = dict(
      source = sources,  # indices correspond to labels
      target = targets,  # indices correspond to labels
      value = values
  ))])

fig.update_layout(title_text="Customer Movements Across CIBIL Score Buckets Over Months", font_size=10)
fig.show()






import pandas as pd
import plotly.graph_objects as go

# Sample DataFrame setup
# You should replace this with your actual data loading method, e.g., pd.read_csv()
data = {
    'cusid': [1, 2, 3, 4, 5],
    'nov-bucket': ['600-650', '650-700', '600-650', '700-750', '650-700'],
    'jan-bucket': ['650-700', '700-750', '600-650', '750-800', '700-750'],
    'feb-bucket': ['700-750', '750-800', '650-700', '800-850', '750-800'],
    'apr-bucket': ['750-800', '800-850', '700-750', '850-900', '800-850']
}

df = pd.DataFrame(data)

# Prepare data for the Sankey diagram
# This includes creating a list of source-target pairs with counts

# Define buckets and months
buckets = df['nov-bucket'].unique().tolist() + df['jan-bucket'].unique().tolist() + df['feb-bucket'].unique().tolist() + df['apr-bucket'].unique().tolist()
buckets = list(set(buckets))  # Unique list of all buckets
months = ['nov-bucket', 'jan-bucket', 'feb-bucket', 'apr-bucket']

# Creating a mapping of bucket to index for labels
bucket_to_index = {bucket: idx for idx, bucket in enumerate(buckets)}

# Initialize source, target, and values for the Sankey diagram
sources = []
targets = []
values = []

# Process transitions
for i in range(len(months) - 1):
    current_month = df[months[i]]
    next_month = df[months[i + 1]]

    # Count transitions from current month to next month
    transitions = df.groupby([months[i], months[i + 1]]).size().reset_index(name='count')
    for _, row in transitions.iterrows():
        sources.append(bucket_to_index[row[months[i]]])
        targets.append(bucket_to_index[row[months[i + 1]]])
        values.append(row['count'])

# Create the Sankey diagram
fig = go.Figure(data=[go.Sankey(
    node=dict(
        pad=15,
        thickness=20,
        line=dict(color='black', width=0.5),
        label=buckets,
        color='blue'
    ),
    link=dict(
        source=sources,  # indices correspond to labels, eg A1, A2, A1, B1, ...
        target=targets,
        value=values
    ))])

fig.update_layout(title_text='Customer Movements Across CIBIL Score Buckets', font_size=10)
fig.show()



SELECT DISTINCT
    CAST(a.pxdecisiontime AS DATE) AS decision_date,
    d.cust_id_num,
    a.pysubjectid,
    c.pyoutcome,
    a.contentid,
    b.PRPS_CDE,
    CASE
        WHEN b.OVS_PRSN_IND = 'N' THEN 'DOM'
        WHEN b.OVS_PRSN_IND = 'Y' THEN 'NR'
        ELSE ''
    END AS DOM_NR,
    b.Indv_Pho_Num_Mobl_Pagr_3_Txt
FROM
    PMDINPA1.PR_DATA_IH_FACT_reporting a
LEFT JOIN
    PMDINPA1.PR_DATA_IH_DIM_OUTCOME c ON a.pzoutcomeid = c.pzid
LEFT JOIN
    ISSINPA1.PEGA_CAR b ON a.pysubjectid = b.customer_id
LEFT JOIN
    ISSINPA1.PEGA_XAR d ON a.pysubjectid = b.customer_id
WHERE
    a.contentid = 'MECLO1_MEC004_PGCCD1_442_IA_1'
    AND a.pxdecisiontime >= '07-NOV-2023'
    AND a.pxdecisiontime <= CURRENT_DATE
    AND c.pyoutcome = 'Accepted'
GROUP BY
    a.pysubjectid,
    a.contentid,
    c.pyoutcome,
    CAST(a.pxdecisiontime AS DATE),
    b.PRPS_CDE,
    b.OVS_PRSN_IND,
    b.Indv_Pho_Num_Mobl_Pagr_3_Txt,
    d.cust_id_num;








Delivered Bucket = 
VAR DeliveredCount = CALCULATE(
    COUNTROWS('YourTable'),
    'YourTable'[delivered] = 1,
    ALLEXCEPT('YourTable', 'YourTable'[customer_id])
)
RETURN
    SWITCH(
        TRUE(),
        DeliveredCount = 1, "1 delivery",
        DeliveredCount = 2, "2 deliveries",
        DeliveredCount >= 3 && DeliveredCount <= 5, "3-5 deliveries",
        DeliveredCount > 5 && DeliveredCount <= 20, "5-20 deliveries",
        DeliveredCount > 20 && DeliveredCount <= 40, "20-40 deliveries",
        DeliveredCount > 40, "More than 40 deliveries",
        "Undelivered" // You can customize this to manage edge cases where delivered count may be zero
    )





Customer Count by Bucket = 
VAR Customers1_2 = CALCULATE(COUNTROWS('Table'), FILTER('Table', 'Table'[Message Delivered] > 0 && 'Table'[Message Delivered] <= 2))
VAR Customers3_5 = CALCULATE(COUNTROWS('Table'), FILTER('Table', 'Table'[Message Delivered] > 2 && 'Table'[Message Delivered] <= 5))
VAR Customers6_10 = CALCULATE(COUNTROWS('Table'), FILTER('Table', 'Table'[Message Delivered] > 5 && 'Table'[Message Delivered] <= 10))
VAR Customers11_15 = CALCULATE(COUNTROWS('Table'), FILTER('Table', 'Table'[Message Delivered] > 10 && 'Table'[Message Delivered] <= 15))
VAR Customers16_20 = CALCULATE(COUNTROWS('Table'), FILTER('Table', 'Table'[Message Delivered] > 15 && 'Table'[Message Delivered] <= 20))
VAR CustomersMoreThan20 = CALCULATE(COUNTROWS('Table'), FILTER('Table', 'Table'[Message Delivered] > 20))

RETURN
"1-2: " & Customers1_2 & "; " &
"3-5: " & Customers3_5 & "; " &
"6-10: " & Customers6_10 & "; " &
"11-15: " & Customers11_15 & "; " &
"16-20: " & Customers16_20 & "; " &
"More than 20: " & CustomersMoreThan20






Customer Count by Bucket = 
VAR TotalMessages = [Total Messages]  // Reference your Total Messages measure
VAR CustomerID = VALUES('Table'[Customer ID])  // Unique customer IDs

RETURN
CONCATENATE(
    "1-2: " & CALCULATE(COUNTROWS(CustomerID), TotalMessages > 0, TotalMessages <= 2) & "; ",
    CONCATENATE(
        "3-5: " & CALCULATE(COUNTROWS(CustomerID), TotalMessages > 2, TotalMessages <= 5) & "; ",
        CONCATENATE(
            "6-10: " & CALCULATE(COUNTROWS(CustomerID), TotalMessages > 5, TotalMessages <= 10) & "; ",
            CONCATENATE(
                "11-15: " & CALCULATE(COUNTROWS(CustomerID), TotalMessages > 10, TotalMessages <= 15) & "; ",
                CONCATENATE(
                    "16-20: " & CALCULATE(COUNTROWS(CustomerID), TotalMessages > 15, TotalMessages <= 20) & "; ",
                    "More than 20: " & CALCULATE(COUNTROWS(CustomerID), TotalMessages > 20)
                )
            )
        )
    )
)






Customer Count by Bucket = 
VAR Customers1_2 = CALCULATE(COUNTROWS(VALUES('Table'[Customer ID])), [Total Messages] > 0, [Total Messages] <= 2)
VAR Customers3_5 = CALCULATE(COUNTROWS(VALUES('Table'[Customer ID])), [Total Messages] > 2, [Total Messages] <= 5)
VAR Customers6_10 = CALCULATE(COUNTROWS(VALUES('Table'[Customer ID])), [Total Messages] > 5, [Total Messages] <= 10)
VAR Customers11_15 = CALCULATE(COUNTROWS(VALUES('Table'[Customer ID])), [Total Messages] > 10, [Total Messages] <= 15)
VAR Customers16_20 = CALCULATE(COUNTROWS(VALUES('Table'[Customer ID])), [Total Messages] > 15, [Total Messages] <= 20)
VAR CustomersMoreThan20 = CALCULATE(COUNTROWS(VALUES('Table'[Customer ID])), [Total Messages] > 20)

RETURN
"1-2: " & Customers1_2 & "; " &
"3-5: " & Customers3_5 & "; " &
"6-10: " & Customers6_10 & "; " &
"11-15: " & Customers11_15 & "; " &
"16-20: " & Customers16_20 & "; " &
"More than 20: " & CustomersMoreThan20







Customers Per Bucket = COUNTROWS('Customer Buckets')



Customer Buckets = 
ADDCOLUMNS(
    SUMMARIZE(
        'Table', 
        'Table'[Customer ID], 
        "TotalMessages", [Total Messages]
    ),
    "Message Bucket", 
    SWITCH(
        TRUE(),
        'TotalMessages' <= 2, "1-2",
        'TotalMessages' <= 5, "3-5",
        'TotalMessages' <= 10, "6-10",
        'TotalMessages' <= 15, "11-15",
        'TotalMessages' <= 20, "16-20",
        "More than 20"
    )
)







Customer Count by Bucket = 
VAR Bucket1 = CALCULATE(DISTINCTCOUNT('YourTable'[Customer ID]), FILTER(ALL('YourTable'), [Total Messages Per Customer] >= 1 && [Total Messages Per Customer] <= 2))
VAR Bucket2 = CALCULATE(DISTINCTCOUNT('YourTable'[Customer ID]), FILTER(ALL('YourTable'), [Total Messages Per Customer] >= 3 && [Total Messages Per Customer] <= 5))
VAR Bucket3 = CALCULATE(DISTINCTCOUNT('YourTable'[Customer ID]), FILTER(ALL('YourTable'), [Total Messages Per Customer] >= 6 && [Total Messages Per Customer] <= 10))
VAR Bucket4 = CALCULATE(DISTINCTCOUNT('YourTable'[Customer ID]), FILTER(ALL('YourTable'), [Total Messages Per Customer] >= 11 && [Total Messages Per Customer] <= 15))
VAR Bucket5 = CALCULATE(DISTINCTCOUNT('YourTable'[Customer ID]), FILTER(ALL('YourTable'), [Total Messages Per Customer] >= 16 && [Total Messages Per Customer] <= 20))
VAR Bucket6 = CALCULATE(DISTINCTCOUNT('YourTable'[Customer ID]), FILTER(ALL('YourTable'), [Total Messages Per Customer] > 20))
RETURN
CONCATENATEX(
    VALUES('YourTable'[Message Bucket]),
    SWITCH(
        TRUE(),
        'YourTable'[Message Bucket] = "1-2", Bucket1,
        'YourTable'[Message Bucket] = "3-5", Bucket2,
        'YourTable'[Message Bucket] = "6-10", Bucket3,
        'YourTable'[Message Bucket] = "11-15", Bucket4,
        'YourTable'[Message Bucket] = "16-20", Bucket5,
        'YourTable'[Message Bucket] = "More than 20", Bucket6
    ), ", "
)



Customer Count by Bucket = CALCULATE(
    DISTINCTCOUNT('YourTable'[Customer ID]),
    ALLEXCEPT('YourTable', 'YourTable'[Message Bucket])
)



Total Messages = SUMX(
  DISTINCT('YourTable'[Customer ID]),
  CALCULATE(SUM('YourTable'[Messages]))
)

Message Bucket = SWITCH(
    TRUE(),
    [Total Messages] >= 1 && [Total Messages] <= 2, "1-2",
    [Total Messages] >= 3 && [Total Messages] <= 5, "3-5",
    [Total Messages] >= 6 && [Total Messages] <= 10, "6-10",
    [Total Messages] >= 11 && [Total Messages] <= 15, "11-15",
    [Total Messages] >= 16 && [Total Messages] <= 20, "16-20",
    "More than 20"
)






proc sql;
   create table customer_presence as
   select 
       coalesce(a.custid, b.custid, c.custid, d.custid) as custid,
       a.cib_score as cib_score1,
       b.cib_score as cib_score2,
       c.cib_score as cib_score3,
       d.cib_score as cib_score4,
       /* Create flags for presence in each table */
       (case when a.custid is not null then 'Y' else 'N' end) as in_table1,
       (case when b.custid is not null then 'Y' else 'N' end) as in_table2,
       (case when c.custid is not null then 'Y' else 'N' end) as in_table3,
       (case when d.custid is not null then 'Y' else 'N' end) as in_table4,
       /* Create a flag for presence in all tables */
       (case when a.custid is not null and b.custid is not null and c.custid is not null and d.custid is not null then 'Y' else 'N' end) as in_all_tables
   from dataset1 a
   full outer join dataset2 b on a.custid = b.custid
   full outer join dataset3 c on coalesce(a.custid, b.custid) = c.custid
   full outer join dataset4 d on coalesce(a.custid, b.custid, c.custid) = d.custid;
quit;









import pandas as pd
from sqlalchemy import create_engine

# Database connection string
credentials = 'postgresql://username:password@hostname:port/database'
engine = create_engine(credentials)

# SQL query to fetch data
query = """
SELECT * FROM your_table;  -- replace 'your_table' with your actual table name
"""

# Fetch data from PostgreSQL and load into DataFrame
with engine.connect() as connection:
    df_sql = pd.read_sql_query(query, connection)

# Load data from Excel file
df_excel = pd.read_excel('path_to_your_excel_file.xlsx')

# Ensure that 'contentid' is the common column in both DataFrames and set how the join should be handled (e.g., 'inner', 'outer', 'left', 'right')
df_joined = pd.merge(df_sql, df_excel, on='contentid', how='inner')

# Save the joined DataFrame to an Excel file in the desired location
output_path = 'desired_location/final_output.xlsx'
df_joined.to_excel(output_path, index=False)

# Optionally, print a preview of the final DataFrame
print(df_joined.head())








ff
%macro import_and_combine_csvs;
    %local dir_list num_dirs i current_dir filepath;

    * Example of manually setting directory paths - replace with dynamic fetching method if available;
    data directories;
        input directory :$100.;
        datalines;
        /path/to/sftp/email_nonpega/Email_01_04_24
        /path/to/sftp/email_nonpega/Email_02_04_24
        /path/to/sftp/email_nonpega/Email_03_04_24
    ;
    run;

    * Fetch the list of directories;
    proc sql noprint;
        select directory into :dir_list separated by ' ' from directories;
    quit;

    %let num_dirs = &sqlobs;

    * Loop through each directory and import all CSV files;
    %do i = 1 %to &num_dirs;
        %let current_dir = %scan(&dir_list, &i);

        * Set up a temporary filename reference for listing CSV files;
        filename filelist pipe "ls &current_dir/*.csv";
        
        * Use a separate data step to process the file list;
        data _null_;
            infile filelist truncover;
            input filepath $200.;
            call symputx('csvfile', cats("&current_dir/", filepath));
            * Ensure only data manipulation commands are called;
            call execute(cats('proc import datafile="', symget('csvfile'), '" out=work.temp dbms=csv replace; getnames=yes; run;'));
            call execute('proc append base=work.master data=work.temp; run;');
        run;

        * Clear the filename reference;
        filename filelist clear;
    %end;

%mend import_and_combine_csvs;

* Initialize the master dataset if necessary;
data work.master;
    length var1 $100 var2 $100;  * Define columns as per your CSV file structure;
    stop;
run;

%import_and_combine_csvs;

* Optionally export the master dataset;
proc export data=work.master
    outfile="/path/to/output/master_combined.csv"
    dbms=csv
    replace;
run;








nd


%macro import_and_combine_csvs;
    %local dir_list num_dirs i current_dir filepath filename;

    * Example of manually setting directory paths - replace with dynamic fetching method if available;
    data directories;
        input directory :$100.;
        datalines;
        /path/to/sftp/email_nonpega/Email_01_04_24
        /path/to/sftp/email_nonpega/Email_02_04_24
        /path/to/sftp/email_nonpega/Email_03_04_24
    ;
    run;

    * Fetch the list of directories;
    proc sql noprint;
        select directory into :dir_list separated by ' ' from directories;
    quit;

    %let num_dirs = &sqlobs;

    * Loop through each directory and import all CSV files;
    %do i = 1 %to &num_dirs;
        %let current_dir = %scan(&dir_list, &i);

        * Assume CSV files are named in a predictable manner or just import all CSVs;
        filename csvfile pipe "ls &current_dir/*.csv";
        
        data _null_;
            infile csvfile truncover;
            input filepath $200.;
            call execute(cats('proc import datafile="', "&current_dir/", filepath, '" out=work.temp dbms=csv replace; getnames=yes; run;'));
            call execute('proc append base=work.master data=work.temp; run;');
        run;
        
        filename csvfile clear;
    %end;

%mend import_and_combine_csvs;

* Initialize the master dataset if necessary;
data work.master;
    length var1 $100 var2 $100;  * Define columns as per your CSV file structure;
    stop;
run;

%import_and_combine_csvs;

* Optionally export the master dataset;
proc export data=work.master
    outfile="/path/to/output/master_combined.csv"
    dbms=csv
    replace;
run;










* Define a macro to import and append CSV files from a given directory;
%macro import_and_append_files(directory);
    * List all CSV files in the specified directory;
    filename csvfiles pipe "dir ""&directory\*.csv"" /b";
    data _null_;
        infile csvfiles truncover;
        input filepath $100.;
        call symputx('fullpath', cats("&directory\", filepath));
        call execute(cats('%import_csv(', "&fullpath", ')'));
    run;
    filename csvfiles clear;
%mend;

* Define a macro to import a single CSV file using PROC IMPORT;
%macro import_csv(filepath);
    proc import datafile="&filepath" out=temp dbms=csv replace;
        getnames=yes;
    run;
    proc append base=work.master data=temp force;
    run;
%mend;

* Example dataset containing the folder paths (replace with dynamic input if available);
data folders;
    input folderpath $100.;
    datalines;
    C:\path\to\sftp\email_nonpega\Email_01_04_24
    C:\path\to\sftp\email_nonpega\Email_02_04_24
    C:\path\to\sftp\email_nonpega\Email_03_04_24
;
run;

* Initialize or clear the master dataset to ensure it starts empty;
proc datasets lib=work nolist;
    delete master;
run;

* Define the master dataset to specify the variables structure (adjust according to your data);
data work.master;
    length var1 $100 var2 $100;  * Adapt these variables as per your actual CSV structure;
    stop;
run;

* Process all CSV files in each directory;
data _null_;
    set folders;
    call execute(cats('%import_and_append_files(', folderpath, ')'));
run;

* Optionally export the master dataset to a CSV file for further use;
proc export data=work.master outfile="C:\path\to\export\master_combined.csv" dbms=csv replace;
run;










* Set up library references and path variables;
%let basepath = /path/to/sftp/email_nonpega;
%let logpath = /path/to/log/logfile.txt;

* Define a macro to handle import and append operations;
%macro import_and_append(filepath);
    %local dsid rc;
    %let dsid = %sysfunc(fileexist(&filepath));
    %if &dsid %then %do;
        proc import datafile="&filepath"
            out=temp
            dbms=csv
            replace;
            getnames=yes;
            guessingrows=max;
        run;

        * Append the data to the master dataset;
        proc append base=work.master data=temp force;
        run;
    %end;
    %else %do;
        %put WARNING: The file &filepath does not exist.;
    %end;

    * Log the processed file;
    data _null_;
        file "&logpath" mod;
        put "&filepath";
    run;
%mend import_and_append;

* Initialize or clear the master dataset if necessary;
proc datasets lib=work nolist;
    delete master;
run;

data work.master;
    length var1 $100 var2 $100;  * Specify the structure according to the actual data columns in CSV files;
    stop;
run;

* Assuming 'file_paths' dataset has paths of CSV files to process;
data _null_;
    set file_paths;
    call execute(cats('%import_and_append(', path, ')'));
run;

* Export the master dataset if needed to an external location or further analysis;
proc export data=work.master
    outfile="/path/to/export/master_combined.csv"
    dbms=csv
    replace;
run;











%let basepath = /path/to/sftp/email_nonpega;

/* Log file setup */
%let logpath = /path/to/sftp/process_log.txt;

/* Ensure the log file exists */
data _null_;
    file "&logpath" mod;
run;

/* Read the log file to avoid re-processing */
data processed;
    infile "&logpath" truncover;
    input foldername $100.;
run;

/* Get a list of folders in the base directory */
filename dirlist pipe "ls &basepath";
data folders;
    length folder $100;
    infile dirlist truncover;
    input folder $100.;
    if index(folder, 'Email_') then output; /* Filter based on your naming convention */
run;

/* Exclude already processed folders */
proc sql;
    create table folders_to_process as
    select a.folder from folders a
    where not exists (select 1 from processed b where a.folder = b.foldername);
quit;

/* Append all CSV files from each unprocessed folder */
data master;
    length sourcefile $200;
    retain sourcefile;
    set folders_to_process;
    folderpath = catx('/', "&basepath", folder);
    d = dopen(folderpath);
    if d > 0 then do;
        count = dnum(d);
        do i = 1 to count;
            sourcefile = dread(d, i);
            if upcase(scan(sourcefile, -1, '.')) = 'CSV' then do;
                infile = catx('/', folderpath, sourcefile);
                infile dummy filevar=infile dlm=',' firstobs=2 dsd truncover end=eof;
                do while(not eof);
                    input var1 var2 var3; /* Adjust according to your actual data structure */
                    output master;
                end;
            end;
        end;
        rc = dclose(d);
    end;
run;

/* Log processed folders */
data _null_;
    set folders_to_process;
    file "&logpath" mod;
    put folder;
run;

/* Export the master dataset */
proc export data=master outfile="&basepath/master_data.csv" dbms=csv replace;
run;







import paramiko
import os
from datetime import datetime
import pandas as pd
import re

# Server connection details
host = 'inmsas.in.bsbo'
port = 22
username = 'd5456'
password_filepath = r'C:\Users\d5456\credentials.txt'

# Paths on the SFTP server and local system
base_directory = '/appvol/mix_nas/inm/INMDNA/IMCE/sandbox/Bureau_strategy/Dev/Output/email_nonpega'
local_temp_directory = 'C:/local/temp/'
local_master_csv = 'C:/local/path/to/master.csv'
remote_master_directory = '/path/to/sftp/master/location/'
log_file_path = 'processed_folders.log'

# Read password from file
with open(password_filepath, 'r') as file:
    mypwd = file.read().strip()

# Create SSH and SFTP clients
ssh_client = paramiko.SSHClient()
ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
ssh_client.connect(host, port, username, mypwd)

sftp = ssh_client.open_sftp()

# Read processed folders from the log
def read_processed_folders(log_path):
    try:
        with open(log_path, 'r') as file:
            return set(file.read().splitlines())
    except FileNotFoundError:
        return set()

# Log a processed folder
def log_processed_folder(log_path, folder_name):
    with open(log_path, 'a') as file:
        file.write(folder_name + '\n')

processed_folders = read_processed_folders(log_file_path)

# Function to get and sort folders by date
def get_sorted_folders(sftp, directory):
    folder_regex = r'^Email_(\d{2})_(\d{2})_(\d{2})$'
    folders = [f for f in sftp.listdir(directory) if re.match(folder_regex, f)]
    folders.sort(key=lambda x: datetime.strptime(x, 'Email_%d_%m_%y'))
    return folders

sorted_folders = get_sorted_folders(sftp, base_directory)

# Download, combine, and process files from each folder
combined_df = pd.DataFrame()
for folder_name in sorted_folders:
    if folder_name not in processed_folders:
        folder_path = os.path.join(base_directory, folder_name)
        files = sftp.listdir(folder_path)
        for file in files:
            if file.endswith('.csv'):
                local_path = os.path.join(local_temp_directory, file)
                sftp.get(os.path.join(folder_path, file), local_path)
                df = pd.read_csv(local_path)
                combined_df = pd.concat([combined_df, df], ignore_index=True)
        log_processed_folder(log_file_path, folder_name)

# Save the combined data to a master CSV file and upload to SFTP
combined_df.to_csv(local_master_csv, index=False)
remote_master_file_path = os.path.join(remote_master_directory, 'final_master.csv')
sftp.put(local_master_csv, remote_master_file_path)

# Clean up and close connections
sftp.close()
ssh_client.close()

print("Processing complete. Master file updated and uploaded.")








import paramiko
import os
from datetime import datetime
import pandas as pd

# Server connection details
host = "inmsas.in.bsbo"  # SFTP and SSH server address
port = 22  # Common port for SFTP and SSH
username = "d5456"  # SFTP and SSH username
password_filepath = r"C:\Users\d5456\credentials.txt"  # Path to the password file

# Paths on the SFTP server and local system
base_directory = "/appvol/mix_nas/inm/INMDNA/IMCE/sandbox/Bureau_strategy/Dev/Output/"
local_temp_directory = "C:/local/temp/"
local_master_csv = "C:/local/path/to/master.csv"
remote_master_directory = "/path/to/sftp/master/location/"  # Remote directory for the master file
log_file_path = "processed_folders.log"

# Read password from file
with open(password_filepath, 'r') as file:
    mypwd = file.read().strip()

# Create SSH and SFTP clients
ssh_client = paramiko.SSHClient()
ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
ssh_client.connect(host, port, username, mypwd)

sftp = ssh_client.open_sftp()

# Function to get the newest folder within a specific directory
def get_newest_folder(sftp, directory):
    folders = [(f.filename, f.st_mtime) for f in sftp.listdir_attr(directory) if f.longname.startswith('d')]
    return max(folders, key=lambda x: x[1])[0] if folders else None

# Read processed folders from the log
def read_processed_folders(log_path):
    try:
        with open(log_path, 'r') as file:
            return file.read().splitlines()
    except FileNotFoundError:
        return []

# Log a processed folder
def log_processed_folder(log_path, folder_name):
    with open(log_path, 'a') as file:
        file.write(folder_name + '\n')

processed_folders = read_processed_folders(log_file_path)
newest_folder_name, _ = get_newest_folder(sftp, base_directory)

# Check if folder is already processed
if newest_folder_name not in processed_folders:
    # Download files from the newest folder
    newest_folder_path = os.path.join(base_directory, newest_folder_name)
    files = sftp.listdir(newest_folder_path)
    os.makedirs(local_temp_directory, exist_ok=True)
    for file in files:
        if file.endswith('.csv'):
            sftp.get(os.path.join(newest_folder_path, file), os.path.join(local_temp_directory, file))

    # Combine CSV files into a master file
    csv_files = [pd.read_csv(os.path.join(local_temp_directory, f)) for f in os.listdir(local_temp_directory) if f.endswith('.csv')]
    combined_df = pd.concat(csv_files, ignore_index=True)
    combined_df.to_csv(local_master_csv, index=False)

    # Upload the master CSV file back to a specific location on the SFTP server
    remote_master_file_path = os.path.join(remote_master_directory, "final_master.csv")
    sftp.put(local_master_csv, remote_master_file_path)

    # Log the processed folder
    log_processed_folder(log_file_path, newest_folder_name)

    print(f"Folder {newest_folder_name} processed and master file uploaded.")
else:
    print(f"Folder {newest_folder_name} has already been processed.")

# Clean up and close connections
sftp.close()
ssh_client.close()









import subprocess
import re
import pandas as pd
from datetime import datetime
import os

# SFTP Details and Credentials
sftp_username = "your_sftp_username"
sftp_password = "your_sftp_password"
sftp_host = "sftp.example.com"
sftp_hostkey = "ssh-rsa 2048 xxxxxxxxxxx...="
sftp_target_directory = "/path/to/your/directories/"
local_temp_directory = "C:/local/temp/"
local_master_csv = "C:/local/path/to/master.csv"
log_file_path = "processed_folders.log"
winscp_path = "C:/Program Files (x86)/WinSCP/WinSCP.com"

# Functions for log handling
def read_processed_folders(log_path):
    try:
        with open(log_path, 'r') as file:
            return file.read().splitlines()
    except FileNotFoundError:
        return []

def log_processed_folder(log_path, folder_name):
    with open(log_path, 'a') as file:
        file.write(folder_name + '\n')

processed_folders = read_processed_folders(log_file_path)

# Generate and execute WinSCP command to list directories and parse output
winscp_commands = f'''
open sftp://{sftp_username}:{sftp_password}@{sftp_host}/ -hostkey="{sftp_hostkey}"
ls {sftp_target_directory}
exit
'''
process = subprocess.Popen([winscp_path, '/command', *winscp_commands.split('\n')],
                           stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
stdout, stderr = process.communicate()

# Assuming stdout lists directories in "YYYY-MM-DD HH:MM <DIR> directory_name" format
regex = r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2})\s+<DIR>\s+(\S+)"
folders = [(match.groups()[1], datetime.strptime(match.groups()[0], "%Y-%m-%d %H:%M"))
           for match in re.finditer(regex, stdout)]
folders.sort(key=lambda x: x[1], reverse=True)

# Process folders
for folder_name, _ in folders:
    if folder_name not in processed_folders:
        print(f"Processing new folder: {folder_name}")

        # Download CSV files from the folder
        download_commands = f'''
        open sftp://{sftp_username}:{sftp_password}@{sftp_host}/ -hostkey="{sftp_hostkey}"
        lcd {local_temp_directory}
        cd {sftp_target_directory}/{folder_name}
        mget *.csv
        exit
        '''
        subprocess.run([winscp_path, '/command', *download_commands.split('\n')])

        # Combine CSV files if there are multiple, else use the single CSV file directly
        combined_df = pd.concat([pd.read_csv(f) for f in os.listdir(local_temp_directory) if f.endswith('.csv')])

        # Update master CSV file
        if os.path.exists(local_master_csv):
            master_df = pd.read_csv(local_master_csv)
            updated_master_df = pd.concat([master_df, combined_df])
        else:
            updated_master_df = combined_df

        updated_master_df.to_csv(local_master_csv, index=False)

        # Log the processed folder
        log_processed_folder(log_file_path, folder_name)
        print(f"Folder {folder_name} processed and logged.")
        break
    else:
        print(f"Skipping already processed folder: {folder_name}")










import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime
import numpy as np

# Placeholder DataFrame setup
df = pd.DataFrame({
    'Transaction Date': pd.date_range(start='2023-01-01', periods=365, freq='D'),
    'Transaction Amount': np.random.randint(100, 500, size=365),
    'POS': np.random.choice([0, 1], size=365),
    'Category': np.random.choice(['Dining', 'Grocery', 'Entertainment'], size=365),
    'Merchant Name': np.random.choice(['Merchant A', 'Merchant B', 'Merchant C'], size=365),
    'City': np.random.choice(['City X', 'City Y', 'City Z'], size=365)
})
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div([
    html.H1('Credit Card Spend Analysis Dashboard', style={'color': 'white', 'backgroundColor': 'black'}),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=df['Transaction Date'].min(),
        end_date=datetime.date.today(),
        display_format='YYYY-MM-DD',
        style={'color': 'black'}
    ),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1W - Weekly', 'value': 'W'},
            {'label': '1M - Monthly', 'value': 'M'},
            {'label': '1Y - Yearly', 'value': 'Y'},
        ],
        value='M',  # Default selection
        labelStyle={'display': 'inline-block', 'margin': '10px', 'color': 'white'}
    ),
    dcc.Graph(id='transaction-time-series'),
    dcc.Graph(id='category-bar-chart'),
    dcc.Graph(id='merchant-bar-chart'),
    dcc.Graph(id='city-bar-chart'),
], style={'backgroundColor': 'rgb(40, 40, 40)', 'color': 'white', 'padding': '10px'})

@app.callback(
    [Output('transaction-time-series', 'figure'),
     Output('category-bar-chart', 'figure'),
     Output('merchant-bar-chart', 'figure'),
     Output('city-bar-chart', 'figure')],
    [Input('period-selector', 'value'),
     Input('date-picker-range', 'start_date'),
     Input('date-picker-range', 'end_date'),
     Input('transaction-time-series', 'hoverData')]
)
def update_charts(period, start_date, end_date, hoverData):
    filtered_df = df[(df['Transaction Date'] >= pd.to_datetime(start_date)) & 
                     (df['Transaction Date'] <= pd.to_datetime(end_date))]

    # Time Series Chart logic remains the same
    fig_time_series = go.Figure()
    fig_time_series.add_trace(go.Scatter(
        x=filtered_df['Transaction Date'], 
        y=filtered_df['Transaction Amount'], 
        mode='lines',
        name='Overall',
        line=dict(color='red', width=3)
    ))
    fig_time_series.update_layout(
        title='Transaction Amount Over Time',
        plot_bgcolor='black', 
        paper_bgcolor='black', 
        font=dict(color='white')
    )
    
    # Check if hoverData is None, return the existing figures to prevent errors
    if not hoverData:
        return fig_time_series, go.Figure(), go.Figure(), go.Figure()
    
    # Assuming hoverData is available, parse it
    hover_date_str = hoverData['points'][0]['x']
    hover_date = pd.to_datetime(hover_date_str)

    # Filter data for the hovered date
    hover_filtered_df = filtered_df[filtered_df['Transaction Date'].dt.date == hover_date.date()]

    # Aggregate and create figures for category, merchant, and city based on hovered date
    category_agg = hover_filtered_df.groupby('Category')['Transaction Amount'].sum().nlargest(5)
    fig_category = go.Figure(go.Bar(x=category_agg.index, y=category_agg.values, marker_color='cyan'))
    fig_category.update_layout(title='Top 5 Categories', plot_bgcolor='black', paper_bgcolor='black', font=dict(color='white'))

    merchant_agg = hover_filtered_df.groupby('Merchant Name')['Transaction Amount'].sum().nlargest(5)
    fig_merchant = go.Figure(go.Bar(x=merchant_agg.index, y=merchant_agg.values, marker_color='magenta'))
    fig_merchant.update_layout(title='Top 5 Merchants’, plot_bgcolor=‘black’, paper_bgcolor=‘black’, font=dict(color=‘white’))
city_agg = hover_filtered_df.groupby('City')['Transaction Amount'].sum().nlargest(5)
fig_city = go.Figure(go.Bar(x=city_agg.index, y=city_agg.values, marker_color='yellow'))
fig_city.update_layout(title='Top 5 Cities', plot_bgcolor='black', paper_bgcolor='black', font=dict(color='white'))

return fig_time_series, fig_category, fig_merchant, fig_city








nd


import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime
import numpy as np

# Placeholder DataFrame setup - replace with your data loading logic
df = pd.DataFrame({
    'Transaction Date': pd.date_range(start='2023-01-01', periods=365, freq='D'),
    'Transaction Amount': np.random.randint(100, 500, size=365),
    'POS': np.random.choice([0, 1], size=365),
    'Category': np.random.choice(['Dining', 'Grocery', 'Entertainment'], size=365),
    'Merchant Name': np.random.choice(['Merchant A', 'Merchant B', 'Merchant C'], size=365),
    'City': np.random.choice(['City X', 'City Y', 'City Z'], size=365)
})
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div([
    html.H1('Credit Card Spend Analysis Dashboard', style={'color': 'white', 'backgroundColor': 'black'}),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=df['Transaction Date'].min(),
        end_date=datetime.date.today(),
        display_format='YYYY-MM-DD',
        style={'color': 'black'}
    ),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1W - Weekly', 'value': 'W'},
            {'label': '1M - Monthly', 'value': 'M'},
            {'label': '1Y - Yearly', 'value': 'Y'},
        ],
        value='M',  # Default selection
        labelStyle={'display': 'inline-block', 'margin': '10px', 'color': 'white'}
    ),
    dcc.Graph(id='transaction-time-series'),
    html.Div([
        dcc.Graph(id='category-bar-chart'),
        dcc.Graph(id='merchant-bar-chart'),
        dcc.Graph(id='city-bar-chart'),
    ], style={'display': 'flex', 'justifyContent': 'space-between', 'marginTop': '20px'}),
], style={'backgroundColor': 'rgb(40, 40, 40)', 'color': 'white', 'padding': '10px'})

@app.callback(
    [
        Output('transaction-time-series', 'figure'),
        Output('category-bar-chart', 'figure'),
        Output('merchant-bar-chart', 'figure'),
        Output('city-bar-chart', 'figure'),
    ],
    [
        Input('period-selector', 'value'),
        Input('date-picker-range', 'start_date'),
        Input('date-picker-range', 'end_date'),
    ]
)
def update_charts(period, start_date, end_date):
    filtered_df = df[(df['Transaction Date'] >= pd.to_datetime(start_date)) & (df['Transaction Date'] <= pd.to_datetime(end_date))]
    period_agg = 'D' if period == 'W' else period
    aggregated_df = filtered_df.resample(period_agg, on='Transaction Date').sum().reset_index()
    pos_filtered_df = filtered_df[filtered_df['POS'] == 1].resample(period_agg, on='Transaction Date').sum().reset_index()
    
    fig_time_series = go.Figure()

    # Overall Transactions Line
    fig_time_series.add_trace(go.Scatter(
        x=aggregated_df['Transaction Date'], 
        y=aggregated_df['Transaction Amount'], 
        mode='lines+markers',
        name='Overall',
        line=dict(color='red', width=3, shape='spline'),
        fill='tozeroy',
        fillcolor='rgba(255, 0, 0, 0.3)',
        hoverinfo='text',
        text=[f'Date: {d}<br>Amount: {a}' for d, a in zip(aggregated_df['Transaction Date'], aggregated_df['Transaction Amount'])],
    ))

    # POS Transactions Line
    fig_time_series.add_trace(go.Scatter(
        x=pos_filtered_df['Transaction Date'], 
        y=pos_filtered_df['Transaction Amount'], 
        mode='lines+markers',
        name='POS',
        line=dict(color='yellow', width=3, dash='dot', shape='spline'),
        fill='tonexty',
        fillcolor='rgba(255, 255, 0, 0.3)',
        hoverinfo='text',
        text=[f'Date: {d}<br>Amount: {a}' for d, a in zip(pos_filtered_df['Transaction Date'], pos_filtered_df['Transaction Amount’])],
))
fig_time_series.update_layout(
    title='Enhanced Transaction Amount Over Time',
    xaxis_title='Transaction Date',
    yaxis_title='Transaction Amount',
    hovermode='closest',
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgb(243, 243, 243)',
    font=dict(color='white'),
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
    margin=dict(l=20, r=20, t=40, b=20)
)

# Category Bar Chart
category_agg = filtered_df.groupby('Category')['Transaction Amount'].sum().sort_values()
fig_category = go.Figure(go.Bar(
    x=category_agg.values,
    y=category_agg.index,
    orientation='h'
))
fig_category.update_layout(
    title='Spend by Category',
    xaxis_title='Total Transaction Amount',
    yaxis_title='Category',
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgb(243, 243, 243)',
    font=dict(color='white')
)

# Merchant Bar Chart
merchant_agg = filtered_df.groupby('Merchant Name')['Transaction Amount'].sum().sort_values()
fig_merchant = go.Figure(go.Bar(
    x=merchant_agg.values,
    y=merchant_agg.index,
    orientation='h'
))
fig_merchant.update_layout(
    title='Spend by Merchant',
    xaxis_title='Total Transaction Amount',
    yaxis_title='Merchant Name',
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgb(243, 243, 243)',
    font=dict(color='white')
)

# City Bar Chart
city_agg = filtered_df.groupby('City')['Transaction Amount'].sum().sort_values()
fig_city = go.Figure(go.Bar(
    x=city_agg.values,
    y=city_agg.index,
    orientation='h'
))
fig_city.update_layout(
    title='Spend by City',
    xaxis_title='Total Transaction Amount',
    yaxis_title='City',
    plot_bgcolor='rgba(0,0,0,0)',
    paper_bgcolor='rgb(243, 243, 243)',
    font=dict(color='white')
)

return fig_time_series, fig_category, fig_merchant, fig_city











next
import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime
import numpy as np

# Placeholder DataFrame setup - replace with your actual data loading logic
df = pd.DataFrame({
    'Transaction Date': pd.date_range(start='2023-01-01', periods=365, freq='D'),
    'Transaction Amount': np.random.randint(100, 500, size=365),
    'POS': np.random.choice([0, 1], size=365),
    'Category': np.random.choice(['Dining', 'Grocery', 'Entertainment'], size=365),
    'Merchant Name': np.random.choice(['Merchant A', 'Merchant B', 'Merchant C'], size=365),
    'City': np.random.choice(['City X', 'City Y', 'City Z'], size=365)
})
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div([
    html.H1('Credit Card Spend Analysis Dashboard', style={'color': 'white', 'backgroundColor': 'black'}),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=df['Transaction Date'].min(),
        end_date=datetime.date.today(),
        display_format='YYYY-MM-DD',
    ),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1W - Weekly', 'value': 'W'},
            {'label': '1M - Monthly', 'value': 'M'},
            {'label': '1Y - Yearly', 'value': 'Y'},
        ],
        value='M',  # Default selection
        labelStyle={'display': 'inline-block', 'margin': '10px'}
    ),
    dcc.Graph(id='transaction-time-series'),
    html.Div([
        dcc.Graph(id='category-bar-chart'),
        dcc.Graph(id='merchant-bar-chart'),
        dcc.Graph(id='city-bar-chart'),
    ], style={'display': 'flex', 'justifyContent': 'space-between'}),
], style={'backgroundColor': 'rgb(40, 40, 40)', 'color': 'white', 'padding': '10px'})

@app.callback(
    [
        Output('transaction-time-series', 'figure'),
        Output('category-bar-chart', 'figure'),
        Output('merchant-bar-chart', 'figure'),
        Output('city-bar-chart', 'figure'),
    ],
    [
        Input('period-selector', 'value'),
        Input('date-picker-range', 'start_date'),
        Input('date-picker-range', 'end_date'),
    ]
)
def update_charts(period, start_date, end_date):
    filtered_df = df[(df['Transaction Date'] >= pd.to_datetime(start_date)) & (df['Transaction Date'] <= pd.to_datetime(end_date))]
    period_agg = 'D' if period == 'W' else period
    aggregated_df = filtered_df.resample(period_agg, on='Transaction Date').sum().reset_index()
    pos_filtered_df = filtered_df[filtered_df['POS'] == 1].resample(period_agg, on='Transaction Date').sum().reset_index()

    # Time Series Chart (remains unchanged)
    fig_time_series = go.Figure()
    # Add traces and layout adjustments for time series chart here
    
    # Category Bar Chart
    category_agg = filtered_df.groupby('Category')['Transaction Amount'].sum().sort_values()
    fig_category = go.Figure(go.Bar(
        x=category_agg.values,
        y=category_agg.index,
        orientation='h'
    ))
    fig_category.update_layout(
        title='Spend by Category',
        xaxis_title='Total Transaction Amount',
        yaxis_title='Category',
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgb(243, 243, 243)',
        font=dict(color='black')
    )

    # Merchant Bar Chart
    merchant_agg = filtered_df.groupby('Merchant Name')['Transaction Amount'].sum().sort_values()
    fig_merchant = go.Figure(go.Bar(
        x=merchant_agg.values,
        y=merchant_agg.index,
        orientation='h'
    ))
    fig_merchant.update_layout(
        title='Spend by Merchant',
        xaxis_title='Total Transaction Amount',
        yaxis_title='Merchant Name',
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgb(243, 243, 243)',
        font=dict(color='black')
    )

    # City Bar Chart
    city_agg = filtered_df.groupby('City')['Transaction Amount'].sum().sort_values()
    fig_city= go.Figure(go.Bar(
x=city_agg.values,
y=city_agg.index,
orientation=‘h’
))
fig_city.update_layout(
title=‘Spend by City’,
xaxis_title=‘Total Transaction Amount’,
yaxis_title=‘City’,
plot_bgcolor=‘rgba(0,0,0,0)’,
paper_bgcolor=‘rgb(243, 243, 243)’,
font=dict(color=‘black’)
)








new

import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime
import numpy as np

# Placeholder DataFrame setup - replace with your actual data loading logic
df = pd.DataFrame({
    'Transaction Date': pd.date_range(start='2023-01-01', periods=365, freq='D'),
    'Transaction Amount': np.random.randint(100, 500, size=365),
    'POS': np.random.choice([0, 1], size=365),
    'Category': np.random.choice(['Dining', 'Grocery', 'Entertainment'], size=365),
    'Merchant Name': np.random.choice(['Merchant A', 'Merchant B', 'Merchant C'], size=365),
    'City': np.random.choice(['City X', 'City Y', 'City Z'], size=365)
})
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div([
    html.H1('Credit Card Spend Analysis Dashboard', style={'textAlign': 'center', 'color': '#007BFF', 'padding': '10px'}),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=df['Transaction Date'].min(),
        end_date=datetime.date.today(),
        display_format='YYYY-MM-DD',
        style={'padding': '10px', 'fontSize': '16px'}
    ),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1W - Weekly', 'value': 'W'},
            {'label': '1M - Monthly', 'value': 'M'},
            {'label': '1Y - Yearly', 'value': 'Y'},
        ],
        value='M',  # Default selection
        labelStyle={'display': 'inline-block', 'margin': '10px', 'fontSize': '16px'}
    ),
    dcc.Graph(id='transaction-time-series'),
    dcc.Graph(id='category-bar-chart'),
    dcc.Graph(id='merchant-bar-chart'),
    dcc.Graph(id='city-bar-chart'),
], style={'backgroundColor': '#FAFAFA', 'padding': '20px', 'borderRadius': '5px'})

@app.callback(
    [
        Output('transaction-time-series', 'figure'),
        Output('category-bar-chart', 'figure'),
        Output('merchant-bar-chart', 'figure'),
        Output('city-bar-chart', 'figure')
    ],
    [
        Input('period-selector', 'value'),
        Input('date-picker-range', 'start_date'),
        Input('date-picker-range', 'end_date')
    ]
)
def update_charts(period, start_date, end_date):
    filtered_df = df[(df['Transaction Date'] >= pd.to_datetime(start_date)) &
                     (df['Transaction Date'] <= pd.to_datetime(end_date))]
    # Time Series Chart
    period_agg = 'D' if period == 'W' else period
    aggregated_df = filtered_df.resample(period_agg, on='Transaction Date').sum().reset_index()
    pos_filtered_df = filtered_df[filtered_df['POS'] == 1].resample(period_agg, on='Transaction Date').sum().reset_index()
    
    fig_time_series = go.Figure()
    # Add traces and layout adjustments as per your given time series chart configuration
    
    # Category Bar Chart
    category_agg = filtered_df.groupby('Category')['Transaction Amount'].sum().sort_values()
    fig_category = go.Figure(go.Bar(
        x=category_agg.values,
        y=category_agg.index,
        orientation='h',
        marker_color='cyan'
    ))
    fig_category.update_layout(
        title='Spend by Category',
        xaxis_title='Amount',
        yaxis_title='Category',
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgb(243, 243, 243)',
        font=dict(color='black')
    )
    
    # Merchant Bar Chart
    merchant_agg = filtered_df.groupby('Merchant Name')['Transaction Amount'].sum().sort_values()
    fig_merchant = go.Figure(go.Bar(
        x=merchant_agg.values,
        y=merchant_agg.index,
        orientation='h',
        marker_color='magenta'
    ))
    fig_merchant.update_layout(
        title='Spend by Merchant',
        xaxis_title='Amount',
        yaxis_title='Merchant',
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgb(243, 243, 243)',
        font=dict(color='black')
    )

    # City Bar Chart
    city_agg = filtered_df.groupby('City')['Transaction Amount’].sum().sort_values()
fig_city = go.Figure(go.Bar(
x=city_agg.values,
y=city_agg.index,
orientation=‘h’,
marker_color=‘yellow’
))
fig_city.update_layout(
title=‘Spend by City’,
xaxis_title=‘Amount’,
yaxis_title=‘City’,
plot_bgcolor=‘rgba(0,0,0,0)’,
paper_bgcolor=‘rgb(243, 243, 243)’,
font=dict(color=‘black’)
)
return fig_time_series, fig_category, fig_merchant, fig_city

if name == ‘main’:
app.run_server(debug=True








dfd
import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime
import numpy as np

# Placeholder DataFrame setup - replace with your actual data loading logic
df = pd.DataFrame({
    'Transaction Date': pd.date_range(start='2023-01-01', periods=365, freq='D'),
    'Transaction Amount': np.random.randint(100, 500, size=365),
    'POS': np.random.choice([0, 1], size=365),
    'Category': np.random.choice(['Dining', 'Grocery', 'Entertainment'], size=365),
    'Merchant Name': np.random.choice(['Merchant A', 'Merchant B', 'Merchant C'], size=365),
    'City': np.random.choice(['City X', 'City Y', 'City Z'], size=365)
})
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div([
    html.H1('Credit Card Spend Analysis Dashboard', style={'textAlign': 'center', 'color': '#007BFF', 'padding': '10px'}),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=df['Transaction Date'].min(),
        end_date=datetime.date.today(),
        display_format='YYYY-MM-DD',
        style={'padding': '10px', 'fontSize': '16px'}
    ),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1W - Weekly', 'value': 'W'},
            {'label': '1M - Monthly', 'value': 'M'},
            {'label': '1Y - Yearly', 'value': 'Y'},
        ],
        value='M',  # Default selection
        labelStyle={'display': 'inline-block', 'margin': '10px', 'fontSize': '16px'}
    ),
    dcc.Graph(id='transaction-time-series'),
    dcc.Graph(id='category-bar-chart'),
    dcc.Graph(id='merchant-bar-chart'),
    dcc.Graph(id='city-bar-chart'),
], style={'backgroundColor': '#FAFAFA', 'padding': '20px', 'borderRadius': '5px'})

@app.callback(
    [Output('transaction-time-series', 'figure'),
     Output('category-bar-chart', 'figure'),
     Output('merchant-bar-chart', 'figure'),
     Output('city-bar-chart', 'figure')],
    [Input('period-selector', 'value'),
     Input('date-picker-range', 'start_date'),
     Input('date-picker-range', 'end_date')]
)
def update_charts(period, start_date, end_date):
    filtered_df = df[(df['Transaction Date'] >= pd.to_datetime(start_date)) &
                     (df['Transaction Date'] <= pd.to_datetime(end_date))]

    # Time Series Chart
    fig_time_series = go.Figure()
    fig_time_series.add_trace(go.Scatter(
        x=filtered_df['Transaction Date'],
        y=filtered_df['Transaction Amount'],
        mode='lines+markers',
        name='Overall',
        line=dict(color='red', width=2)
    ))
    fig_time_series.update_layout(
        title='Transaction Amount Over Time',
        xaxis_title='Transaction Date',
        yaxis_title='Transaction Amount',
        plot_bgcolor='#FAFAFA',
        paper_bgcolor='#FAFAFA',
        font=dict(color='black')
    )

    # Category Bar Chart
    category_agg = filtered_df.groupby('Category')['Transaction Amount'].sum().sort_values(ascending=False)
    fig_category = go.Figure(go.Bar(
        x=category_agg.index,
        y=category_agg.values,
        marker_color='cyan'
    ))
    fig_category.update_layout(
        title='Spend by Category',
        xaxis_title='Category',
        yaxis_title='Amount',
        plot_bgcolor='#FAFAFA',
        paper_bgcolor='#FAFAFA',
        font=dict(color='black')
    )

    # Merchant Bar Chart
    merchant_agg = filtered_df.groupby('Merchant Name')['Transaction Amount'].sum().sort_values(ascending=False)
    fig_merchant = go.Figure(go.Bar(
        x=merchant_agg.index,
        y=merchant_agg.values,
        marker_color='magenta'
    ))
    fig_merchant.update_layout(
        title='Spend by Merchant',
        xaxis_title='Merchant',
        yaxis_title='Amount',
        plot_bgcolor='#FAFAFA',
        paper_bgcolor='#FAFAFA',
        font=dict(color='black')
    )

    # City Bar Chart
    city_agg = filtered_df.groupby('City')['Transaction Amount
‘].sum().sort_values(ascending=False)
fig_city = go.Figure(go.Bar(
x=city_agg.index,
y=city_agg.values,
marker_color=‘yellow’
))
fig_city.update_layout(
title=‘Spend by City’,
xaxis_title=‘City’,
yaxis_title=‘Amount’,
plot_bgcolor=’#FAFAFA’,
paper_bgcolor=’#FAFAFA’,
font=dict(color=‘black’)
)
return fig_time_series, fig_category, fig_merchant, fig_city

if name == ‘main’:
app.run_server(debug=True)






r
import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime

# Placeholder DataFrame setup - replace with your actual data loading logic
df = pd.DataFrame({
    'Transaction Date': pd.date_range(start='2023-01-01', periods=365, freq='D'),
    'Transaction Amount': pd.np.random.randint(100, 500, size=365),
    'POS': pd.np.random.choice([0, 1], size=365),
    'Category': pd.np.random.choice(['Dining', 'Grocery', 'Entertainment'], size=365),
    'Merchant Name': pd.np.random.choice(['Merchant A', 'Merchant B', 'Merchant C'], size=365),
    'City': pd.np.random.choice(['City X', 'City Y', 'City Z'], size=365)
})
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div([
    html.H1('Credit Card Spend Analysis Dashboard', style={'color': 'white', 'backgroundColor': 'black'}),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=df['Transaction Date'].min(),
        end_date=datetime.date.today(),
        display_format='YYYY-MM-DD',
    ),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1W - Weekly', 'value': 'W'},
            {'label': '1M - Monthly', 'value': 'M'},
            {'label': '1Y - Yearly', 'value': 'Y'},
        ],
        value='M',  # Default selection
        labelStyle={'display': 'inline-block', 'margin': '10px', 'color': 'white'}
    ),
    dcc.Graph(id='transaction-time-series'),
    dcc.Graph(id='category-bar-chart'),
    dcc.Graph(id='merchant-bar-chart'),
    dcc.Graph(id='city-bar-chart'),
], style={'backgroundColor': 'rgb(40, 40, 40)', 'color': 'white', 'padding': '10px'})

# Callback for the time series chart
@app.callback(
    Output('transaction-time-series', 'figure'),
    [Input('period-selector', 'value'),
     Input('date-picker-range', 'start_date'),
     Input('date-picker-range', 'end_date')]
)
def update_time_series_chart(period, start_date, end_date):
    # Your existing logic for updating the time series chart
    # ...

# Generate static bar chart figures for Categories, Merchants, and Cities
@app.callback(
    Output('category-bar-chart', 'figure'),
    Output('merchant-bar-chart', 'figure'),
    Output('city-bar-chart', 'figure'),
    [Input('date-picker-range', 'start_date'),
     Input('date-picker-range', 'end_date')]
)
def update_bar_charts(start_date, end_date):
    filtered_df = df[(df['Transaction Date'] >= pd.to_datetime(start_date)) & (df['Transaction Date'] <= pd.to_datetime(end_date))]
    
    # Aggregating data for category, merchant, and city
    category_agg = filtered_df.groupby('Category')['Transaction Amount'].sum().nlargest(10)
    merchant_agg = filtered_df.groupby('Merchant Name')['Transaction Amount'].sum().nlargest(10)
    city_agg = filtered_df.groupby('City')['Transaction Amount'].sum().nlargest(10)
    
    # Creating bar chart for Category
    category_fig = go.Figure(go.Bar(
        x=category_agg.index,
        y=category_agg.values,
        marker_color='cyan'
    ))
    category_fig.update_layout(title='Top Categories', xaxis_title='Category', yaxis_title='Amount', plot_bgcolor='black', paper_bgcolor='black', font=dict(color='white'))
    
    # Creating bar chart for Merchant Name
    merchant_fig = go.Figure(go.Bar(
        x=merchant_agg.index,
        y=merchant_agg.values,
        marker_color='magenta'
    ))
    merchant_fig.update_layout(title='Top Merchants', xaxis_title='Merchant', yaxis_title='Amount', plot_bgcolor='black', paper_bgcolor='black', font=dict(color='white'))

    # Creating bar chart for City
    city_fig = go.Figure(go.Bar(
        x=city_agg.index,
        y=city_agg.values,
        marker_color='yellow'
    ))
    city_fig.update_layout(title='Top Cities', xaxis_title='City', yaxis_title='Amount', plot_bgcolor='black', paper_bgcolor='black', font=dict(color='white'))

    return category_fig, merchant_fig, city_fig









import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime

# Placeholder DataFrame setup - replace with your actual data loading logic
df = pd.DataFrame({
    'Transaction Date': pd.date_range(start='2023-01-01', periods=365, freq='D'),
    'Transaction Amount': pd.np.random.randint(100, 500, size=365),
    'POS': pd.np.random.choice([0, 1], size=365)
})
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div([
    html.H1('Credit Card Spend Analysis Dashboard', style={'color': 'white', 'backgroundColor': 'black'}),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=df['Transaction Date'].min(),
        end_date=datetime.date.today(),
        display_format='YYYY-MM-DD',
        style={'color': 'black'}
    ),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1W - Weekly', 'value': 'W'},
            {'label': '1M - Monthly', 'value': 'M'},
            {'label': '1Y - Yearly', 'value': 'Y'},
        ],
        value='M',  # Default selection
        labelStyle={'display': 'inline-block', 'margin': '10px', 'color': 'white'},
        style={'textAlign': 'center'}
    ),
    dcc.Graph(id='transaction-time-series'),
], style={'backgroundColor': 'black', 'color': 'white', 'padding': '10px'})

@app.callback(
    Output('transaction-time-series', 'figure'),
    [Input('period-selector', 'value'),
     Input('date-picker-range', 'start_date'),
     Input('date-picker-range', 'end_date')]
)
def update_chart(period, start_date, end_date):
    # Placeholder functionality for dynamic aggregation based on 'period'
    
    # Example trace for Overall Transactions
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=df['Transaction Date'], 
        y=df['Transaction Amount'], 
        mode='lines+markers',
        name='Overall',
        line=dict(color='red', width=2),
        hoverinfo='text',
        text=[f'Date: {d.strftime("%Y-%m-%d")}<br>Amount: {a}' for d, a in zip(df['Transaction Date'], df['Transaction Amount'])],
    ))

    # Styling the figure
    fig.update_layout(
        title='Transaction Amount Over Time',
        xaxis_title='Transaction Date',
        yaxis_title='Transaction Amount',
        hovermode='closest',
        plot_bgcolor='black',
        paper_bgcolor='black',
        font=dict(color='white'),
        legend=dict(orientation="h", yanchor="bottom", y=-0.2, xanchor="center", x=0.5)
    )
    
    return fig

if __name__ == '__main__':
    app.run_server(debug=True)











import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime

# Assuming the DataFrame 'df' is already loaded with your data
# Placeholder DataFrame setup - replace with your data loading logic
df = pd.DataFrame({
    'Transaction Date': pd.date_range(start='2023-01-01', periods=365, freq='D'),
    'Transaction Amount': pd.np.random.randint(100, 500, size=365),
    'POS': pd.np.random.choice([0, 1], size=365)
})
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div([
    html.H1('Credit Card Spend Analysis Dashboard', style={'color': 'white', 'backgroundColor': 'black'}),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1W - Weekly', 'value': 'W'},
            {'label': '1M - Monthly', 'value': 'M'},
            {'label': '1Y - Yearly', 'value': 'Y'},
        ],
        value='M',  # Default selection
        labelStyle={'display': 'inline-block', 'margin': '10px', 'color': 'white'},
        style={'textAlign': 'center'}
    ),
    dcc.Graph(id='transaction-time-series'),
], style={'backgroundColor': 'black', 'color': 'white', 'padding': '10px'})

@app.callback(
    Output('transaction-time-series', 'figure'),
    [Input('period-selector', 'value')]
)
def update_chart(period):
    # Placeholder for the actual logic to aggregate data based on 'period'
    
    fig = go.Figure()

    # Sample trace using the whole dataset
    fig.add_trace(go.Scatter(
        x=df['Transaction Date'], 
        y=df['Transaction Amount'], 
        mode='lines+markers',
        name='Overall',
        line=dict(color='red', width=3),
    ))

    # Update layout for better visuals
    fig.update_layout(
        title='Transaction Amount Over Time',
        xaxis_title='Transaction Date',
        yaxis_title='Transaction Amount',
        hovermode='closest',
        plot_bgcolor='black',
        paper_bgcolor='black',
        font=dict(color='white'),
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
    )
    
    return fig

if __name__ == '__main__':
    app.run_server(debug=True)








@app.callback(
    Output('transaction-time-series', 'figure'),
    [Input('period-selector', 'value'),
     Input('date-picker-range', 'start_date'),
     Input('date-picker-range', 'end_date')]
)
def update_chart(period, start_date, end_date):
    filtered_df = df[(df['Transaction Date'] >= pd.to_datetime(start_date)) & (df['Transaction Date'] <= pd.to_datetime(end_date))]
    period_agg = 'D' if period == 'W' else period
    aggregated_df = filtered_df.resample(period_agg, on='Transaction Date').sum().reset_index()
    pos_filtered_df = filtered_df[filtered_df['POS'] == 1].resample(period_agg, on='Transaction Date').sum().reset_index()
    
    fig = go.Figure()

    # Overall Transactions Line
    fig.add_trace(go.Scatter(
        x=aggregated_df['Transaction Date'], 
        y=aggregated_df['Transaction Amount'], 
        mode='lines+markers',
        name='Overall',
        line=dict(color='red', width=3, shape='spline'),
        fill='tozeroy',
        fillcolor='rgba(255, 0, 0, 0.3)',
        hoverinfo='text',
        text=[f'Date: {d}<br>Amount: {a}' for d, a in zip(aggregated_df['Transaction Date'], aggregated_df['Transaction Amount'])],
    ))

    # POS Transactions Line
    fig.add_trace(go.Scatter(
        x=pos_filtered_df['Transaction Date'], 
        y=pos_filtered_df['Transaction Amount'], 
        mode='lines+markers',
        name='POS',
        line=dict(color='yellow', width=3, dash='dot', shape='spline'),
        fill='tonexty',
        fillcolor='rgba(255, 255, 0, 0.3)',
        hoverinfo='text',
        text=[f'Date: {d}<br>Amount: {a}' for d, a in zip(pos_filtered_df['Transaction Date'], pos_filtered_df['Transaction Amount'])],
    ))

    # Enhance layout
    fig.update_layout(
        title='Enhanced Transaction Amount Over Time',
        xaxis_title='Transaction Date',
        yaxis_title='Transaction Amount',
        hovermode='closest',
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgb(243, 243, 243)',
        font=dict(color='black'),
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        margin=dict(l=20, r=20, t=40, b=20)
    )

    # Example of adding an annotation
    if not aggregated_df.empty:
        fig.add_annotation(
            x=aggregated_df['Transaction Date'].iloc[-1],
            y=aggregated_df['Transaction Amount'].iloc[-1],
            text="Latest",
            showarrow=True,
            arrowhead=1,
            ax=0,
            ay=-40
        )

    return fig





b

import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime

# Assuming the DataFrame 'df' is already loaded with your data
# Placeholder DataFrame setup - replace with your data loading logic
df = pd.DataFrame({
    'Transaction Date': pd.date_range(start='2023-01-01', periods=365, freq='D'),
    'Transaction Amount': pd.np.random.randint(100, 500, size=365),
    'POS': pd.np.random.choice([0, 1], size=365)
})
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div([
    html.H1('Credit Card Spend Analysis Dashboard', style={'color': 'white', 'backgroundColor': 'black'}),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=df['Transaction Date'].min(),
        end_date=datetime.date.today(),
        display_format='YYYY-MM-DD',
    ),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1W - Weekly', 'value': 'W'},
            {'label': '1M - Monthly', 'value': 'M'},
            {'label': '1Y - Yearly', 'value': 'Y'},
        ],
        value='M',  # Default selection
        labelStyle={'display': 'inline-block', 'margin': '10px'}
    ),
    dcc.Graph(id='transaction-time-series'),
], style={'backgroundColor': 'rgb(40, 40, 40)', 'color': 'white', 'padding': '10px'})

@app.callback(
    Output('transaction-time-series', 'figure'),
    [Input('period-selector', 'value'),
     Input('date-picker-range', 'start_date'),
     Input('date-picker-range', 'end_date')]
)
def update_chart(period, start_date, end_date):
    filtered_df = df[(df['Transaction Date'] >= pd.to_datetime(start_date)) & (df['Transaction Date'] <= pd.to_datetime(end_date))]
    
    # Aggregating based on the selected period
    period_agg = 'D' if period == 'W' else period
    aggregated_df = filtered_df.resample(period_agg, on='Transaction Date').sum().reset_index()
    
    # Preparing data for Overall and POS lines
    overall = go.Scatter(x=aggregated_df['Transaction Date'], y=aggregated_df['Transaction Amount'], mode='lines', name='Overall', line=dict(color='red'), fill='tozeroy')
    
    pos_filtered_df = filtered_df[filtered_df['POS'] == 1].resample(period_agg, on='Transaction Date').sum().reset_index()
    pos = go.Scatter(x=pos_filtered_df['Transaction Date'], y=pos_filtered_df['Transaction Amount'], mode='lines', name='POS', line=dict(color='yellow'), fill='tozeroy')
    
    # Creating the figure
    fig = go.Figure(data=[overall, pos])
    fig.update_layout(
        title='Transaction Amount Over Time',
        xaxis_title='Transaction Date',
        yaxis_title='Transaction Amount',
        hovermode='x',
        plot_bgcolor='black',
        paper_bgcolor='rgb(40, 40, 40)',
        font=dict(color='white'),
        legend=dict(font=dict(size=10, color='white')),
        xaxis=dict(gridcolor='darkgrey', color='white'),
        yaxis=dict(gridcolor='darkgrey', color='white')
    )
    
    return fig

if __name__ == '__main__':
    app.run_server(debug=True)









import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime

# Placeholder for loading your DataFrame
# Ensure to replace this with your actual data loading logic
# Example: df = pd.read_excel('path_to_your_file.xlsx')
df = pd.DataFrame({
    'Transaction Date': pd.date_range(start='2022-01-01', periods=120, freq='D'),
    'Transaction Amount': pd.np.random.randint(100, 500, size=120),
    'POS': pd.np.random.choice([0, 1], size=120)
})
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div([
    html.H1('Credit Card Spend Analysis Dashboard', style={'color': 'white', 'backgroundColor': 'black'}),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=df['Transaction Date'].min(),
        end_date=datetime.date.today(),
        display_format='YYYY-MM-DD',
    ),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1W - Weekly', 'value': 'W'},
            {'label': '1M - Monthly', 'value': 'M'},
            {'label': '1Y - Yearly', 'value': 'Y'},
        ],
        value='M',  # Default selection
        labelStyle={'display': 'inline-block', 'margin': '10px'}
    ),
    dcc.Graph(id='time-series-chart'),
], style={'backgroundColor': 'rgb(40, 40, 40)', 'color': 'white', 'padding': '10px'})

@app.callback(
    Output('time-series-chart', 'figure'),
    [Input('period-selector', 'value'),
     Input('date-picker-range', 'start_date'),
     Input('date-picker-range', 'end_date')]
)
def update_chart(period, start_date, end_date):
    # Filter data based on the selected date range
    filtered_df = df[(df['Transaction Date'] >= pd.to_datetime(start_date)) & (df['Transaction Date'] <= pd.to_datetime(end_date))]
    
    # Aggregate data based on the selected period
    if period == 'W':
        aggregated_df = filtered_df.resample('W', on='Transaction Date').sum().reset_index()
    elif period == 'M':
        aggregated_df = filtered_df.resample('M', on='Transaction Date').sum().reset_index()
    else:  # 'Y'
        aggregated_df = filtered_df.resample('Y', on='Transaction Date').sum().reset_index()
    
    # Create the figure
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=aggregated_df['Transaction Date'], y=aggregated_df['Transaction Amount'], mode='lines+markers', name='Aggregated Transactions'))
    fig.update_layout(
        title='Aggregated Transaction Amount Over Time',
        xaxis_title='Date',
        yaxis_title='Transaction Amount',
        plot_bgcolor='black',
        paper_bgcolor='rgb(40, 40, 40)',
        font=dict(color='white')
    )
    
    return fig

if __name__ == '__main__':
    app.run_server(debug=True)






import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime

# Load your data
df = pd.read_excel('sample.xlsx')
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

# Initialize the Dash app
app = Dash(__name__)

# App layout
app.layout = html.Div(children=[
    html.H1("Credit Card Spend Analysis Dashboard"),
    dcc.DatePickerRange(
        id='date-picker-range',
        start_date=df['Transaction Date'].min(),
        end_date=datetime.date.today(),
        display_format='YYYY-MM-DD',
    ),
    dcc.RadioItems(
        id='period-selector',
        options=[
            {'label': '1D', 'value': 'D'},
            {'label': '1W', 'value': 'W'},
            {'label': '1M', 'value': 'M'},
            {'label': '1Y', 'value': 'Y'},
        ],
        value='M',  # Default to 1M
        labelStyle={'display': 'inline-block'}
    ),
    dcc.Graph(id='transaction-time-series'),
])

# Callback to update the graph based on the input period and date range
@app.callback(
    Output('transaction-time-series', 'figure'),
    [Input('date-picker-range', 'start_date'),
     Input('date-picker-range', 'end_date'),
     Input('period-selector', 'value')]
)
def update_graph(start_date, end_date, period):
    # Filter based on the selected date range
    filtered_df = df[(df['Transaction Date'] >= pd.to_datetime(start_date)) & (df['Transaction Date'] <= pd.to_datetime(end_date))]

    # Aggregate data based on the selected period
    if period == 'D':
        aggregated_df = filtered_df.resample('D', on='Transaction Date').sum().reset_index()
    elif period == 'W':
        aggregated_df = filtered_df.resample('W-Mon', on='Transaction Date').sum().reset_index()
    elif period == 'M':
        aggregated_df = filtered_df.resample('M', on='Transaction Date').sum().reset_index()
    elif period == 'Y':
        aggregated_df = filtered_df.resample('Y', on='Transaction Date').sum().reset_index()

    # Create the figure
    figure = go.Figure()
    figure.add_trace(go.Scatter(x=aggregated_df['Transaction Date'], y=aggregated_df['Transaction Amount'], mode='lines+markers', name='Total Transaction Amount'))
    figure.update_layout(title='Transaction Amount Over Time', xaxis_title='Date', yaxis_title='Amount', plot_bgcolor='lightgrey')

    return figure

# Run the server
if __name__ == '__main__':
    app.run_server(debug=True)ty
import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc
from dash.dependencies import Input, Output
import datetime

# Sample DataFrame setup
# Replace this with your actual data loading logic
data = {
    'Transaction Date': pd.date_range(start='2022-01-01', periods=120, freq='D'),
    'Transaction Amount': pd.np.random.randint(100, 500, size=120),
    'POS': pd.np.random.choice([0, 1], size=120)
}
df = pd.DataFrame(data)
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

app = Dash(__name__)

app.layout = html.Div(children=[
    html.H1('Credit Card Spend Analysis Dashboard', style={'color': 'white', 'backgroundColor': 'black'}),
    
    html.Div([
        dcc.RadioItems(
            id='time-period-selector',
            options=[
                {'label': '1D', 'value': '1D'},
                {'label': '1W', 'value': '1W'},
                {'label': '1M', 'value': '1M'},
                {'label': '1Y', 'value': '1Y'},
            ],
            value='1M',  # Default value
            labelStyle={'display': 'inline-block', 'color': 'white'},
            style={'padding': '20px'}
        ),
        dcc.DatePickerRange(
            id='date-picker-range',
            start_date=datetime.date.today() - datetime.timedelta(days=30),
            end_date=datetime.date.today(),
            calendar_orientation='horizontal',
        ),
    ], style={'backgroundColor': 'black'}),
    
    dcc.Graph(id='time-series-chart'),
])

@app.callback(
    Output('time-series-chart', 'figure'),
    [Input('time-period-selector', 'value'),
     Input('date-picker-range', 'start_date'),
     Input('date-picker-range', 'end_date')]
)
def update_chart(selected_period, start_date, end_date):
    filtered_df = df.copy()  # Placeholder for actual filtering based on inputs
    
    # Example filtering logic (to be replaced with actual logic based on 'selected_period')
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date)
    filtered_df = filtered_df[(filtered_df['Transaction Date'] >= start_date) & (filtered_df['Transaction Date'] <= end_date)]

    # Creating the time series plot
    fig = go.Figure()
    
    fig.add_trace(go.Scatter(x=filtered_df['Transaction Date'], y=filtered_df['Transaction Amount'], mode='lines', name='Overall', line=dict(color='red')))
    if 'POS' in filtered_df.columns:
        pos_df = filtered_df[filtered_df['POS'] == 1]
        fig.add_trace(go.Scatter(x=pos_df['Transaction Date'], y=pos_df['Transaction Amount'], mode='lines', name='POS', line=dict(color='yellow')))
    
    # Customizing the plot appearance
    fig.update_layout(plot_bgcolor='black', paper_bgcolor='black', font=dict(color='white'))
    
    return fig

if __name__ == '__main__':
    app.run_server(debug=True)








tt
import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc

# Load your DataFrame
# df = pd.read_csv('your_data.csv')  # Replace with your actual data loading logic

# Ensure the 'Transaction Date' is a datetime type
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])

# Aggregate the transaction amounts by date for Overall and POS
df_overall = df.groupby('Transaction Date')['Transaction Amount'].sum().reset_index(name='Overall')
df_pos = df[df['POS'] == 1].groupby('Transaction Date')['Transaction Amount'].sum().reset_index(name='POS')

# Create the time series plot
fig_time_series = go.Figure()

# Add trace for Overall with red color
fig_time_series.add_trace(go.Scatter(
    x=df_overall['Transaction Date'], 
    y=df_overall['Overall'], 
    mode='lines',
    line=dict(color='red', width=2),
    fill='tozeroy',  # Fill to zero on the y-axis
    fillcolor='rgba(255, 0, 0, 0.5)',  # Red fill with transparency for shadow effect
    hoverinfo='y',  # Show only the y value on hover
    name='Overall'
))

# Add trace for POS with yellow color
fig_time_series.add_trace(go.Scatter(
    x=df_pos['Transaction Date'], 
    y=df_pos['POS'], 
    mode='lines',
    line=dict(color='yellow', width=2),
    fill='tozeroy',  # Fill to zero on the y-axis
    fillcolor='rgba(255, 255, 0, 0.5)',  # Yellow fill with transparency for shadow effect
    hoverinfo='y',  # Show only the y value on hover
    name='POS'
))

# Update the layout to customize the look
fig_time_series.update_layout(
    title='Transaction Amount Over Time',
    xaxis_title='Transaction Date',
    yaxis_title='Transaction Amount',
    hovermode='x',
    plot_bgcolor='black',  # Set the plot background to black
    paper_bgcolor='rgb(40, 40, 40)',  # Set the paper background to dark grey
    font=dict(color='white'),  # Set the font color to white
    legend=dict(
        font=dict(
            size=10,
            color='white'
        ),
    ),
    xaxis=dict(
        gridcolor='darkgrey',  # Set grid color to dark grey
        color='white'  # Set axis text color to white
    ),
    yaxis=dict(
        gridcolor='darkgrey',  # Set grid color to dark grey
        color='white'  # Set axis text color to white
    )
)

# Create a Dash application
app = Dash(__name__)

# Define the app layout
app.layout = html.Div(children=[
    html.H1('Credit Card Spend Analysis Dashboard', style={'color': 'white', 'backgroundColor': 'black'}),
    dcc.Graph(id='time-series-chart', figure=fig_time_series, style={'backgroundColor': 'black'}),
    # More components will be added here later
], style={'backgroundColor': 'rgb(40, 40, 40)'})

# Run the server
if __name__ == '__main__':
    app.run_server(debug=True)










# Import necessary libraries
import pandas as pd
import plotly.express as px
from dash import Dash, html, dcc

# Assuming you have loaded your DataFrame as `df`
# df = pd.read_csv('your_data.csv')  # or however you acquire your data

# Preprocessing: Ensure Transaction Date is a datetime type and aggregate the data
df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])
agg_df = df.groupby(['Transaction Date', 'General Category']).agg({'Transaction Amount': 'sum'}).reset_index()

# Create the time series plot
fig_time_series = px.line(agg_df, x='Transaction Date', y='Transaction Amount', color='General Category', title='Transaction Amount Over Time')

# Create a Dash application
app = Dash(__name__)

# Define the app layout to include the time series plot
app.layout = html.Div(children=[
    html.H1('Credit Card Spend Analysis Dashboard'),
    dcc.Graph(id='time-series-chart', figure=fig_time_series),
    # You will add more components to the layout later
])

# Run the application
if __name__ == '__main__':
    app.run_server(debug=True)
















import pandas as pd
import plotly.graph_objs as go
from dash import Dash, html, dcc, Input, Output

# Load your DataFrame here
# df = pd.read_csv('your_data.csv')

app = Dash(__name__)

app.layout = html.Div(children=[
    html.H1('Credit Card Spend Analysis'),
    
    # Time Series Plot
    dcc.Graph(id='time-series-chart'),
    
    # Bar Charts for Categories, Age Groups, Top Merchants
    html.Div(children=[
        dcc.Graph(id='category-bar-chart'),
        dcc.Graph(id='age-group-bar-chart'),
        dcc.Graph(id='merchant-bar-chart'),
    ], style={'display': 'flex', 'flex-direction': 'row'}),
    
    # ... include other dashboard components as needed ...
])

@app.callback(
    Output('time-series-chart', 'figure'),
    # Input(...) - define any inputs you need for interactive components
)
def update_time_series(/* parameters if needed */):
    # Aggregate your data by date and transaction type here
    # Create a figure and return it
    figure = go.Figure()  # Your Plotly figure logic here
    return figure

@app.callback(
    Output('category-bar-chart', 'figure'),
    # Input(...) - define any inputs for the bar chart if needed
)
def update_category_bar_chart(/* parameters if needed */):
    # Aggregate your data for category spend here
    # Create a figure and return it
    figure = go.Figure()  # Your Plotly figure logic here
    return figure

# Define similar callbacks for other components as needed...

if __name__ == '__main__':
    app.run_server(debug=True)









=IF(AND(B2=1, G2=1), "1", "0") & IF(AND(C2=1, H2=1), "1", "0") & IF(AND(D2=1, I2=1), "1", "0") & IF(AND(E2=1, J2=1), "1", "0") & IF(AND(F2=1, K2=1), "1", "0")
=IF(B2=G2, "1", "0") & IF(C2=H2, "1", "0") & IF(D2=I2, "1", "0") & IF(E2=J2, "1", "0") & IF(F2=K2, "1", "0")

=IF(OR((B2=G2)*(B2<>""), (C2=H2)*(C2<>""), (D2=I2)*(D2<>""), (E2=J2)*(E2<>""), (F2=K2)*(F2<>"")), 1, 0)








CREATE OR REPLACE TEMP TABLE AMBS_BASE1 AS
WITH Preprocessed AS (
  SELECT
    *,
    CASE
      WHEN TRIM(BLOCK_CODE_1) = '' THEN NULL
      ELSE DATE_BLOCK_CODE_1
    END AS DATE_BLOCK_CODE_1_new,
    CASE
      WHEN TRIM(BLOCK_CODE_2) = '' THEN NULL
      ELSE DATE_BLOCK_CODE_2
    END AS DATE_BLOCK_CODE_2_new
  FROM
    `project.dataset.AMBS_BASE` -- Adjust this to your actual project and dataset
)

SELECT
  *,
  LEAST(
    COALESCE(DATE_BLOCK_CODE_1_new, DATE_BLOCK_CODE_2_new),
    COALESCE(DATE_BLOCK_CODE_2_new, DATE_BLOCK_CODE_1_new)
  ) AS FIRST_BLOCK_DATE,
  CASE
    WHEN DATE_BLOCK_CODE_1_new IS NOT NULL THEN DATE_BLOCK_CODE_1_new
    ELSE DATE_BLOCK_CODE_2_new
  END AS PRIORITY_BLOCK_DATE,
  CASE
    WHEN REGEXP_REPLACE(BLOCK_CODE_1, r'\\s+', '') IN ('', 'A', 'B', 'C', 'D', 'E')
         AND REGEXP_REPLACE(BLOCK_CODE_2, r'\\s+', '') IN ('', 'A', 'B', 'C', 'D', 'E')
         AND REGEXP_REPLACE(int_Status, r'\\s+', '') IN ('A', 'D') THEN 1
    ELSE 0
  END AS aif_ind1,
  CASE
    WHEN int_status NOT IN ('f', 'g', 'h', 'u')
         AND (REGEXP_REPLACE(BLOCK_CODE_1, r'\\s+', '') IN ('A', 'Q', 'H') OR CURR_BAL / 100 > 0)
         AND CHGOFF_STATUS NOT IN ('5', '6') THEN 1
    ELSE 0
  END AS aif_risk1,
  CASE
    WHEN REGEXP_REPLACE(int_Status, r'\\s+', '') IN ('A', 'D') THEN 1
    ELSE 0
  END AS VAL_INT_STATUS
FROM
  Preprocessed;







CREATE OR REPLACE TEMP TABLE AMBS_BASE2 AS
SELECT 
  *,
  CASE
    WHEN LOGO = 420 THEN "CASHBACK"
    WHEN LOGO IN (410, 510, 501) THEN "GOLD"
    WHEN LOGO = 401 AND DATE_OPENED >= '2018-08-01' THEN "SVC"
    WHEN LOGO = 401 AND DATE_OPENED < '2018-08-01' THEN "GOLD"
    WHEN LOGO IN (415, 418) THEN "PLATINUM"
    WHEN LOGO = 515 THEN "PREMIER"
    ELSE "OTHER"
  END AS CARD_TYPE1
FROM AMBS_BASE1
WHERE aif_ind1 = 1;








CREATE OR REPLACE TEMP TABLE AMBS_BASE1 AS
SELECT
  *,
  -- Handle the conversion of BLOCK_CODE_X to dates; assume they're already in a compatible format or need conversion.
  CASE
    WHEN TRIM(BLOCK_CODE_1) = '' THEN NULL
    ELSE DATE_BLOCK_CODE_1
  END AS DATE_BLOCK_CODE_1_new,
  CASE
    WHEN TRIM(BLOCK_CODE_2) = '' THEN NULL
    ELSE DATE_BLOCK_CODE_2
  END AS DATE_BLOCK_CODE_2_new,
  -- Compute the FIRST_BLOCK_DATE as the minimum of the new date fields.
  LEAST(
    COALESCE(DATE_BLOCK_CODE_1_new, DATE_BLOCK_CODE_2_new),
    COALESCE(DATE_BLOCK_CODE_2_new, DATE_BLOCK_CODE_1_new)
  ) AS FIRST_BLOCK_DATE,
  -- Set PRIORITY_BLOCK_DATE based on the non-null condition of DATE_BLOCK_CODE_1_new.
  CASE
    WHEN DATE_BLOCK_CODE_1_new IS NOT NULL THEN DATE_BLOCK_CODE_1_new
    ELSE DATE_BLOCK_CODE_2_new
  END AS PRIORITY_BLOCK_DATE,
  -- Conditional logic for aif_ind1, considering compression (i.e., removing spaces and comparing).
  CASE
    WHEN REGEXP_REPLACE(BLOCK_CODE_1, r'\\s+', '') IN ('', 'A', 'B', 'C', 'D', 'E')
         AND REGEXP_REPLACE(BLOCK_CODE_2, r'\\s+', '') IN ('', 'A', 'B', 'C', 'D', 'E')
         AND REGEXP_REPLACE(int_Status, r'\\s+', '') IN ('A', 'D') THEN 1
    ELSE 0
  END AS aif_ind1,
  -- Logic for aif_risk1 with similar use of REGEXP_REPLACE to simulate SAS's compress function.
  CASE
    WHEN int_status NOT IN ('f', 'g', 'h', 'u')
         AND (REGEXP_REPLACE(BLOCK_CODE_1, r'\\s+', '') IN ('A', 'Q', 'H') OR CURR_BAL / 100 > 0)
         AND CHGOFF_STATUS NOT IN ('5', '6') THEN 1
    ELSE 0
  END AS aif_risk1,
  -- Conditional logic for VAL_INT_STATUS.
  CASE
    WHEN REGEXP_REPLACE(int_Status, r'\\s+', '') IN ('A', 'D') THEN 1
    ELSE 0
  END AS VAL_INT_STATUS
FROM
  `project.dataset.AMBS_BASE`; -- Adjust this to your project and dataset names











CREATE TEMP TABLE Temp_AMBS_BASE AS
WITH AMBS_BASE AS (
  -- Your original dataset query here. This is just a placeholder.
  SELECT * FROM `your-dataset.your-table`
)

, PREPARED_DATA AS (
  SELECT
    *,
    CASE
      WHEN BLOCK_CODE_1 IS NOT NULL AND BLOCK_CODE_1 != '' THEN DATE_BLOCK_CODE_1
      ELSE NULL
    END AS DATE_BLOCK_CODE_1_new,
    CASE
      WHEN BLOCK_CODE_2 IS NOT NULL AND BLOCK_CODE_2 != '' THEN DATE_BLOCK_CODE_2
      ELSE NULL
    END AS DATE_BLOCK_CODE_2_new,
    LEAST(COALESCE(DATE_BLOCK_CODE_1_new, DATE_BLOCK_CODE_2_new), COALESCE(DATE_BLOCK_CODE_2_new, DATE_BLOCK_CODE_1_new)) AS FIRST_BLOCK_DATE,
    CASE
      WHEN DATE_BLOCK_CODE_1_new IS NOT NULL THEN DATE_BLOCK_CODE_1_new
      ELSE DATE_BLOCK_CODE_2_new
    END AS PRIORITY_BLOCK_DATE
  FROM
    AMBS_BASE
)

SELECT
  *,
  CASE
    WHEN REGEXP_REPLACE(BLOCK_CODE_2, r'\s+', '') IN ('', 'A', 'B', 'C', 'D', 'E')
         AND REGEXP_REPLACE(int_Status, r'\s+', '') IN ('A', 'D') THEN 1
    ELSE 0
  END AS aif_ind1,
  CASE
    WHEN int_status NOT IN ('f', 'g', 'h', 'u')
         AND (REGEXP_REPLACE(BLOCK_CODE_1, r'\s+', '') IN ('A', 'Q', 'H') OR CURR_BAL/100 > 0)
         AND CHGOFF_STATUS NOT IN ('5', '6') THEN 1
    ELSE 0
  END AS aif_risk1,
  CASE
    WHEN REGEXP_REPLACE(int_Status, r'\s+', '') IN ('A', 'D') THEN 1
    ELSE 0
  END AS VAL_INT_STATUS
FROM
  PREPARED_DATA;
