/* ===== Setup ===== */
options mprint mlogic symbolgen nocenter validvarname=any;
ods graphics on;

libname bank "/your/path/here";
filename outdir ".";

/* ===== Cohort & Transactions ===== */
proc sql;
  create table work.q1_cohort as
  select acct, date_opened
  from bank.premier_card_acq;
quit;

proc sql;
  create table work.q1_txn_raw as
  select c.acct, c.date_opened,
         t.tran_amt, t.mt_eff_date, t.mt_category_code, t.sector
  from work.q1_cohort c
  left join bank.card_tran3 t
    on c.acct = t.acct;
quit;

/* ===== Relative month windows ===== */
data work.q1_txn_rel;
  set work.q1_txn_raw;
  if not missing(mt_eff_date) then do;
    months_since_open = intck('month', date_opened, mt_eff_date, 'c');
    in_m1  = (months_since_open = 0);
    in_m3  = (0 <= months_since_open <= 2);
    in_m12 = (0 <= months_since_open <= 11);
  end;
  else do; in_m1=0; in_m3=0; in_m12=0; end;
run;

/* ===== Account-level spend ===== */
proc sql;
  create table work.acct_spend as
  select acct,
         sum(case when in_m1  then tran_amt else 0 end)  as M1_spend,
         sum(case when in_m3  then tran_amt else 0 end)  as M3_spend,
         sum(case when in_m12 then tran_amt else 0 end)  as M12_spend,
         sum(case when in_m12 and not missing(tran_amt) then 1 else 0 end) as txns_12m,
         sum(case when in_m1  and not missing(tran_amt) then 1 else 0 end) as txns_m1
  from work.q1_txn_rel
  group by acct;
quit;

proc sql;
  create table work.q1_spend_12m as
  select a.acct, a.date_opened,
         coalesce(b.M1_spend,0)  as M1_spend,
         coalesce(b.M3_spend,0)  as M3_spend,
         coalesce(b.M12_spend,0) as M12_spend,
         coalesce(b.txns_12m,0)  as txns_12m,
         coalesce(b.txns_m1,0)   as txns_m1
  from work.q1_cohort a
  left join work.acct_spend b on a.acct=b.acct;
quit;

/* ===== First behaviors (M1 earliest merchant + sector) ===== */
proc sort data=work.q1_txn_rel out=work.q1_txn_sorted;
  by acct mt_eff_date;
run;

/* First ever txn (overall) */
data work.first_txn_overall;
  set work.q1_txn_sorted;
  by acct;
  if first.acct then do;
    first_txn_date = mt_eff_date;
    first_tran_amt = tran_amt;
    length first_category $50; first_category = vvalue(mt_category_code);
    output;
  end;
  keep acct first_txn_date first_tran_amt first_category;
run;

/* First-month earliest merchant + sector */
data work.first_M1_both;
  set work.q1_txn_sorted;
  by acct mt_eff_date;
  retain flag 0;
  if first.acct then flag=0;
  if months_since_open=0 and flag=0 then do;
    length first_merchant $50 first_sector $50;
    first_merchant = vvalue(mt_category_code);
    first_sector   = coalescec(sector,'Other');
    first_month_amt  = tran_amt;
    first_month_date = mt_eff_date;
    flag=1; output;
  end;
  keep acct first_merchant first_sector first_month_amt first_month_date;
run;

/* Attach first behavior columns */
proc sql;
  create table work.q1_spend_12m as
  select a.*, f.first_txn_date, f.first_tran_amt, f.first_category,
         m.first_merchant, m.first_sector, m.first_month_amt, m.first_month_date
  from work.q1_spend_12m a
  left join work.first_txn_overall f on a.acct=f.acct
  left join work.first_M1_both   m on a.acct=m.acct;
quit;

/* ===== M1 quintile buckets & lift ===== */
proc rank data=work.q1_spend_12m groups=5 out=work.q1_spend_buckets;
  var M1_spend;
  ranks M1_q;
run;

data work.q1_spend_buckets;
  set work.q1_spend_buckets;
  length M1_bucket $20;
  select (M1_q);
    when (0) M1_bucket='Q1 (Lowest)';
    when (1) M1_bucket='Q2';
    when (2) M1_bucket='Q3';
    when (3) M1_bucket='Q4';
    when (4) M1_bucket='Q5 (Highest)';
    otherwise M1_bucket='Unknown';
  end;
run;

proc sql;
  create table work.lift_m1_bucket as
  select M1_bucket, count(*) as customers,
         mean(M1_spend)  as avg_M1_spend,
         mean(M12_spend) as avg_M12_spend
  from work.q1_spend_buckets
  group by M1_bucket
  order by customers desc;
quit;

/* ===== Dynamic Good flags (P70) ===== */
proc means data=work.q1_spend_12m noprint p70;
  var M3_spend M12_spend;
  output out=work.good_cutoffs p70(M3_spend M12_spend)=M3_p70 M12_p70;
run;

data work.q1_spend_classified;
  if _n_=1 then set work.good_cutoffs(keep=M3_p70 M12_p70);
  set work.q1_spend_12m;
  Good_M3  = (M3_spend  >= M3_p70);
  Good_M12 = (M12_spend >= M12_p70);
run;

/* ===== Correlations ===== */
proc corr data=work.q1_spend_classified plots=matrix(histogram);
  var M1_spend M3_spend M12_spend txns_m1 txns_12m;
run;

/* ===== Sector-level summaries ===== */
proc sql;
  create table work.sector_summary as
  select coalesce(first_sector,'Other') as first_sector length=40,
         count(*) as customers,
         mean(M12_spend) as avg_M12_spend,
         mean(Good_M12)  as pct_good12,
         mean(M3_spend)  as avg_M3_spend
  from work.q1_spend_classified
  group by calculated first_sector
  order by customers desc;
quit;

/* ===== Merchant-level summaries ===== */
proc sql;
  create table work.merchant_summary as
  select first_merchant,
         count(*) as customers,
         mean(M12_spend) as avg_M12_spend,
         mean(Good_M12)  as pct_good12,
         mean(M3_spend)  as avg_M3_spend
  from work.q1_spend_classified
  group by first_merchant
  having calculated customers >= 30 /* ignore tiny cells */
  order by customers desc, avg_M12_spend desc;
quit;

/* ===== Build modeling dataset with safe categories ===== */
proc sort data=work.q1_spend_classified out=work._base; by acct; run;

/* Sector safe var */
data work._sector;
  set work._base(keep=acct first_sector);
  length sector_s $40;
  sector_s = coalescec(first_sector,'(No M1 Txn)');
run;

/* Merchant: collapse to Top-50 for modeling */
proc sql;
  create table work._merch_rank as
  select first_merchant, count(*) as customers
  from work._base
  group by first_merchant
  order by customers desc;
quit;

proc sql noprint;
  select first_merchant into :topmerch1-:topmerch50
  from work._merch_rank(obs=50);
quit;

data work._merchant;
  set work._base(keep=acct first_merchant);
  length merchant_s $60;
  if first_merchant in
    ("&topmerch1","&topmerch2","&topmerch3","&topmerch4","&topmerch5",
     "&topmerch6","&topmerch7","&topmerch8","&topmerch9","&topmerch10",
     "&topmerch11","&topmerch12","&topmerch13","&topmerch14","&topmerch15",
     "&topmerch16","&topmerch17","&topmerch18","&topmerch19","&topmerch20",
     "&topmerch21","&topmerch22","&topmerch23","&topmerch24","&topmerch25",
     "&topmerch26","&topmerch27","&topmerch28","&topmerch29","&topmerch30",
     "&topmerch31","&topmerch32","&topmerch33","&topmerch34","&topmerch35",
     "&topmerch36","&topmerch37","&topmerch38","&topmerch39","&topmerch40",
     "&topmerch41","&topmerch42","&topmerch43","&topmerch44","&topmerch45",
     "&topmerch46","&topmerch47","&topmerch48","&topmerch49","&topmerch50")
  then merchant_s = first_merchant;
  else merchant_s = '(Other/No M1)';
run;

/* Final modeling table */
proc sql;
  create table work.model_in as
  select b.*, s.sector_s, m.merchant_s
  from work._base b
  left join work._sector   s on b.acct=s.acct
  left join work._merchant m on b.acct=m.acct;
quit;

/* ===== Models (Sector) ===== */
%let refcat = %str((No M1 Txn));

proc glm data=work.model_in plots=none;
  class sector_s (ref="&refcat") / param=ref;
  model M12_spend = M1_spend first_tran_amt txns_m1 sector_s;
run; quit;

proc logistic data=work.model_in plots=roc order=internal;
  class sector_s (ref="&refcat") / param=ref;
  model Good_M12(event='1') = M1_spend first_tran_amt txns_m1 sector_s;
run;

/* ===== Models (Merchant Top-50) ===== */
%let refmer = %str((Other/No M1));

proc glm data=work.model_in plots=none;
  class merchant_s (ref="&refmer") / param=ref;
  model M12_spend = M1_spend first_tran_amt txns_m1 merchant_s;
run; quit;

proc logistic data=work.model_in plots=roc order=internal;
  class merchant_s (ref="&refmer") / param=ref;
  model Good_M12(event='1') = M1_spend first_tran_amt txns_m1 merchant_s;
run;

/* ===== Retention (by sector) ===== */
proc sql;
  create table work.acct_month as
  select acct, months_since_open as rel_month,
         sum(tran_amt) as month_spend,
         sum(case when not missing(tran_amt) then 1 else 0 end) as month_txns
  from work.q1_txn_rel
  where 0 <= months_since_open <= 11
  group by acct, months_since_open;
quit;

data work.grid; set work.q1_cohort; do rel_month=0 to 11; output; end; run;

proc sql;
  create table work.acct_month_full as
  select g.acct, g.rel_month,
         coalesce(m.month_spend,0) as month_spend,
         coalesce(m.month_txns,0)  as month_txns
  from work.grid g
  left join work.acct_month m
    on g.acct=m.acct and g.rel_month=m.rel_month;
quit;

data work.acct_month_full; set work.acct_month_full; active=(month_txns>0); run;

proc summary data=work.acct_month_full nway;
  class rel_month;
  var active month_spend;
  output out=work.retention_overall mean=;
run;

proc sql;
  create table work.acct_month_join as
  select a.acct, coalesce(b.first_sector,'(No M1 Txn)') as sector_s length=40,
         a.rel_month, a.month_spend, a.month_txns, a.active
  from work.acct_month_full a
  left join work.q1_spend_12m b on a.acct=b.acct;
quit;

proc summary data=work.acct_month_join nway;
  class sector_s rel_month;
  var active month_spend;
  output out=work.retention_by_sector mean=;
run;

/* ===== Executive summaries ===== */
proc sql;
  create table work.good_summary as
  select "M3" as metric, count(*) as total_cust, sum(Good_M3) as good_cust,
         mean(M3_spend) as avg_all,
         mean(case when Good_M3=1 then M3_spend end) as avg_good,
         mean(case when Good_M3=0 then M3_spend end) as avg_rest,
         calculated avg_good / calculated avg_rest as lift
  from work.q1_spend_classified
  union all
  select "M12", count(*), sum(Good_M12),
         mean(M12_spend),
         mean(case when Good_M12=1 then M12_spend end),
         mean(case when Good_M12=0 then M12_spend end),
         calculated avg_good / calculated avg_rest
  from work.q1_spend_classified;
quit;

/* ===== Exports ===== */
%macro expo(ds, fn);
proc export data=&ds outfile="%sysfunc(pathname(outdir))/&fn..csv" dbms=csv replace; run;
%mend;

%expo(work.q1_spend_12m,                q1_spend_12m_by_acct);
%expo(work.lift_m1_bucket,              lift_by_m1_bucket);
%expo(work.good_cutoffs,                good_spend_cutoffs);
%expo(work.good_summary,                good_spend_summary);
%expo(work.sector_summary,              first_sector_summary);
%expo(work.merchant_summary,            first_merchant_summary);
%expo(work.retention_overall,           retention_overall);
%expo(work.retention_by_sector,         retention_by_sector);
%expo(work.model_in,                    model_input_snapshot);

ods graphics off;






















gehejwiwkwkwkwkkwwkkwkwmwne


/* ===== Setup ===== */
options mprint mlogic symbolgen nocenter validvarname=any;
ods graphics on;

libname bank "/your/path/here";
filename outdir ".";

/* ===== Cohort & Transactions ===== */
proc sql;
  create table work.q1_cohort as
  select acct, date_opened
  from bank.premier_card_acq;
quit;

proc sql;
  create table work.q1_txn_raw as
  select c.acct, c.date_opened,
         t.tran_amt, t.mt_eff_date, t.mt_category_code, t.sector
  from work.q1_cohort c
  left join bank.card_tran3 t
    on c.acct = t.acct;
quit;

/* ===== Relative month windows ===== */
data work.q1_txn_rel;
  set work.q1_txn_raw;
  if not missing(mt_eff_date) then do;
    months_since_open = intck('month', date_opened, mt_eff_date, 'c');
    in_m1  = (months_since_open = 0);
    in_m3  = (0 <= months_since_open <= 2);
    in_m12 = (0 <= months_since_open <= 11);
  end;
  else do; in_m1=0; in_m3=0; in_m12=0; end;
run;

/* ===== Account-level spend ===== */
proc sql;
  create table work.acct_spend as
  select acct,
         sum(case when in_m1  then tran_amt else 0 end)  as M1_spend,
         sum(case when in_m3  then tran_amt else 0 end)  as M3_spend,
         sum(case when in_m12 then tran_amt else 0 end)  as M12_spend,
         sum(case when in_m12 and not missing(tran_amt) then 1 else 0 end) as txns_12m,
         sum(case when in_m1  and not missing(tran_amt) then 1 else 0 end) as txns_m1
  from work.q1_txn_rel
  group by acct;
quit;

proc sql;
  create table work.q1_spend_12m as
  select a.acct, a.date_opened,
         coalesce(b.M1_spend,0)  as M1_spend,
         coalesce(b.M3_spend,0)  as M3_spend,
         coalesce(b.M12_spend,0) as M12_spend,
         coalesce(b.txns_12m,0)  as txns_12m,
         coalesce(b.txns_m1,0)   as txns_m1
  from work.q1_cohort a
  left join work.acct_spend b on a.acct=b.acct;
quit;

/* ===== First behaviors (M1 earliest merchant + sector) ===== */
proc sort data=work.q1_txn_rel out=work.q1_txn_sorted;
  by acct mt_eff_date;
run;

/* First ever txn (overall) */
data work.first_txn_overall;
  set work.q1_txn_sorted;
  by acct;
  if first.acct then do;
    first_txn_date = mt_eff_date;
    first_tran_amt = tran_amt;
    length first_category $50; first_category = vvalue(mt_category_code);
    output;
  end;
  keep acct first_txn_date first_tran_amt first_category;
run;

/* First-month earliest merchant + sector */
data work.first_M1_both;
  set work.q1_txn_sorted;
  by acct mt_eff_date;
  retain flag 0;
  if first.acct then flag=0;
  if months_since_open=0 and flag=0 then do;
    length first_merchant $50 first_sector $50;
    first_merchant = vvalue(mt_category_code);
    first_sector   = coalescec(sector,'Other');
    first_month_amt  = tran_amt;
    first_month_date = mt_eff_date;
    flag=1; output;
  end;
  keep acct first_merchant first_sector first_month_amt first_month_date;
run;

/* Attach first behavior columns */
proc sql;
  create table work.q1_spend_12m as
  select a.*, f.first_txn_date, f.first_tran_amt, f.first_category,
         m.first_merchant, m.first_sector, m.first_month_amt, m.first_month_date
  from work.q1_spend_12m a
  left join work.first_txn_overall f on a.acct=f.acct
  left join work.first_M1_both   m on a.acct=m.acct;
quit;

/* ===== M1 quintile buckets & lift ===== */
proc rank data=work.q1_spend_12m groups=5 out=work.q1_spend_buckets;
  var M1_spend;
  ranks M1_q;
run;

data work.q1_spend_buckets;
  set work.q1_spend_buckets;
  length M1_bucket $20;
  select (M1_q);
    when (0) M1_bucket='Q1 (Lowest)';
    when (1) M1_bucket='Q2';
    when (2) M1_bucket='Q3';
    when (3) M1_bucket='Q4';
    when (4) M1_bucket='Q5 (Highest)';
    otherwise M1_bucket='Unknown';
  end;
run;

proc sql;
  create table work.lift_m1_bucket as
  select M1_bucket, count(*) as customers,
         mean(M1_spend)  as avg_M1_spend,
         mean(M12_spend) as avg_M12_spend
  from work.q1_spend_buckets
  group by M1_bucket
  order by customers desc;
quit;

/* ===== Dynamic Good flags (P70) ===== */
proc means data=work.q1_spend_12m noprint p70;
  var M3_spend M12_spend;
  output out=work.good_cutoffs p70(M3_spend M12_spend)=M3_p70 M12_p70;
run;

data work.q1_spend_classified;
  if _n_=1 then set work.good_cutoffs(keep=M3_p70 M12_p70);
  set work.q1_spend_12m;
  Good_M3  = (M3_spend  >= M3_p70);
  Good_M12 = (M12_spend >= M12_p70);
run;

/* ===== Correlations ===== */
proc corr data=work.q1_spend_classified plots=matrix(histogram);
  var M1_spend M3_spend M12_spend txns_m1 txns_12m;
run;

/* ===== Sector-level summaries ===== */
proc sql;
  create table work.sector_summary as
  select coalesce(first_sector,'Other') as first_sector length=40,
         count(*) as customers,
         mean(M12_spend) as avg_M12_spend,
         mean(Good_M12)  as pct_good12,
         mean(M3_spend)  as avg_M3_spend
  from work.q1_spend_classified
  group by calculated first_sector
  order by customers desc;
quit;

/* ===== Merchant-level summaries ===== */
proc sql;
  create table work.merchant_summary as
  select first_merchant,
         count(*) as customers,
         mean(M12_spend) as avg_M12_spend,
         mean(Good_M12)  as pct_good12,
         mean(M3_spend)  as avg_M3_spend
  from work.q1_spend_classified
  group by first_merchant
  having calculated customers >= 30 /* ignore tiny cells */
  order by customers desc, avg_M12_spend desc;
quit;

/* ===== Build modeling dataset with safe categories ===== */
proc sort data=work.q1_spend_classified out=work._base; by acct; run;

/* Sector safe var */
data work._sector;
  set work._base(keep=acct first_sector);
  length sector_s $40;
  sector_s = coalescec(first_sector,'(No M1 Txn)');
run;

/* Merchant: collapse to Top-50 for modeling */
proc sql;
  create table work._merch_rank as
  select first_merchant, count(*) as customers
  from work._base
  group by first_merchant
  order by customers desc;
quit;

proc sql noprint;
  select first_merchant into :topmerch1-:topmerch50
  from work._merch_rank(obs=50);
quit;

data work._merchant;
  set work._base(keep=acct first_merchant);
  length merchant_s $60;
  if first_merchant in
    ("&topmerch1","&topmerch2","&topmerch3","&topmerch4","&topmerch5",
     "&topmerch6","&topmerch7","&topmerch8","&topmerch9","&topmerch10",
     "&topmerch11","&topmerch12","&topmerch13","&topmerch14","&topmerch15",
     "&topmerch16","&topmerch17","&topmerch18","&topmerch19","&topmerch20",
     "&topmerch21","&topmerch22","&topmerch23","&topmerch24","&topmerch25",
     "&topmerch26","&topmerch27","&topmerch28","&topmerch29","&topmerch30",
     "&topmerch31","&topmerch32","&topmerch33","&topmerch34","&topmerch35",
     "&topmerch36","&topmerch37","&topmerch38","&topmerch39","&topmerch40",
     "&topmerch41","&topmerch42","&topmerch43","&topmerch44","&topmerch45",
     "&topmerch46","&topmerch47","&topmerch48","&topmerch49","&topmerch50")
  then merchant_s = first_merchant;
  else merchant_s = '(Other/No M1)';
run;

/* Final modeling table */
proc sql;
  create table work.model_in as
  select b.*, s.sector_s, m.merchant_s
  from work._base b
  left join work._sector   s on b.acct=s.acct
  left join work._merchant m on b.acct=m.acct;
quit;

/* ===== Models (Sector) ===== */
%let refcat = %str((No M1 Txn));

proc glm data=work.model_in plots=none;
  class sector_s (ref="&refcat") / param=ref;
  model M12_spend = M1_spend first_tran_amt txns_m1 sector_s;
run; quit;

proc logistic data=work.model_in plots=roc order=internal;
  class sector_s (ref="&refcat") / param=ref;
  model Good_M12(event='1') = M1_spend first_tran_amt txns_m1 sector_s;
run;

/* ===== Models (Merchant Top-50) ===== */
%let refmer = %str((Other/No M1));

proc glm data=work.model_in plots=none;
  class merchant_s (ref="&refmer") / param=ref;
  model M12_spend = M1_spend first_tran_amt txns_m1 merchant_s;
run; quit;

proc logistic data=work.model_in plots=roc order=internal;
  class merchant_s (ref="&refmer") / param=ref;
  model Good_M12(event='1') = M1_spend first_tran_amt txns_m1 merchant_s;
run;

/* ===== Retention (by sector) ===== */
proc sql;
  create table work.acct_month as
  select acct, months_since_open as rel_month,
         sum(tran_amt) as month_spend,
         sum(case when not missing(tran_amt) then 1 else 0 end) as month_txns
  from work.q1_txn_rel
  where 0 <= months_since_open <= 11
  group by acct, months_since_open;
quit;

data work.grid; set work.q1_cohort; do rel_month=0 to 11; output; end; run;

proc sql;
  create table work.acct_month_full as
  select g.acct, g.rel_month,
         coalesce(m.month_spend,0) as month_spend,
         coalesce(m.month_txns,0)  as month_txns
  from work.grid g
  left join work.acct_month m
    on g.acct=m.acct and g.rel_month=m.rel_month;
quit;

data work.acct_month_full; set work.acct_month_full; active=(month_txns>0); run;

proc summary data=work.acct_month_full nway;
  class rel_month;
  var active month_spend;
  output out=work.retention_overall mean=;
run;

proc sql;
  create table work.acct_month_join as
  select a.acct, coalesce(b.first_sector,'(No M1 Txn)') as sector_s length=40,
         a.rel_month, a.month_spend, a.month_txns, a.active
  from work.acct_month_full a
  left join work.q1_spend_12m b on a.acct=b.acct;
quit;

proc summary data=work.acct_month_join nway;
  class sector_s rel_month;
  var active month_spend;
  output out=work.retention_by_sector mean=;
run;

/* ===== Executive summaries ===== */
proc sql;
  create table work.good_summary as
  select "M3" as metric, count(*) as total_cust, sum(Good_M3) as good_cust,
         mean(M3_spend) as avg_all,
         mean(case when Good_M3=1 then M3_spend end) as avg_good,
         mean(case when Good_M3=0 then M3_spend end) as avg_rest,
         calculated avg_good / calculated avg_rest as lift
  from work.q1_spend_classified
  union all
  select "M12", count(*), sum(Good_M12),
         mean(M12_spend),
         mean(case when Good_M12=1 then M12_spend end),
         mean(case when Good_M12=0 then M12_spend end),
         calculated avg_good / calculated avg_rest
  from work.q1_spend_classified;
quit;

/* ===== Exports ===== */
%macro expo(ds, fn);
proc export data=&ds outfile="%sysfunc(pathname(outdir))/&fn..csv" dbms=csv replace; run;
%mend;

%expo(work.q1_spend_12m,                q1_spend_12m_by_acct);
%expo(work.lift_m1_bucket,              lift_by_m1_bucket);
%expo(work.good_cutoffs,                good_spend_cutoffs);
%expo(work.good_summary,                good_spend_summary);
%expo(work.sector_summary,              first_sector_summary);
%expo(work.merchant_summary,            first_merchant_summary);
%expo(work.retention_overall,           retention_overall);
%expo(work.retention_by_sector,         retention_by_sector);
%expo(work.model_in,                    model_input_snapshot);

ods graphics off;























yfuccicig



/* ===== Setup ===== */
options mprint mlogic symbolgen nocenter validvarname=any;
ods graphics on;

libname bank "/your/path/here";
filename outdir ".";

/* ===== Cohort & Transactions ===== */
proc sql;
  create table work.q1_cohort as
  select acct, date_opened
  from bank.premier_card_acq;
quit;

proc sql;
  create table work.q1_txn_raw as
  select c.acct, c.date_opened, t.tran_amt, t.mt_eff_date, t.mt_category_code
  from work.q1_cohort c
  left join bank.card_tran3 t
    on c.acct = t.acct;
quit;

/* ===== Relative month windows ===== */
data work.q1_txn_rel;
  set work.q1_txn_raw;
  if not missing(mt_eff_date) then do;
    months_since_open = intck('month', date_opened, mt_eff_date, 'c');
    in_m1  = (months_since_open = 0);
    in_m3  = (0 <= months_since_open <= 2);
    in_m12 = (0 <= months_since_open <= 11);
  end;
  else do; in_m1=0; in_m3=0; in_m12=0; end;
run;

/* ===== Account-level spend ===== */
proc sql;
  create table work.acct_spend as
  select acct,
         sum(case when in_m1  then tran_amt else 0 end) as M1_spend,
         sum(case when in_m3  then tran_amt else 0 end) as M3_spend,
         sum(case when in_m12 then tran_amt else 0 end) as M12_spend,
         sum(case when in_m12 and not missing(tran_amt) then 1 else 0 end) as txns_12m,
         sum(case when in_m1  and not missing(tran_amt) then 1 else 0 end) as txns_m1
  from work.q1_txn_rel
  group by acct;
quit;

proc sql;
  create table work.acct_spend_full as
  select a.acct, a.date_opened,
         coalesce(b.M1_spend,0)  as M1_spend,
         coalesce(b.M3_spend,0)  as M3_spend,
         coalesce(b.M12_spend,0) as M12_spend,
         coalesce(b.txns_12m,0)  as txns_12m,
         coalesce(b.txns_m1,0)   as txns_m1
  from work.q1_cohort a
  left join work.acct_spend b on a.acct=b.acct;
quit;

/* ===== First behaviors ===== */
proc sort data=work.q1_txn_rel out=work.q1_txn_sorted;
  by acct mt_eff_date;
run;

/* First ever txn */
data work.first_txn_overall;
  set work.q1_txn_sorted;
  by acct;
  if first.acct then do;
    first_txn_date = mt_eff_date;
    first_tran_amt = tran_amt;
    length first_category $50; first_category = vvalue(mt_category_code);
    output;
  end;
  keep acct first_txn_date first_tran_amt first_category;
run;

/* First-month (M1) earliest txn */
data work.first_month_txn;
  set work.q1_txn_sorted;
  by acct mt_eff_date;
  retain flag 0;
  if first.acct then flag=0;
  if months_since_open=0 and flag=0 then do;
    first_month_cat  = vvalue(mt_category_code);
    first_month_amt  = tran_amt;
    first_month_date = mt_eff_date;
    flag=1; output;
  end;
  keep acct first_month_cat first_month_amt first_month_date;
run;

proc sql;
  create table work.q1_spend_12m as
  select a.*, f.first_txn_date, f.first_tran_amt, f.first_category,
         m.first_month_cat, m.first_month_amt, m.first_month_date
  from work.acct_spend_full a
  left join work.first_txn_overall f on a.acct=f.acct
  left join work.first_month_txn  m on a.acct=m.acct;
quit;

/* ===== M1 buckets (quintiles) & lifts ===== */
proc rank data=work.q1_spend_12m groups=5 out=work.q1_spend_buckets;
  var M1_spend;
  ranks M1_q;
run;

data work.q1_spend_buckets;
  set work.q1_spend_buckets;
  length M1_bucket $20;
  select (M1_q);
    when (0) M1_bucket='Q1 (Lowest)';
    when (1) M1_bucket='Q2';
    when (2) M1_bucket='Q3';
    when (3) M1_bucket='Q4';
    when (4) M1_bucket='Q5 (Highest)';
    otherwise M1_bucket='Unknown';
  end;
  M1_cat = coalesce(first_month_cat,'(No M1 Txn)');
run;

proc sql;
  create table work.lift_m1_bucket as
  select M1_bucket, count(*) as customers,
         mean(M1_spend)  as avg_M1_spend,
         mean(M12_spend) as avg_M12_spend
  from work.q1_spend_buckets
  group by M1_bucket
  order by customers desc;
quit;

/* ===== Dynamic Good spend flags (P70) ===== */
proc univariate data=work.q1_spend_12m noprint;
  var M3_spend M12_spend;
  output out=work.good_cutoffs p70=M3_p70 M12_p70;
run;

data work.q1_spend_classified;
  if _n_=1 then set work.good_cutoffs;
  set work.q1_spend_12m;
  Good_M3  = (M3_spend  >= M3_p70);
  Good_M12 = (M12_spend >= M12_p70);
run;

/* Quick sanity */
proc freq data=work.q1_spend_classified;
  tables Good_M3 Good_M12;
run;

/* ===== Correlations & models ===== */
proc corr data=work.q1_spend_classified plots=matrix(histogram);
  var M1_spend M3_spend M12_spend txns_m1 txns_12m;
run;

proc glmselect data=work.q1_spend_buckets plots=all;
  class M1_cat / param=ref ref='(No M1 Txn)';
  model M12_spend = M1_spend first_tran_amt txns_m1 M1_cat / selection=none;
run; quit;

proc logistic data=work.q1_spend_classified plots=roc;
  class first_month_cat / param=ref ref='(No M1 Txn)';
  model Good_M12(event='1') = M1_spend first_tran_amt txns_m1 first_month_cat;
run;

/* ===== Deciles & contribution ===== */
proc rank data=work.q1_spend_classified groups=10 out=work.deciles;
  var M12_spend;
  ranks dec_M12;
run;

proc summary data=work.deciles nway;
  class dec_M12;
  var M1_spend M3_spend M12_spend txns_m1;
  output out=work.dec_profile mean= / autoname n=customers;
run;

proc sql;
  create table work.dec_contrib as
  select dec_M12, customers, M12_spend_Mean,
         (customers*M12_spend_Mean) as decile_total
  from work.dec_profile
  order by dec_M12 desc;
quit;

/* ===== Retention tables ===== */
proc sql;
  create table work.acct_month as
  select acct, months_since_open as rel_month,
         sum(tran_amt) as month_spend,
         sum(case when not missing(tran_amt) then 1 else 0 end) as month_txns
  from work.q1_txn_rel
  where 0 <= months_since_open <= 11
  group by acct, months_since_open;
quit;

data work.grid;
  set work.q1_cohort;
  do rel_month=0 to 11; output; end;
run;

proc sql;
  create table work.acct_month_full as
  select g.acct, g.rel_month,
         coalesce(m.month_spend,0) as month_spend,
         coalesce(m.month_txns,0)  as month_txns
  from work.grid g
  left join work.acct_month m
    on g.acct=m.acct and g.rel_month=m.rel_month;
quit;

data work.acct_month_full;
  set work.acct_month_full;
  active = (month_txns>0);
run;

proc summary data=work.acct_month_full nway;
  class rel_month;
  var active month_spend;
  output out=work.retention_tbl mean=;
run;

proc sql;
  create table work.acct_month_join as
  select a.acct, b.M1_cat, a.rel_month, a.month_spend, a.month_txns, a.active
  from work.acct_month_full a
  left join work.q1_spend_buckets b on a.acct=b.acct;
quit;

proc summary data=work.acct_month_join nway;
  class M1_cat rel_month;
  var active month_spend;
  output out=work.retention_by_cat mean=;
run;

/* ===== Executive summaries ===== */
proc sql;
  create table work.good_summary as
  select "M3" as metric,
         count(*) as total_cust,
         sum(Good_M3) as good_cust,
         mean(M3_spend) as avg_all,
         mean(case when Good_M3=1 then M3_spend end) as avg_good,
         mean(case when Good_M3=0 then M3_spend end) as avg_rest,
         calculated avg_good / calculated avg_rest as lift
  from work.q1_spend_classified
  union all
  select "M12" as metric,
         count(*) as total_cust,
         sum(Good_M12) as good_cust,
         mean(M12_spend) as avg_all,
         mean(case when Good_M12=1 then M12_spend end) as avg_good,
         mean(case when Good_M12=0 then M12_spend end) as avg_rest,
         calculated avg_good / calculated avg_rest as lift
  from work.q1_spend_classified;
quit;

proc sql;
  create table work.good_summary_cat as
  select coalesce(first_month_cat,'(No M1 Txn)') as first_month_cat length=50,
         count(*) as total_cust,
         sum(Good_M3)  as good3_cust,
         mean(M3_spend) as avg3_all,
         mean(case when Good_M3=1 then M3_spend end) as avg3_good,
         mean(case when Good_M3=0 then M3_spend end) as avg3_rest,
         calculated avg3_good / calculated avg3_rest as lift3,
         sum(Good_M12) as good12_cust,
         mean(M12_spend) as avg12_all,
         mean(case when Good_M12=1 then M12_spend end) as avg12_good,
         mean(case when Good_M12=0 then M12_spend end) as avg12_rest,
         calculated avg12_good / calculated avg12_rest as lift12
  from work.q1_spend_classified
  group by calculated first_month_cat
  order by total_cust desc;
quit;

/* ===== Exports ===== */
%macro expo(ds, fn);
proc export data=&ds outfile="%sysfunc(pathname(outdir))/&fn..csv" dbms=csv replace; run;
%mend;

%expo(work.q1_spend_12m,                q1_spend_12m_by_acct);
%expo(work.lift_m1_bucket,              lift_by_m1_bucket);
%expo(work.dec_profile,                 decile_profile);
%expo(work.dec_contrib,                 decile_contribution);
%expo(work.retention_tbl,               retention_overall);
%expo(work.retention_by_cat,            retention_by_first_month_category);
%expo(work.good_summary,                good_spend_summary);
%expo(work.good_summary_cat,            good_spend_summary_by_category);

ods graphics off;







































/*==========================================================
  0) CONFIG & LIBS
==========================================================*/
options mprint mlogic symbolgen nocenter validvarname=any;
options dtreset formchar="|----|+|---+=|-/\<>*";
ods graphics on;

libname bank "/your/path/here";  /* TODO: set your path */
filename outdir ".";             /* exports go to current directory */

/* Cohort and transaction windows */
%let cohort_start = '01JAN2024'd;
%let cohort_end   = '31MAR2024'd;
%let txn_start    = '01JAN2024'd;
%let txn_end      = '31MAR2025'd;

/* First-month spend buckets (editable) */
%let m1_low_hi   = 1000;    /* <= this = Low */
%let m1_mid_hi   = 5000;    /* <= this = Mid */
%let m1_high_hi  = 15000;   /* <= this = UpperMid; > = High */

/* "Good Spend" method:
   - PCTL = percentile-based (uses P70 by default)
   - ABS  = absolute business thresholds (set below)              */
%let GOOD_METHOD = PCTL;    /* PCTL | ABS */
%let PCTL_LEVEL  = 70;      /* percentile cutoff, e.g., 70 = top 30% */
%let GOOD_M3_ABS  = 15000;  /* used only if GOOD_METHOD=ABS */
%let GOOD_M12_ABS = 50000;  /* used only if GOOD_METHOD=ABS */

/* High spender flag (for logistic model) */
%let high12_cut = 10000;

/*==========================================================
  1) BUILD Q1-2024 COHORT
==========================================================*/
proc sql;
  create table work.q1_cohort as
  select acct, date_opened
  from bank.premier_card_acq
  where &cohort_start. <= date_opened <= &cohort_end.;
quit;

/*==========================================================
  2) ATTACH TRANSACTIONS (Jan-2024..Mar-2025)
==========================================================*/
proc sql;
  create table work.q1_txn_raw as
  select c.acct,
         c.date_opened,
         t.tran_amt,
         t.mt_eff_date,
         t.mt_category_code
  from work.q1_cohort c
  left join bank.card_tran3 t
    on c.acct = t.acct
   and &txn_start. <= t.mt_eff_date <= &txn_end.;
quit;

/*==========================================================
  3) RELATIVE MONTHS SINCE OPEN (M1/M3/M12 flags)
==========================================================*/
data work.q1_txn_rel;
  set work.q1_txn_raw;
  if not missing(mt_eff_date) then do;
    months_since_open = intck('month', date_opened, mt_eff_date, 'c'); /* 0..11 */
    in_m1  = (months_since_open = 0);
    in_m3  = (0 <= months_since_open <= 2);
    in_m12 = (0 <= months_since_open <= 11);
  end;
  else do;
    months_since_open = .;
    in_m1 = 0; in_m3 = 0; in_m12 = 0;
  end;
run;

/*==========================================================
  4) ACCT-LEVEL SUMMARIES (M1/M3/M12) + COUNTS
==========================================================*/
proc sql;
  create table work.acct_spend as
  select acct,
         sum(case when in_m1  then tran_amt else 0 end)  as M1_spend  format=comma16.2,
         sum(case when in_m3  then tran_amt else 0 end)  as M3_spend  format=comma16.2,
         sum(case when in_m12 then tran_amt else 0 end)  as M12_spend format=comma16.2,
         sum(case when in_m12 and not missing(tran_amt) then 1 else 0 end) as txns_12m,
         sum(case when in_m1  and not missing(tran_amt) then 1 else 0 end) as txns_m1
  from work.q1_txn_rel
  group by acct;
quit;

/* Ensure cohort coverage (zero-fill) */
proc sql;
  create table work.acct_spend_full as
  select a.acct, a.date_opened,
         coalesce(b.M1_spend ,0) as M1_spend  format=comma16.2,
         coalesce(b.M3_spend ,0) as M3_spend  format=comma16.2,
         coalesce(b.M12_spend,0) as M12_spend format=comma16.2,
         coalesce(b.txns_12m ,0) as txns_12m,
         coalesce(b.txns_m1  ,0) as txns_m1
  from work.q1_cohort a
  left join work.acct_spend b
    on a.acct = b.acct;
quit;

/*==========================================================
  5) FIRST TRANSACTION (overall) & FIRST-MONTH CATEGORY
==========================================================*/
proc sort data=work.q1_txn_rel out=work.q1_txn_sorted;
  by acct mt_eff_date;
run;

/* First ever transaction in the analysis window */
data work.first_txn_overall(keep=acct first_txn_date first_tran_amt first_category);
  set work.q1_txn_sorted;
  by acct;
  if first.acct then do;
    if not missing(mt_eff_date) then do;
      first_txn_date = mt_eff_date;
      first_tran_amt = tran_amt;
      length first_category $50; first_category = vvalue(mt_category_code);
    end;
    else do; first_txn_date=.; first_tran_amt=.; length first_category $50; first_category=""; end;
    output;
  end;
run;

/* First-MONTH category (earliest txn where months_since_open=0) */
data work.first_month_txn(keep=acct first_month_cat first_month_amt first_month_date);
  set work.q1_txn_sorted;
  by acct mt_eff_date;
  retain took_cat 0;
  if first.acct then took_cat=0;
  if not missing(months_since_open) and months_since_open=0 and took_cat=0 then do;
    first_month_cat  = vvalue(mt_category_code);
    first_month_amt  = tran_amt;
    first_month_date = mt_eff_date;
    took_cat = 1; output;
  end;
run;

/* Merge first behaviors into acct table */
proc sql;
  create table work.q1_spend_12m as
  select a.*,
         f.first_txn_date  format=date9.,
         f.first_tran_amt  format=comma16.2,
         f.first_category,
         m.first_month_cat,
         m.first_month_amt format=comma16.2,
         m.first_month_date format=date9.
  from work.acct_spend_full a
  left join work.first_txn_overall f on a.acct=f.acct
  left join work.first_month_txn  m on a.acct=m.acct;
quit;

/*==========================================================
  6) CATEGORY SUMMARY (BY FIRST-MONTH CATEGORY)
==========================================================*/
proc sql;
  create table work.cat_summary as
  select coalesce(first_month_cat,'(No M1 Txn)') as first_month_cat length=50,
         count(*)                                 as customers format=comma12.,
         mean(M1_spend)                           as avg_M1_spend  format=comma16.2,
         mean(M12_spend)                          as avg_M12_spend format=comma16.2,
         sum(M12_spend)                           as total_M12_spend format=comma16.2
  from work.q1_spend_12m
  group by calculated first_month_cat
  order by total_M12_spend desc, customers desc;
quit;

/*==========================================================
  7) M1 BUCKETS + LIFT TABLES
==========================================================*/
data work.q1_spend_buckets;
  set work.q1_spend_12m;
  length M1_bucket $20 M1_cat $50;
  if M1_spend <= &m1_low_hi. then M1_bucket='Low';
  else if M1_spend <= &m1_mid_hi. then M1_bucket='Mid';
  else if M1_spend <= &m1_high_hi. then M1_bucket='UpperMid';
  else M1_bucket='High';
  M1_cat = coalesce(first_month_cat,'(No M1 Txn)');
run;

proc sql;
  /* Lift by M1 bucket */
  create table work.lift_m1_bucket as
  select M1_bucket,
         count(*)        as customers format=comma12.,
         mean(M1_spend)  as avg_M1_spend  format=comma16.2,
         mean(M12_spend) as avg_M12_spend format=comma16.2
  from work.q1_spend_buckets
  group by M1_bucket
  order by case M1_bucket when 'Low' then 1 when 'Mid' then 2 when 'UpperMid' then 3 when 'High' then 4 end;

  /* Lift by (M1 bucket × first-month category) */
  create table work.lift_bucket_cat as
  select M1_bucket,
         M1_cat as first_month_cat,
         count(*)        as customers format=comma12.,
         mean(M1_spend)  as avg_M1_spend  format=comma16.2,
         mean(M12_spend) as avg_M12_spend format=comma16.2
  from work.q1_spend_buckets
  group by M1_bucket, M1_cat
  order by customers desc, avg_M12_spend desc;
quit;

/*==========================================================
  8) "GOOD SPEND" CLASSIFICATION FOR M3 & M12
     - Choose percentile-based or absolute thresholds
==========================================================*/

/* Percentile-based cutoffs (PCTL) */
%macro build_good_pctl;
  proc univariate data=work.q1_spend_12m noprint;
    var M3_spend M12_spend;
    output out=work.good_cutoffs
      p&PCTL_LEVEL. = M3_pctl M12_pctl
      p50 = M3_median M12_median;
  run;

  data work.q1_spend_classified;
    if _n_=1 then set work.good_cutoffs;
    set work.q1_spend_12m;
    Good_M3  = (M3_spend  >= M3_pctl);
    Good_M12 = (M12_spend >= M12_pctl);
  run;
%mend;

/* Absolute business thresholds (ABS) */
%macro build_good_abs;
  data work.q1_spend_classified;
    set work.q1_spend_12m;
    Good_M3  = (M3_spend  >= &GOOD_M3_ABS.);
    Good_M12 = (M12_spend >= &GOOD_M12_ABS.);
  run;

  /* For reporting, create a synthetic cutoffs table */
  data work.good_cutoffs;
    length note $100;
    M3_pctl = .;  M12_pctl = .;
    M3_median = .; M12_median = .;
    note = cats("ABS thresholds: M3>=", &GOOD_M3_ABS., ", M12>=", &GOOD_M12_ABS.);
  run;
%mend;

/* Driver */
%if &GOOD_METHOD.=PCTL %then %build_good_pctl;
%else %if &GOOD_METHOD.=ABS %then %build_good_abs;

/* Quick distribution of "Good" flags */
proc freq data=work.q1_spend_classified;
  tables Good_M3 Good_M12 / nocum;
run;

/*==========================================================
  9) CORRELATIONS & MODELS
==========================================================*/
ods select PearsonCorr;
proc corr data=work.q1_spend_classified plots=matrix(histogram);
  var M1_spend M3_spend M12_spend txns_m1 txns_12m;
run;

/* Regression: drivers of M12 amount */
proc glmselect data=work.q1_spend_buckets plots=all;
  class M1_cat / param=ref ref='(No M1 Txn)';
  model M12_spend = M1_spend first_tran_amt txns_m1 M1_cat
        / selection=none;   /* keep full */
run; quit;

/* Logistic: predict "Good M12" (or use High12 by amount cutoff) */
data work.model_in;
  set work.q1_spend_buckets;
  High12 = (M12_spend >= &high12_cut.);
run;

proc logistic data=work.model_in plots=roc;
  class M1_cat / param=ref ref='(No M1 Txn)';
  model Good_M12(event='1') = M1_spend first_tran_amt txns_m1 M1_cat;
run;

/*==========================================================
  10) DECILES ON M12 + PROFILES
==========================================================*/
proc rank data=work.q1_spend_classified groups=10 out=work.deciles;
  var M12_spend;
  ranks dec_M12;
run;

proc summary data=work.deciles nway;
  class dec_M12;
  var M1_spend M3_spend M12_spend txns_m1;
  output out=work.dec_profile
    mean(M1_spend)=avg_M1_spend
    mean(M3_spend)=avg_M3_spend
    mean(M12_spend)=avg_M12_spend
    mean(txns_m1)=avg_txns_m1
    n()=customers;
run;

proc sql;
  create table work.dec_contrib as
  select dec_M12, customers, avg_M12_spend,
         (customers*avg_M12_spend) as decile_total format=comma16.2
  from work.dec_profile
  order by dec_M12 desc;
quit;

/*==========================================================
  11) RETENTION / ACTIVITY TABLES
==========================================================*/
/* Month-level rollup per acct (0..11) */
proc sql;
  create table work.acct_month as
  select acct,
         months_since_open as rel_month,
         sum(tran_amt) as month_spend format=comma16.2,
         sum(case when not missing(tran_amt) then 1 else 0 end) as month_txns
  from work.q1_txn_rel
  where 0 <= months_since_open <= 11
  group by acct, months_since_open;
quit;

/* grid for zero-activity months */
data work.grid;
  set work.q1_cohort;
  do rel_month=0 to 11; output; end;
run;

proc sql;
  create table work.acct_month_full as
  select g.acct, g.rel_month,
         coalesce(m.month_spend,0) as month_spend format=comma16.2,
         coalesce(m.month_txns ,0) as month_txns
  from work.grid g
  left join work.acct_month m
    on g.acct=m.acct and g.rel_month=m.rel_month;
quit;

data work.acct_month_full;
  set work.acct_month_full;
  active = (month_txns>0);
run;

/* Overall retention curve */
proc summary data=work.acct_month_full nway;
  class rel_month;
  var active month_spend;
  output out=work.retention_tbl
    mean(active)=pct_active
    mean(month_spend)=avg_spend_per_cust;
run;

/* Retention by first-month category (from q1_spend_buckets for M1_cat) */
proc sql;
  create table work.acct_month_join as
  select a.acct, b.M1_cat, a.rel_month, a.month_spend, a.month_txns, a.active
  from work.acct_month_full a
  left join work.q1_spend_buckets b
    on a.acct=b.acct;
quit;

proc summary data=work.acct_month_join nway;
  class M1_cat rel_month;
  var active month_spend;
  output out=work.retention_by_cat
    mean(active)=pct_active
    mean(month_spend)=avg_spend_per_cust;
run;

/*==========================================================
  12) QA & COVERAGE (counts + missing list)
==========================================================*/
proc sql noprint;
  select count(*) into :n_cohort from work.q1_cohort;
  select count(*) into :n_out    from work.q1_spend_12m;
quit;
%put NOTE: Cohort=&n_cohort Output=&n_out Missing=%eval(&n_cohort - &n_out);

proc sql;
  create table work.missing_accts as
  select a.acct, a.date_opened
  from work.q1_cohort a
  left join work.q1_spend_12m b on a.acct=b.acct
  where b.acct is null;
quit;

/*==========================================================
  13) (OPTIONAL) QUICK SGPLOT VISUALS
==========================================================*/
proc sgplot data=work.retention_tbl;
  series x=rel_month y=pct_active;
  yaxis label="% Active (0-1)";
  xaxis label="Months Since Open (0..11)";
  title "Cohort Retention: % Active by Month Since Open";
run;

proc sgplot data=work.lift_m1_bucket;
  vbar M1_bucket / response=avg_M12_spend datalabel;
  title "Avg 12M Spend by M1 Spend Bucket";
run;

/*==========================================================
  14) EXPORTS
==========================================================*/
%macro expo(ds, fn);
proc export data=&ds.
  outfile="%sysfunc(pathname(outdir))/&fn..csv"
  dbms=csv replace;
run;
%mend;

%expo(work.q1_spend_12m,                         q1_spend_12m_by_acct);
%expo(work.cat_summary,                           q1_first_month_category_summary);
%expo(work.lift_m1_bucket,                        lift_by_m1_bucket);
%expo(work.lift_bucket_cat,                       lift_by_bucket_x_category);
%expo(work.q1_spend_classified,                   spend_with_good_flags);
%expo(work.dec_profile,                           decile_profile);
%expo(work.dec_contrib,                           decile_contribution);
%expo(work.retention_tbl,                         retention_overall);
%expo(work.retention_by_cat,                      retention_by_first_month_category);
%expo(work.missing_accts,                         missing_accounts);
%expo(work.good_cutoffs,                          good_spend_cutoffs_info);

ods graphics off;








































/*==========================================================
  CONFIG & LIBS
==========================================================*/
options mprint mlogic symbolgen nocenter validvarname=any;
options dtreset formchar="|----|+|---+=|-/\<>*";

libname bank "/your/path/here";  /* <-- Replace with actual path */

/* Dates */
%let cohort_start = '01JAN2024'd;
%let cohort_end   = '31MAR2024'd;
%let txn_start    = '01JAN2024'd;
%let txn_end      = '31MAR2025'd;

/*==========================================================
  1) BUILD Q1-2024 COHORT
     - Keep only accounts opened Jan–Mar 2024
==========================================================*/
proc sql;
  create table work.q1_cohort as
  select acct,
         date_opened
  from bank.premier_card_acq
  where &cohort_start. <= date_opened <= &cohort_end.;
quit;

/* QA: Cohort size */
proc sql;
  select count(*) as n_accounts format=comma12.
  from work.q1_cohort;
quit;

/*==========================================================
  2) ATTACH TRANSACTIONS (Jan-2024 to Mar-2025)
     - Keep txns for the cohort only
==========================================================*/
proc sql;
  create table work.q1_txn_raw as
  select c.acct,
         c.date_opened,
         t.tran_amt,
         t.mt_eff_date,
         t.mt_category_code
  from work.q1_cohort c
  left join bank.card_tran3 t
    on c.acct = t.acct
       and &txn_start. <= t.mt_eff_date <= &txn_end.;
quit;

/*==========================================================
  3) RELATIVE MONTHS FROM OPEN DATE
     - months_since_open = 0 (open month), 1 (next), …, 11 (12th month)
==========================================================*/
data work.q1_txn_rel;
  set work.q1_txn_raw;
  /* Only compute if txn_date present; keep missing rows for no-transaction accounts */
  if not missing(mt_eff_date) then do;
    months_since_open = intck('month', date_opened, mt_eff_date, 'c');
    /* Bound for 12M horizon */
    in_m1  = (0 <= months_since_open <= 0);
    in_m3  = (0 <= months_since_open <= 2);
    in_m12 = (0 <= months_since_open <= 11);
  end;
  else do;
    months_since_open = .;
    in_m1  = 0;
    in_m3  = 0;
    in_m12 = 0;
  end;
run;

/*==========================================================
  4) ACCOUNT-LEVEL SPEND SUMMARIES (M1 / M3 / M12) + TXN COUNTS
==========================================================*/
proc sql;
  create table work.acct_spend as
  select acct,
         /* Sum with guards */
         sum(case when in_m1  then tran_amt else 0 end)  as M1_spend  format=comma16.2,
         sum(case when in_m3  then tran_amt else 0 end)  as M3_spend  format=comma16.2,
         sum(case when in_m12 then tran_amt else 0 end)  as M12_spend format=comma16.2,
         /* Useful diagnostics */
         sum(case when in_m12 and not missing(tran_amt) then 1 else 0 end) as txns_12m,
         sum(case when in_m1  and not missing(tran_amt) then 1 else 0 end) as txns_m1
  from work.q1_txn_rel
  group by acct;
quit;

/* Ensure all cohort accts appear even if no txn */
proc sql;
  create table work.acct_spend_full as
  select a.acct,
         a.date_opened,
         coalesce(b.M1_spend ,0) as M1_spend  format=comma16.2,
         coalesce(b.M3_spend ,0) as M3_spend  format=comma16.2,
         coalesce(b.M12_spend,0) as M12_spend format=comma16.2,
         coalesce(b.txns_12m ,0) as txns_12m,
         coalesce(b.txns_m1  ,0) as txns_m1
  from work.q1_cohort a
  left join work.acct_spend b
    on a.acct = b.acct;
quit;

/*==========================================================
  5) FIRST TRANSACTION (DATE, CATEGORY, AMOUNT)
     - Earliest mt_eff_date per acct within the txn window
==========================================================*/
proc sort data=work.q1_txn_rel out=work.q1_txn_sorted;
  by acct mt_eff_date;
run;

data work.first_txn(keep=acct first_txn_date first_tran_amt first_category);
  set work.q1_txn_sorted;
  by acct;
  if first.acct then do;
    if not missing(mt_eff_date) then do;
      first_txn_date  = mt_eff_date;
      first_tran_amt  = tran_amt;
      length first_category $50;
      first_category  = vvalue(mt_category_code); /* preserve as text */
      output;
    end;
    else do;
      /* No txn case: keep row with missing first_txn_date for completeness */
      first_txn_date  = .;
      first_tran_amt  = .;
      length first_category $50;
      first_category  = "";
      output;
    end;
  end;
run;

/*==========================================================
  6) FINAL ACCOUNT-LEVEL OUTPUT (merge spend + first txn)
==========================================================*/
proc sql;
  create table work.q1_spend_12m as
  select a.*,
         f.first_txn_date format=date9.,
         f.first_tran_amt format=comma16.2,
         f.first_category
  from work.acct_spend_full a
  left join work.first_txn f
    on a.acct = f.acct;
quit;

/*==========================================================
  7) CATEGORY-LEVEL INSIGHTS
     - Average 12M spend by first-month category
     - Also show penetration & average M1 spend
==========================================================*/
/* Determine customer’s FIRST-MONTH category: category in months_since_open=0.
   If multiple in same month, we take earliest by date (already sorted). */
data work.first_month_txn(keep=acct first_month_cat first_month_amt);
  set work.q1_txn_sorted;
  by acct mt_eff_date;
  if first.acct then do; retain took_cat 0; took_cat = 0; end;

  /* Only consider M1 transactions */
  if not missing(months_since_open) and months_since_open=0 and took_cat=0 then do;
    first_month_cat = vvalue(mt_category_code);
    first_month_amt = tran_amt;
    took_cat = 1;
    output;
  end;
run;

/* Attach first-month category to account table */
proc sql;
  create table work.q1_spend_with_cat as
  select a.*,
         c.first_month_cat,
         c.first_month_amt format=comma16.2
  from work.q1_spend_12m a
  left join work.first_month_txn c
    on a.acct = c.acct;
quit;

/* Aggregates by first-month category */
proc sql;
  create table work.cat_summary as
  select coalesce(first_month_cat,'(No M1 Txn)') as first_month_cat length=50,
         count(*)                               as customers format=comma12.,
         mean(M12_spend)                        as avg_M12_spend format=comma16.2,
         mean(M1_spend)                         as avg_M1_spend  format=comma16.2,
         sum(M12_spend)                         as total_M12_spend format=comma16.2
  from work.q1_spend_with_cat
  group by calculated first_month_cat
  order by total_M12_spend desc, customers desc;
quit;

/*==========================================================
  8) QA CHECKS
==========================================================*/
/* Coverage: every cohort acct should be present */
proc sql;
  select 
    (select count(*) from work.q1_cohort)        as cohort_accts,
    (select count(*) from work.q1_spend_12m)     as out_accts,
    (calculated cohort_accts - calculated out_accts) as missing_in_output
  ;
quit;

/* Proportion with any 12M txn */
proc sql;
  select sum(txns_12m>0)/count(*) format=percent8.2 as pct_with_12m_txn
  from work.q1_spend_12m;
quit;

/*==========================================================
  9) OPTIONAL EXPORTS
==========================================================*/
proc export data=work.q1_spend_with_cat
    outfile="&sysuserhome./q1_spend_12m_by_acct.csv"
    dbms=csv replace;
run;

proc export data=work.cat_summary
    outfile="&sysuserhome./q1_first_month_category_summary.csv"
    dbms=csv replace;
run;













/*==========================================================
  LIBNAME (set one of these as needed)
----------------------------------------------------------*/
/* Windows example: */
/* libname MYLIB "C:\Users\YourUser\Projects\cust_cube\";  */

/* UNIX/SAS server example: */
/* libname MYLIB "/sas/data/cust_cube/";                   */

/* Database example (Oracle/Teradata – adjust creds/params): */
/* libname MYLIB oracle user=xxx password=yyy path="ORCL"; */
/* libname MYLIB teradata user=xxx password=yyy server=tdprod database=dbname; */

/*==========================================================
  CONFIG
----------------------------------------------------------*/
%let SRCLIB = MYLIB;   /* <- change to the libref you set above */

/* Quiet, but keep enough detail if debugging later */
options nomprint nomlogic nosymbolgen;

/*==========================================================
  1) DISCOVER MONTHLY TABLES (>= JAN23)
----------------------------------------------------------*/
proc sql noprint;
  create table work__cube_list as
  select
      upcase(libname) as libname length=8,
      upcase(memname) as memname length=32,
      /* Extract MMMYY token from cust_cube_MMMYY_data */
      upcase(scan(memname, 3, '_')) as mmmYY length=5,
      /* Convert MMMYY into a true month date for filter/order */
      input(upcase(scan(memname, 3, '_')), monyy5.) as mth format=yymmn7.
  from dictionary.tables
  where upcase(libname) = upcase("&SRCLIB")
    and upcase(memname) like 'CUST_CUBE_%_DATA'
    and calculated mth >= input('JAN23', monyy5.)
  order by calculated mth;
quit;

/* Guard for empty list */
%local n_tables;
proc sql noprint;
  select count(*) into :n_tables from work__cube_list;
quit;

%if &n_tables = 0 %then %do;
  %put ERROR: No tables from JAN23 onward found in &SRCLIB with pattern CUST_CUBE_%_DATA.;
  %return;
%end;

/*==========================================================
  2) UNION ALL MONTHS INTO A SINGLE TABLE
----------------------------------------------------------*/
/* Clean any prior run artifacts */
proc datasets lib=work nolist;
  delete cust_cube_all cust_cube_all_prep cust_cube_premier
         out_new_premier_mom out_months_to_card_freq;
quit;

/* Shell ensures stable structure for PROC APPEND */
data cust_cube_all;
  length cusid $64 ntb_mth $3 final_cus_seg $32 data_mth_char $5;
  /* has_card_mmyy is numeric in source; keep numeric */
  format acquisition_mth yymmn7.;
  stop;
run;

/* Materialize the list into macro vars for looping */
data _null_;
  set work__cube_list end=last;
  retain idx 0;
  idx + 1;
  call symputx(cats('tlib', idx), libname);
  call symputx(cats('tmem', idx), memname);
  call symputx(cats('tmth', idx), mmmYY);
  if last then call symputx('n_tables', idx);
run;

/* Append each monthly table */
%macro append_all;
  %do i=1 %to &n_tables;
    %let _lib = &&tlib&i;
    %let _mem = &&tmem&i;
    %let _mmm = &&tmth&i;

    data work__one;
      set &_lib..&_mem(keep=cusid ntb_mth final_cus_seg has_card_mmyy);
      length data_mth_char $5;
      data_mth_char   = "&_mmm";                        /* e.g., JAN23 */
      acquisition_mth = input(data_mth_char, monyy5.);  /* first of month */
      format acquisition_mth yymmn7.;
    run;

    proc append base=cust_cube_all data=work__one force; run;
  %end;
%mend;
%append_all

/*==========================================================
  3) CONVERT has_card_mmyy (numeric) -> card_mth (month date)
     - If value looks like YYYYMM integer (>99999), parse as YYMMN6.
     - Else treat it as a SAS date and snap to month-begin.
----------------------------------------------------------*/
data cust_cube_all_prep;
  set cust_cube_all;
  length card_mth 8;
  format card_mth yymmn7.;

  if missing(has_card_mmyy) then card_mth = .;
  else do;
    if has_card_mmyy > 99999 then do;                        /* e.g., 202501 */
      card_mth = input(put(has_card_mmyy, z6.), yymmn6.);
    end;
    else do;                                                 /* SAS date -> month begin */
      card_mth = intnx('month', has_card_mmyy, 0, 'b');
    end;
  end;
run;

/*==========================================================
  4) FILTER NEW PREMIER & COMPUTE MONTH LAG
----------------------------------------------------------*/
data cust_cube_premier;
  set cust_cube_all_prep;
  where upcase(ntb_mth) = 'YES' and upcase(final_cus_seg) = 'PREMIER';

  /* Discrete month gap: 0 = same month, 1 = next month, etc. */
  months_to_card = .;
  if nmiss(card_mth, acquisition_mth) = 0 then
    months_to_card = intck('month', acquisition_mth, card_mth);
run;

/*==========================================================
  5) OUTPUTS
----------------------------------------------------------*/
proc sql;
  /* MoM new Premier (order by true date, display formatted) */
  create table out_new_premier_mom as
  select acquisition_mth format=yymmn7. as acquisition_mth label='Acquisition Month',
         count(distinct cusid)          as new_premier_cust
  from cust_cube_premier
  group by acquisition_mth
  order by acquisition_mth;

  /* Frequency: months from acquisition to card */
  create table out_months_to_card_freq as
  select months_to_card,
         count(distinct cusid) as cust_count
  from cust_cube_premier
  group by months_to_card
  order by months_to_card;
quit;

/*==========================================================
  6) OPTIONAL QUICK VIEWS
----------------------------------------------------------*/
title "New Premier Customers (MoM) — From JAN23 Onward";
proc print data=out_new_premier_mom noobs; run;

title "Frequency: Months from Acquisition to Card — From JAN23 Onward";
proc print data=out_months_to_card_freq noobs; run;
title;





















/*=============================*
 | CONFIG
 *=============================*/
%let SRCLIB = MYLIB;     /* <-- CHANGE to your libref */

/*=============================*
 | 1) DISCOVER MONTHLY TABLES (>= JAN23)
 *=============================*/
proc sql noprint;
  create table work__cube_list as
  select  upcase(libname)   as libname length=8,
          upcase(memname)   as memname length=$32,
          upcase(scan(memname, 3, '_')) as mmmYY length=$5,
          /* convert MMMYY to a SAS month date for filtering & ordering */
          input(upcase(scan(memname, 3, '_')), monyy5.) as mth format=yymmn7.
  from dictionary.tables
  where upcase(libname) = upcase("&SRCLIB")
    and upcase(memname) like 'CUST_CUBE_%_DATA'
    /* keep months from JAN23 onwards */
    and calculated mth >= input('JAN23', monyy5.)
  order by calculated mth;
quit;

%let _n=0;
proc sql noprint;
  select count(*) into :_n from work__cube_list;
quit;

%if &_n = 0 %then %do;
  %put ERROR: No source tables found from JAN23 onwards in &SRCLIB with pattern CUST_CUBE_%_DATA.;
  %return;
%end;

/*=============================*
 | 2) UNION FROM JAN23 ONWARD
 *=============================*/
proc datasets lib=work nolist; delete cust_cube_all; quit;

/* Create shell for robust APPEND */
data cust_cube_all;
  length cust_id $64 ntb_mth $3 final_cust_seg $32 card_mmyy $10 data_mth_char $5;
  format acquisition_mth yymmn7.;
  stop;
run;

data _null_;
  set work__cube_list end=last;
  call symputx(cats('tlib',_n_), libname);
  call symputx(cats('tmem',_n_), memname);
  call symputx(cats('tmth',_n_), mmmYY);
  retain _n_ 0;
  _n_+1;
  if last then call symputx('_n_tables', _n_);
run;

%macro append_all;
  %do i=1 %to &_n_tables;
    %let _lib = &&tlib&i;
    %let _mem = &&tmem&i;
    %let _mmm = &&tmth&i;

    data work__one;
      set &*_lib..&*_mem(keep=cust_id ntb_mth final_cust_seg card_mmyy);
      length data_mth_char $5;
      data_mth_char   = "&_mmm";                     /* e.g., JAN23 */
      acquisition_mth = input(data_mth_char, monyy5.); /* first day of month */
      format acquisition_mth yymmn7.;
    run;

    proc append base=cust_cube_all data=work__one force; run;
  %end;
%mend;
%append_all

/*=============================*
 | 3) PARSE card_mmyy -> card_mth
 *=============================*/
data cust_cube_all_prep;
  set cust_cube_all;
  length _c $10;
  _c = strip(upcase(card_mmyy));
  card_mth = .;

  if not missing(_c) then do;
    if prxmatch('/^[A-Z]{3}\d{2}$/', _c) then card_mth = input(_c, monyy5.);      /* JAN25 */
    else if prxmatch('/^\d{4}$/', _c) then do;                                     /* 0125 -> 2025-01 */
      card_mth = input(cats('20',substr(_c,3,2),substr(_c,1,2)), yymmn6.);
    end;
    else if prxmatch('/^\d{6}$/', _c) then card_mth = input(_c, yymmn6.);          /* 202501 */
  end;

  format card_mth yymmn7.;
run;

/*=============================*
 | 4) FILTER New Premier & months_to_card
 *=============================*/
data cust_cube_premier;
  set cust_cube_all_prep;
  where upcase(ntb_mth)='YES' and upcase(final_cust_seg)='PREMIER';
  months_to_card = .;
  if not missing(card_mth) and not missing(acquisition_mth) then
    months_to_card = intck('month', acquisition_mth, card_mth);
run;

/*=============================*
 | 5) OUTPUTS
 *=============================*/
proc sql;
  /* MoM New Premier */
  create table out_new_premier_mom as
  select  put(acquisition_mth, monyy7.) as acquisition_mth label='Acquisition Month',
          count(distinct cust_id) as new_premier_cust
  from cust_cube_premier
  group by acquisition_mth
  order by min(acquisition_mth);

  /* Frequency: months from acquisition to card */
  create table out_months_to_card_freq as
  select  months_to_card,
          count(distinct cust_id) as cust_count
  from cust_cube_premier
  group by months_to_card
  order by months_to_card;
quit;

/* Quick peek (optional) */
title "New Premier Customers (MoM) — From JAN23 Onward";
proc print data=out_new_premier_mom noobs; run;

title "Frequency: Months from Acquisition to Card — From JAN23 Onward";
proc print data=out_months_to_card_freq noobs; run;
title;







I wanted to bring to your attention a challenge we have been facing while running reports. Due to space issues, some of our critical reports are failing and files are not being generated. This requires us to re-run the reports, which takes the same amount of time as the first attempt. At times, this extends our work till late in the evening, and since we have to resume as usual the next day, it does make our schedule a bit difficult to manage.

We have also noticed that the hub refresh time, which earlier used to happen around 12:30 PM, is now happening only after 1:30–2:00 PM. This further delays the process and stretches the BU timelines.

It would be really helpful if something can be done regarding the space availability and refresh timing so that the BAU processes can run smoothly without unnecessary delays. This will also ensure we are able to manage our work within regular hours and maintain a healthy work-life balance.

I would like to request that this space issue and refresh timing be addressed on priority. Without adequate space availability, running critical codes is not feasible. Ensuring timely availability will help us execute BAU processes smoothly and maintain a more sustainable working pattern.













I wanted to clarify regarding the file sharing process. As soon as I receive the file from Gupshup, I make the required modifications and share it the very next day from my side. This has been the consistent practice, ensuring that the Campaign team always receives the file on time and with sufficient window to complete their process.

However, despite multiple follow-ups, there has never been a defined timeline communicated by the Campaign team, and I usually receive the file from them at the very last moment. Because of these delays, I am often left with very little time to work on the files. Still, I make it a point to deliver from my end on time.

Yesterday was the first exception, where the file could not be prepared due to technical issues that were beyond my control. This was not a delay on my part, but a one-off technical challenge. Apart from this, the delivery from my side has always been timely and aligned.































#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Premier Income Estimator — Monotonic Quantile GBM + Conformal + Rules + kNN
===========================================================================

What this does (fact-based, no guesses):
1) Profiles live data (min/max + P10/P50/P90) for TRB, CC limits, income.
2) Calibrates rules from valid-income rows (median ratios + quantile bands).
3) Trains LightGBM quantile models with **monotonic constraints** (q10/q50/q90).
4) Builds **conformal** prediction intervals on a hold-out (coverage ~90%).
5) For each row, computes three estimates:
     R = rules (missing-aware), M = GBM median, K = kNN (missing-aware)
   and blends with a robust **median**, then applies **data-driven sanity caps**.
6) Flags rows using **conformal lower/upper** (or q10/q90 if conformal off).
7) Exports CSV + Excel (flags, summary, data_profile, calibration tables, charts).

Inputs (any case/spaces; normalized internally):
    cusid, cc_max_lmt, cc_tot_lmt, fin_ann_inc1, inc_src, trb_bal_0625_uniq, final_cus_seg
"""
from __future__ import annotations

import argparse
import json
import logging
import os
import sys
from dataclasses import dataclass
from typing import Dict, Tuple, Optional, Any, List

import numpy as np
import pandas as pd

# Matplotlib (no seaborn, single charts)
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

try:
    import lightgbm as lgb
    HAS_LGB = True
except Exception:
    HAS_LGB = False


# ==============================
# Config
# ==============================
@dataclass(frozen=True)
class Defaults:
    q_lo: float = 0.10
    q_hi: float = 0.90
    min_valid_incomes: int = 200       # for GBM+conformal; <200 -> still runs but warns
    min_for_any_train: int = 50        # below this = no training at all
    segment_label: str = "Premier"
    winsor_lo: float = 0.01            # for training features only
    winsor_hi: float = 0.99
    conformal_alpha: float = 0.10      # ~90% coverage
    random_state: int = 42
    gbm_num_leaves: int = 31
    gbm_estimators: int = 600
    gbm_lr: float = 0.05

DEFAULTS = Defaults()


# ==============================
# Logging
# ==============================
def setup_logging(v: int):
    level = logging.WARNING
    if v >= 2:
        level = logging.INFO
    if v >= 3:
        level = logging.DEBUG
    logging.basicConfig(format="%(asctime)s | %(levelname)s | %(message)s",
                        level=level, datefmt="%Y-%m-%d %H:%M:%S")


# ==============================
# IO & Cleaning
# ==============================
def _to_num(x: Any) -> float:
    if x is None:
        return np.nan
    try:
        return float(x)
    except Exception:
        try:
            return float(str(x).replace(",", "").replace("₹", "").strip())
        except Exception:
            return np.nan


def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]
    ren = {"cc_max_limit": "cc_max_lmt", "cc_total_limit": "cc_tot_lmt",
           "trb": "trb_bal_0625_uniq", "income": "fin_ann_inc1"}
    df.rename(columns=ren, inplace=True)
    return df


def read_input(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path)[1].lower()
    if ext in (".xlsx", ".xls"):
        df = pd.read_excel(path, dtype={"cusid": "string"})
    elif ext in (".parquet", ".pq"):
        df = pd.read_parquet(path)
        if "cusid" in df.columns:
            df["cusid"] = df["cusid"].astype("string")
    else:
        df = pd.read_csv(path, dtype={"cusid": "string"})
    df = standardize_columns(df)
    if "cusid" in df.columns:
        df["cusid"] = df["cusid"].astype("string")
    return df


def clean_numeric_cols(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    df = df.copy()
    for c in cols:
        if c in df.columns:
            df[c] = df[c].apply(_to_num)
    return df


# ==============================
# Profiling
# ==============================
PROFILE_COLS = ["cc_max_lmt", "cc_tot_lmt", "trb_bal_0625_uniq", "fin_ann_inc1"]

def profile_vars(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    rows = []
    for c in cols:
        if c not in df.columns: continue
        s = pd.to_numeric(df[c], errors="coerce")
        rows.append({
            "variable": c,
            "non_null": int(s.notna().sum()),
            "min": float(np.nanmin(s)) if s.notna().any() else np.nan,
            "p10": float(np.nanquantile(s, 0.10)) if s.notna().any() else np.nan,
            "p50": float(np.nanmedian(s)) if s.notna().any() else np.nan,
            "p90": float(np.nanquantile(s, 0.90)) if s.notna().any() else np.nan,
            "max": float(np.nanmax(s)) if s.notna().any() else np.nan,
        })
    return pd.DataFrame(rows)


def winsorize_series(s: pd.Series, lo_q: float, hi_q: float) -> pd.Series:
    s = s.astype(float)
    lo = np.nanquantile(s, lo_q)
    hi = np.nanquantile(s, hi_q)
    return s.clip(lower=lo, upper=hi)


# ==============================
# Calibration from valid incomes
# ==============================
def calibrate(df_valid: pd.DataFrame, q_lo: float, q_hi: float):
    work = df_valid.copy()
    work["monthly_income"] = work["fin_ann_inc1"] / 12.0
    work = work[work["monthly_income"] > 0].copy()

    work["r_cc"] = work["cc_max_lmt"] / work["monthly_income"]
    work["r_trb"] = work["trb_bal_0625_uniq"] / work["monthly_income"]
    work.replace([np.inf, -np.inf], np.nan, inplace=True)

    cc_to_monthly = float(np.nanmedian(work["r_cc"]))
    trb_to_months = float(np.nanmedian(work["r_trb"]))
    seg_median_annual = float(np.nanmedian(work["monthly_income"]) * 12.0)
    seg_floor = 0.9 * seg_median_annual  # conservative lower floor

    ratios = {"__overall__": {
        "cc_to_monthly_income_ratio": cc_to_monthly,
        "trb_to_monthly_income_months": trb_to_months,
        "n": int(work.shape[0])
    }}
    mins = {"__overall__": seg_floor}

    # draft expected via rules to compute bands
    def rule_row(r):
        cc_ratio = max(1e-9, cc_to_monthly)
        trb_months = max(1e-9, trb_to_months)
        trb = max(0.0, r.get("trb_bal_0625_uniq", 0.0) or 0.0)
        cmax = max(0.0, r.get("cc_max_lmt", 0.0) or 0.0)
        ctot = max(0.0, r.get("cc_tot_lmt", 0.0) or 0.0)
        monthly_terms = []
        if cmax > 0: monthly_terms.append(cmax/cc_ratio)
        if trb  > 0: monthly_terms.append(trb/trb_months)
        if ctot > 0: monthly_terms.append((ctot/cc_ratio)*0.25)
        if not monthly_terms: return np.nan
        return max(monthly_terms)*12.0

    work["est_rules"] = [rule_row(r) for _, r in work.iterrows()]
    rep_to_exp = work["fin_ann_inc1"] / (work["est_rules"] + 1e-9)
    ql = float(np.nanquantile(rep_to_exp, q_lo))
    qh = float(np.nanquantile(rep_to_exp, q_hi))
    bands = {"__overall__": {"band_lower": max(0.4, ql), "band_upper": min(2.5, qh), "n": int(work.shape[0])}}

    # for Excel
    calib_ratios = pd.DataFrame([{
        "final_cus_seg": "Premier", "n": work.shape[0],
        "cc_to_monthly_income_ratio": cc_to_monthly,
        "trb_to_monthly_income_months": trb_to_months,
        "annual_income_median": seg_median_annual
    }])
    calib_bands = pd.DataFrame([{
        "final_cus_seg": "Premier", "q_lo": bands["__overall__"]["band_lower"],
        "q_hi": bands["__overall__"]["band_upper"], "n": work.shape[0]
    }])
    calib_overall = pd.DataFrame({
        "metric": ["cc_to_monthly_income_ratio","trb_to_monthly_income_months","band_lower","band_upper","n"],
        "value": [cc_to_monthly, trb_to_months, bands["__overall__"]["band_lower"], bands["__overall__"]["band_upper"], work.shape[0]]
    })
    sheets = {"ratios_by_seg": calib_ratios, "bands_by_seg": calib_bands, "overall": calib_overall}
    return ratios, bands, mins, sheets


# ==============================
# Rules, kNN, Sanity caps
# ==============================
FEATS = ["trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt"]

def available_feats(row: pd.Series, feats: List[str]) -> List[str]:
    have = []
    for f in feats:
        v = row.get(f, np.nan)
        if pd.notna(v) and np.isfinite(v) and float(v) > 0.0:
            have.append(f)
    return have


def rules_estimate(row: pd.Series, ratios_overall: Dict[str, float], seg_floor_annual: float) -> float:
    cc_ratio = max(1e-9, ratios_overall["cc_to_monthly_income_ratio"])
    trb_months = max(1e-9, ratios_overall["trb_to_monthly_income_months"])
    trb  = max(0.0, float(row.get("trb_bal_0625_uniq", 0.0)) or 0.0)
    cmax = max(0.0, float(row.get("cc_max_lmt", 0.0)) or 0.0)
    ctot = max(0.0, float(row.get("cc_tot_lmt", 0.0)) or 0.0)

    monthly_terms = []
    if cmax > 0: monthly_terms.append(cmax / cc_ratio)
    if trb  > 0: monthly_terms.append(trb / trb_months)
    if ctot > 0: monthly_terms.append((ctot / cc_ratio) * 0.25)
    if not monthly_terms: return np.nan

    annual = max(monthly_terms) * 12.0
    return float(max(annual, 0.8 * seg_floor_annual))


def knn_block(df_valid: pd.DataFrame, feats: List[str], k: int = 15):
    base = df_valid[feats + ["fin_ann_inc1"]].copy()
    base = base.replace([np.inf, -np.inf], np.nan).dropna(subset=["fin_ann_inc1"])
    if base.empty:
        return lambda row: np.nan

    y_log = np.log1p(base["fin_ann_inc1"].astype(float).values)
    Xcols = {f: base[f].astype(float).values for f in feats}

    def predict(row: pd.Series) -> float:
        pres = available_feats(row, feats)
        if not pres: return np.nan
        dist = None
        for f in pres:
            rv = float(row.get(f, 0.0) or 0.0)
            d = (Xcols[f] - rv) ** 2
            dist = d if dist is None else (dist + d)
        dist = np.sqrt(dist)
        idx = np.argsort(dist)[:max(3, min(k, len(dist)))]
        return float(np.expm1(np.nanmedian(y_log[idx])))
    return predict


def derive_caps(df_valid: pd.DataFrame, feats: List[str]) -> Dict[str, float]:
    caps = {"c_trb": np.inf, "c_cmax": np.inf, "c_ctot": np.inf}
    d = df_valid[feats + ["fin_ann_inc1"]].astype(float).replace([np.inf, -np.inf], np.nan)
    y = d["fin_ann_inc1"]

    def p90_ratio(col):
        r = (y / d[col]).replace([np.inf, -np.inf], np.nan).dropna()
        return float(np.nanquantile(r, 0.90)) if not r.empty else np.inf

    if "trb_bal_0625_uniq" in d: caps["c_trb"]  = p90_ratio("trb_bal_0625_uniq")
    if "cc_max_lmt" in d:        caps["c_cmax"] = p90_ratio("cc_max_lmt")
    if "cc_tot_lmt" in d:        caps["c_ctot"] = p90_ratio("cc_tot_lmt")
    return caps


def apply_caps(row: pd.Series, y_hat: float, caps: Dict[str, float],
               p01_income: float, p99_income: float, seg_floor_annual: float) -> float:
    if pd.isna(y_hat): return np.nan
    up_list = [p99_income]
    trb  = float(row.get("trb_bal_0625_uniq", 0.0) or 0.0)
    cmax = float(row.get("cc_max_lmt", 0.0) or 0.0)
    ctot = float(row.get("cc_tot_lmt", 0.0) or 0.0)
    if np.isfinite(caps["c_trb"])  and trb  > 0: up_list.append(caps["c_trb"]  * trb)
    if np.isfinite(caps["c_cmax"]) and cmax > 0: up_list.append(caps["c_cmax"] * cmax)
    if np.isfinite(caps["c_ctot"]) and ctot > 0: up_list.append(caps["c_ctot"] * ctot)
    upper_cap = min([v for v in up_list if np.isfinite(v)])
    lower_cap = max(p01_income, 0.8 * seg_floor_annual)
    return float(np.clip(y_hat, lower_cap, upper_cap))


# ==============================
# GBM Features & Training (Quantile + Monotonic)
# ==============================
def build_gbm_features(df: pd.DataFrame) -> pd.DataFrame:
    X = df.copy()
    # raw
    for c in FEATS:
        if c in X: X[c] = pd.to_numeric(X[c], errors="coerce")
    # log1p
    for c in FEATS:
        lc = f"log1p_{c}"
        X[lc] = np.log1p(X[c]) if c in X else np.nan
    # missing flags (zero or NaN treated as missing signal)
    for c in FEATS:
        X[f"is_missing_{c}"] = (~(X[c].astype(float) > 0)).astype(int)
    # simple ratios (safe divide)
    X["ratio_trb_cmax"] = (X["trb_bal_0625_uniq"] / (X["cc_max_lmt"] + 1e-9)).replace([np.inf,-np.inf], np.nan)
    X["ratio_trb_ctot"] = (X["trb_bal_0625_uniq"] / (X["cc_tot_lmt"] + 1e-9)).replace([np.inf,-np.inf], np.nan)
    X["ratio_cmax_ctot"] = (X["cc_max_lmt"] / (X["cc_tot_lmt"] + 1e-9)).replace([np.inf,-np.inf], np.nan)
    return X


def gbm_train_quantiles(df_valid: pd.DataFrame, random_state: int,
                        num_leaves: int, n_estimators: int, lr: float):
    """
    Train three monotonic quantile GBMs (q10/q50/q90) on log-income.
    Monotonic constraints apply on log1p_TRB / log1p_CCmax / log1p_CCtot (increasing).
    """
    if not HAS_LGB:
        return None

    X = build_gbm_features(df_valid)
    y = df_valid["fin_ann_inc1"].astype(float)
    y_log = np.log1p(y)

    # train/valid split (20% holdout) for conformal
    rs = np.random.RandomState(DEFAULTS.random_state)
    m = X.shape[0]
    idx = np.arange(m); rs.shuffle(idx)
    split = int(0.8 * m)
    tr_idx, va_idx = idx[:split], idx[split:]
    Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]
    ytr, yva = y_log.iloc[tr_idx], y_log.iloc[va_idx]

    # features order and monotonic constraints
    feats = list(X.columns)
    mono = []
    for f in feats:
        if f.startswith("log1p_trb") or f.startswith("log1p_cc_max") or f.startswith("log1p_cc_tot"):
            mono.append(1)
        else:
            mono.append(0)

    params_base = dict(
        boosting_type="gbdt",
        num_leaves=num_leaves,
        learning_rate=lr,
        n_estimators=n_estimators,
        subsample=0.9,
        colsample_bytree=0.9,
        random_state=random_state,
        monotone_constraints=mono,
        min_data_in_leaf=20,
        verbose=-1
    )

    def train_for_alpha(alpha):
        model = lgb.LGBMRegressor(objective="quantile", alpha=alpha, **params_base)
        model.fit(Xtr, ytr, eval_set=[(Xva, yva)], eval_metric="quantile", verbose=False)
        return model

    models = {
        "q10": train_for_alpha(0.10),
        "q50": train_for_alpha(0.50),
        "q90": train_for_alpha(0.90),
        "feats": feats,
        "valid_idx": va_idx.tolist(),
        "y_log_valid": y_log.iloc[va_idx].values
    }
    return models


def gbm_predict_all(df: pd.DataFrame, models: Dict[str, Any]) -> Dict[str, np.ndarray]:
    X = build_gbm_features(df)[models["feats"]]
    out = {}
    for k in ("q10","q50","q90"):
        out[k] = np.expm1(models[k].predict(X))
    return out


def conformal_from_valid(models: Dict[str, Any], preds_valid_log50: np.ndarray, alpha: float) -> float:
    """
    Simple symmetric absolute residual conformal on validation (log space).
    Returns q such that coverage ~ 1-alpha when using +/- q around median.
    """
    yv = models["y_log_valid"]
    res = np.abs(yv - preds_valid_log50)
    q = np.quantile(res, 1 - alpha)
    return float(q)


# ==============================
# Plotting
# ==============================
def scatter_with_fit(path, x, y, xlab, ylab):
    plt.figure(); plt.scatter(x, y, alpha=0.4)
    plt.xlabel(xlab); plt.ylabel(ylab)
    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()


def actual_vs_pred(path, actual, pred):
    plt.figure(); plt.scatter(pred, actual, alpha=0.4)
    lo = np.nanpercentile(np.concatenate([actual, pred]), 1)
    hi = np.nanpercentile(np.concatenate([actual, pred]), 99)
    plt.plot([lo, hi], [lo, hi])
    plt.xlabel("Predicted Income (₹)"); plt.ylabel("Actual Income (₹)")
    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()


def resid_plots(path_sc, path_hist, pred, resid):
    plt.figure(); plt.scatter(pred, resid, alpha=0.4); plt.axhline(0)
    plt.xlabel("Predicted Income (₹)"); plt.ylabel("Residuals (Actual - Pred)")
    plt.tight_layout(); plt.savefig(path_sc, dpi=140); plt.close()
    plt.figure(); plt.hist(resid[~np.isnan(resid)], bins=40)
    plt.xlabel("Residuals"); plt.ylabel("Frequency")
    plt.tight_layout(); plt.savefig(path_hist, dpi=140); plt.close()


def corr_heatmap(path, df, cols):
    corr = df[cols].corr(method="pearson")
    plt.figure(); plt.imshow(corr, interpolation="nearest")
    plt.xticks(range(len(cols)), cols, rotation=45, ha="right"); plt.yticks(range(len(cols)), cols)
    plt.colorbar(); plt.title("Correlation Matrix")
    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()


# ==============================
# Flagging
# ==============================
def flag_row(reported, lower, upper):
    if pd.isna(reported) or reported <= 0: return "Missing"
    if reported < lower or reported > upper: return "Mismatch"
    return "OK"


# ==============================
# Pipeline
# ==============================
def run_pipeline(
    df: pd.DataFrame,
    outdir: str,
    q_lo: float,
    q_hi: float,
    winsorize_for_train: bool,
    conformal_alpha: float,
    verbose: int
):
    os.makedirs(outdir, exist_ok=True)
    if "final_cus_seg" not in df.columns:
        df["final_cus_seg"] = Defaults.segment_label
    df = clean_numeric_cols(df, PROFILE_COLS)

    # 1) Profile
    profile = profile_vars(df, PROFILE_COLS)

    # 2) Build modeling view
    model_df = df.copy()
    if winsorize_for_train:
        for c in FEATS:
            if c in model_df and model_df[c].notna().any():
                model_df[c] = winsorize_series(model_df[c], Defaults.winsor_lo, Defaults.winsor_hi)

    valid_mask = model_df["fin_ann_inc1"].notna() & (model_df["fin_ann_inc1"] > 0)
    df_valid = model_df.loc[valid_mask].copy()
    n_valid = int(df_valid.shape[0])

    if n_valid < Defaults.min_for_any_train:
        raise RuntimeError(f"Only {n_valid} rows with valid income; need ≥ {Defaults.min_for_any_train} to train.")

    # 3) Calibration for rules & bands
    ratios, bands, mins, calib_sheets = calibrate(df_valid, q_lo, q_hi)
    seg_floor_annual = float(mins["__overall__"])

    # 4) Train GBM quantiles (if available)
    models = None
    if HAS_LGB:
        models = gbm_train_quantiles(
            df_valid, random_state=Defaults.random_state,
            num_leaves=Defaults.gbm_num_leaves,
            n_estimators=Defaults.gbm_estimators,
            lr=Defaults.gbm_lr
        )
        if n_valid < Defaults.min_valid_incomes:
            logging.warning("Valid incomes are %d (< %d). GBM will work but intervals/stability improve with more.",
                            n_valid, Defaults.min_valid_incomes)
    else:
        logging.warning("lightgbm not installed. Skipping GBM; using Rules + kNN only.")

    # 5) Predict GBM on all rows (if trained)
    gbm_preds = {"q10": np.full(len(df), np.nan), "q50": np.full(len(df), np.nan), "q90": np.full(len(df), np.nan)}
    conf_delta = None
    if models is not None:
        gbm_preds = gbm_predict_all(model_df, models)
        # conformal delta on validation
        X_valid_hold = build_gbm_features(df_valid).iloc[models["valid_idx"]][models["feats"]]
        med_valid_log = models["q50"].predict(X_valid_hold)
        conf_delta = conformal_from_valid(models, med_valid_log, conformal_alpha)

    # 6) kNN predictor + caps
    knn_pred = knn_block(df_valid, FEATS, k=15)
    caps = derive_caps(df_valid, FEATS)
    inc_series = df_valid["fin_ann_inc1"].astype(float)
    p01_income = float(np.nanquantile(inc_series, 0.01)) if inc_series.notna().any() else 0.0
    p99_income = float(np.nanquantile(inc_series, 0.99)) if inc_series.notna().any() else np.inf

    # 7) Per-row estimates (R, M, K), blend and cap
    R_list, M_list, K_list, Y_list = [], [], [], []
    L_q, U_q = [], []  # quantile interval from GBM
    L_c, U_c = [], []  # conformal interval around q50

    for i, r in model_df.iterrows():
        # rules
        R = rules_estimate(r, ratios["__overall__"], seg_floor_annual)

        # gbm median
        M = float(gbm_preds["q50"][i]) if np.isfinite(gbm_preds["q50"][i]) else np.nan

        # kNN
        K = knn_pred(r)

        vals = [v for v in (R, M, K) if pd.notna(v) and np.isfinite(v)]
        y_raw = float(np.nanmedian(vals)) if vals else np.nan

        # caps
        y_cap = apply_caps(r, y_raw, caps, p01_income, p99_income, seg_floor_annual)
        R_list.append(R); M_list.append(M); K_list.append(K); Y_list.append(y_cap)

        # intervals
        lq = float(gbm_preds["q10"][i]) if np.isfinite(gbm_preds["q10"][i]) else np.nan
        uq = float(gbm_preds["q90"][i]) if np.isfinite(gbm_preds["q90"][i]) else np.nan
        L_q.append(lq); U_q.append(uq)

        if models is not None and conf_delta is not None and np.isfinite(M):
            # conformal in log space: q50_log +/- delta, back-transform
            mlog = np.log1p(M)
            lc = float(np.expm1(max(0.0, mlog - conf_delta)))
            uc = float(np.expm1(mlog + conf_delta))
        else:
            lc, uc = np.nan, np.nan
        L_c.append(lc); U_c.append(uc)

    # Choose intervals: use conformal if available; else quantile; else calibrated bands
    lower_final, upper_final = [], []
    bl = float(bands["__overall__"]["band_lower"]); bu = float(bands["__overall__"]["band_upper"])
    for i, yhat in enumerate(Y_list):
        if np.isfinite(L_c[i]) and np.isfinite(U_c[i]):
            lo, up = L_c[i], U_c[i]
        elif np.isfinite(L_q[i]) and np.isfinite(U_q[i]):
            lo, up = L_q[i], U_q[i]
        else:
            lo, up = yhat * bl, yhat * bu
        # also clip to global caps
        lo = max(lo, p01_income, 0.8 * seg_floor_annual)
        up = min(up, p99_income)
        lower_final.append(lo); upper_final.append(up)

    # 8) assemble outputs
    out = df.copy()
    out["est_rules"] = R_list
    out["est_gbm"]   = M_list
    out["est_knn"]   = K_list
    out["expected_income"] = Y_list
    out["lower_bound"] = lower_final
    out["upper_bound"] = upper_final
    out["income_gap"] = out["expected_income"] - out["fin_ann_inc1"].fillna(0.0)
    out["income_flag"] = [flag_row(rep, lo, up)
                          for rep, lo, up in zip(out["fin_ann_inc1"], out["lower_bound"], out["upper_bound"])]

    # 9) summary
    summary = (
        out.groupby(["final_cus_seg", "income_flag"], dropna=False)
        .agg(customers=("cusid", "nunique"),
             avg_reported=("fin_ann_inc1", "mean"),
             avg_expected=("expected_income", "mean"),
             total_gap=("income_gap", "sum"))
        .reset_index()
    )

    # 10) charts
    charts = {}
    charts["corr_matrix"] = os.path.join(outdir, "corr_matrix.png")
    corr_cols = [c for c in ["fin_ann_inc1","trb_bal_0625_uniq","cc_max_lmt","cc_tot_lmt"] if c in df.columns]
    if len(corr_cols) >= 2: corr_heatmap(charts["corr_matrix"], df, corr_cols)

    # if GBM present, residual plots w.r.t GBM median
    if any(np.isfinite(M_list)):
        act = out["fin_ann_inc1"].astype(float).values
        pred = np.array([m if np.isfinite(m) else np.nan for m in M_list])
        resid = act - pred
        charts["actual_vs_pred"] = os.path.join(outdir, "actual_vs_pred.png")
        charts["resid_vs_pred"]  = os.path.join(outdir, "resid_vs_pred.png")
        charts["resid_hist"]     = os.path.join(outdir, "resid_hist.png")
        actual_vs_pred(charts["actual_vs_pred"], act, pred)
        resid_plots(charts["resid_vs_pred"], charts["resid_hist"], pred, resid)

    # 11) save files
    out_csv = os.path.join(outdir, "premier_income_flags.csv")
    out_xlsx = os.path.join(outdir, "premier_income_flags.xlsx")
    out.to_csv(out_csv, index=False)

    with pd.ExcelWriter(out_xlsx, engine="xlsxwriter") as writer:
        out.to_excel(writer, index=False, sheet_name="flags")
        summary.to_excel(writer, index=False, sheet_name="summary")
        profile.to_excel(writer, index=False, sheet_name="data_profile")
        # calibration tables
        if not calib_sheets["ratios_by_seg"].empty:
            calib_sheets["ratios_by_seg"].to_excel(writer, index=False, sheet_name="calib_ratios")
        if not calib_sheets["bands_by_seg"].empty:
            calib_sheets["bands_by_seg"].to_excel(writer, index=False, sheet_name="calib_bands")
        if not calib_sheets["overall"].empty:
            calib_sheets["overall"].to_excel(writer, index=False, sheet_name="calib_overall")

        # charts sheet
        wb = writer.book
        ws = wb.add_worksheet("charts"); r = 1
        for name, p in charts.items():
            if os.path.exists(p): ws.insert_image(r, 1, p); r += 22

    meta = {
        "has_lightgbm": HAS_LGB,
        "valid_rows_for_training": n_valid,
        "conformal_alpha": conformal_alpha,
        "bands_lower": float(bands["__overall__"]["band_lower"]),
        "bands_upper": float(bands["__overall__"]["band_upper"])
    }
    with open(os.path.join(outdir, "meta.txt"), "w", encoding="utf-8") as f:
        for k, v in meta.items(): f.write(f"{k}: {v}\n")

    return out_csv, out_xlsx, charts


# ==============================
# CLI
# ==============================
def main(args_list: Optional[List[str]] = None):
    ap = argparse.ArgumentParser(description="Premier Income Estimator — GBM+Conformal+Rules+kNN")
    ap.add_argument("--in", dest="in_path", required=True, help="Input file (.xlsx/.xls/.csv/.parquet)")
    ap.add_argument("--outdir", default="./out", help="Output directory")
    ap.add_argument("--winsorize", action="store_true", help="Winsorize features for training only")
    ap.add_argument("--q_lo", type=float, default=DEFAULTS.q_lo)
    ap.add_argument("--q_hi", type=float, default=DEFAULTS.q_hi)
    ap.add_argument("--conformal_alpha", type=float, default=DEFAULTS.conformal_alpha)
    ap.add_argument("-v", "--verbose", action="count", default=1)
    args = ap.parse_args(args_list)

    setup_logging(args.verbose)

    try:
        df = read_input(args.in_path)
    except Exception as e:
        logging.exception("Failed to read input: %s", e); sys.exit(2)

    required = ["cusid","cc_max_lmt","cc_tot_lmt","fin_ann_inc1","inc_src","trb_bal_0625_uniq"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        logging.error("Missing required columns: %s", missing); sys.exit(2)

    try:
        out_csv, out_xlsx, charts = run_pipeline(
            df=df,
            outdir=args.outdir,
            q_lo=args.q_lo, q_hi=args.q_hi,
            winsorize_for_train=args.winsorize,
            conformal_alpha=args.conformal_alpha,
            verbose=args.verbose
        )
    except Exception as e:
        logging.exception("Run failed: %s", e); sys.exit(2)

    logging.info("Output CSV: %s", out_csv)
    logging.info("Output XLSX: %s", out_xlsx)
    logging.info("Charts saved alongside outputs.")


if __name__ == "__main__":
    # main()  # normal CLI
    # --- Spyder quick-run: uncomment and set your path ---
    # main([
    #     "--in", r"C:\path\to\mydata.xlsx",
    #     "--outdir", r"C:\path\to\out",
    #     "--winsorize",
    #     "-v"
    # ])
    pass

































#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Premier Income Estimator — Production (Data-Driven, Profiling, Regression, Charts)
==================================================================================

Guarantees (fact-based, no guesses):
- Profiles live data (min/max + P10/P50/P90) for: TRB, CC max, CC total, Income.
- Calibrates mismatch bands from data (quantiles of reported/expected via rules).
- Predicts income using Ridge/OLS (closed-form) with K-fold CV over an alpha grid.
- Flags Missing / Mismatch / OK using calibrated bands (no hardcoded thresholds).
- Exports CSV + Excel (flags, summary, data_profile, calibration tables, charts).
- Robust to outliers via optional winsorization (for modeling only).
- Friendly to Spyder and Terminal (CLI). No sklearn dependency.

Required columns (case/spacing tolerant — normalized internally):
    cusid, cc_max_lmt, cc_tot_lmt, fin_ann_inc1, inc_src, trb_bal_0625_uniq, final_cus_seg

Run (terminal):
    python premier_income_estimator_final.py --in "mydata.xlsx" --outdir ./out --mode regression
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import sys
from dataclasses import dataclass
from typing import Dict, Tuple, Optional, Any, List

import numpy as np
import pandas as pd

# Matplotlib (no seaborn, single-plot figures, no custom styles)
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt


# ==============================
# Config & Logging
# ==============================
@dataclass(frozen=True)
class Defaults:
    q_lo: float = 0.10                 # lower quantile for band calibration
    q_hi: float = 0.90                 # upper quantile for band calibration
    kfolds: int = 5                    # CV folds for ridge/OLS
    alpha_grid: tuple = (0.0, 0.1, 1.0, 10.0, 100.0, 1000.0)  # 0.0 -> OLS
    min_valid_incomes: int = 50        # minimum rows with valid income to calibrate/train
    use_log_model: bool = True         # log1p modeling (stable on skew)
    segment_label: str = "Premier"     # default if final_cus_seg missing


DEFAULTS = Defaults()


def setup_logging(verbosity: int = 1):
    level = logging.WARNING
    if verbosity >= 2:
        level = logging.INFO
    if verbosity >= 3:
        level = logging.DEBUG
    logging.basicConfig(
        format="%(asctime)s | %(levelname)s | %(message)s",
        level=level,
        datefmt="%Y-%m-%d %H:%M:%S",
    )


# ==============================
# IO & Cleaning
# ==============================
def _to_num(x: Any) -> float:
    if x is None:
        return np.nan
    try:
        return float(x)
    except Exception:
        try:
            return float(str(x).replace(",", "").replace("₹", "").strip())
        except Exception:
            return np.nan


def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]
    # common variants
    rename = {
        "cc_max_limit": "cc_max_lmt",
        "cc_total_limit": "cc_tot_lmt",
        "trb": "trb_bal_0625_uniq",
        "income": "fin_ann_inc1",
    }
    df.rename(columns=rename, inplace=True)
    return df


def read_input(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path)[1].lower()
    if ext in (".xlsx", ".xls"):
        df = pd.read_excel(path, dtype={"cusid": "string"})
    elif ext in (".parquet", ".pq"):
        df = pd.read_parquet(path)
        if "cusid" in df.columns:
            df["cusid"] = df["cusid"].astype("string")
    else:
        df = pd.read_csv(path, dtype={"cusid": "string"})
    df = standardize_columns(df)
    if "cusid" in df.columns:
        df["cusid"] = df["cusid"].astype("string")
    return df


def clean_numeric_cols(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    df = df.copy()
    for c in cols:
        if c in df.columns:
            df[c] = df[c].apply(_to_num)
    return df


# ==============================
# Data Profiling (facts from live data)
# ==============================
PROFILE_COLS = ["cc_max_lmt", "cc_tot_lmt", "trb_bal_0625_uniq", "fin_ann_inc1"]

def profile_vars(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    rows = []
    for c in cols:
        if c not in df.columns:
            continue
        s = pd.to_numeric(df[c], errors="coerce")
        rows.append({
            "variable": c,
            "non_null": int(s.notna().sum()),
            "min": float(np.nanmin(s)) if s.notna().any() else np.nan,
            "p10": float(np.nanquantile(s, 0.10)) if s.notna().any() else np.nan,
            "p50": float(np.nanmedian(s)) if s.notna().any() else np.nan,
            "p90": float(np.nanquantile(s, 0.90)) if s.notna().any() else np.nan,
            "max": float(np.nanmax(s)) if s.notna().any() else np.nan,
        })
    return pd.DataFrame(rows)


def winsorize_series(s: pd.Series, lo_q: float, hi_q: float) -> pd.Series:
    s = s.astype(float)
    lo = np.nanquantile(s, lo_q)
    hi = np.nanquantile(s, hi_q)
    return s.clip(lower=lo, upper=hi)


# ==============================
# Calibration (purely data-driven)
# ==============================
def calibrate_from_data(df_valid: pd.DataFrame, q_lo: float, q_hi: float):
    """
    Uses ONLY rows with valid income to learn:
      - Ratios for rules: median(cc_max / monthly_income), median(TRB / monthly_income)
      - Segment minimum (floor) from median annual income (data-driven)
      - Bands from quantiles of (reported / expected_rules)
    """
    work = df_valid.copy()
    work["monthly_income"] = work["fin_ann_inc1"] / 12.0
    work = work[work["monthly_income"] > 0].copy()

    work["r_cc"] = work["cc_max_lmt"] / work["monthly_income"]
    work["r_trb"] = work["trb_bal_0625_uniq"] / work["monthly_income"]
    work.replace([np.inf, -np.inf], np.nan, inplace=True)

    cc_to_monthly = float(np.nanmedian(work["r_cc"].values))
    trb_to_months = float(np.nanmedian(work["r_trb"].values))
    seg_min_annual = float(np.nanmedian(work["monthly_income"].values) * 12.0 * 0.9)

    ratios = {"__overall__": {
        "cc_to_monthly_income_ratio": cc_to_monthly,
        "trb_to_monthly_income_months": trb_to_months,
        "n": int(work.shape[0])
    }}
    mins = {"__overall__": seg_min_annual}

    # expected via rules to compute mismatch bands
    def expected_rules_row(row):
        cc_ratio = max(1e-9, cc_to_monthly)
        trb_months = max(1e-9, trb_to_months)
        cc_max = max(0.0, row.get("cc_max_lmt", 0.0) or 0.0)
        cc_tot = max(0.0, row.get("cc_tot_lmt", 0.0) or 0.0)
        trb = max(0.0, row.get("trb_bal_0625_uniq", 0.0) or 0.0)
        monthly = max(cc_max / cc_ratio, trb / trb_months, (cc_tot / cc_ratio) * 0.25)
        return monthly * 12.0

    work["expected_rules"] = [expected_rules_row(r) for _, r in work.iterrows()]
    eps = 1e-9
    rep_to_exp = work["fin_ann_inc1"] / (work["expected_rules"] + eps)
    band_lower = float(np.nanquantile(rep_to_exp.values, q_lo))
    band_upper = float(np.nanquantile(rep_to_exp.values, q_hi))
    bands = {"__overall__": {"band_lower": max(0.4, band_lower),
                             "band_upper": min(2.5, band_upper),
                             "n": int(work.shape[0])}}

    # Summary tables (for Excel)
    calib_ratios = pd.DataFrame([{
        "final_cus_seg": "Premier",
        "n": work.shape[0],
        "cc_to_monthly_income_ratio": cc_to_monthly,
        "trb_to_monthly_income_months": trb_to_months,
        "annual_income_median": float(np.nanmedian(work["monthly_income"]) * 12.0)
    }])
    calib_bands = pd.DataFrame([{
        "final_cus_seg": "Premier",
        "q_lo": bands["__overall__"]["band_lower"], "q_hi": bands["__overall__"]["band_upper"], "n": work.shape[0]
    }])
    calib_overall = pd.DataFrame({
        "metric": ["cc_to_monthly_income_ratio", "trb_to_monthly_income_months",
                   "band_lower", "band_upper", "n"],
        "value": [cc_to_monthly, trb_to_months, bands["__overall__"]["band_lower"],
                  bands["__overall__"]["band_upper"], work.shape[0]]
    })

    summary_tbls = {"ratios_by_seg": calib_ratios, "bands_by_seg": calib_bands, "overall": calib_overall}
    return ratios, bands, mins, summary_tbls


# ==============================
# Modeling (OLS/Ridge via closed-form + CV) — FIXED alpha handling
# ==============================
def _ridge_closed_form(X: np.ndarray, y: np.ndarray, alpha: float) -> np.ndarray:
    """
    Closed-form ridge solution with unpenalized intercept (first column).
    alpha is forced to float >= 0.
    """
    if alpha is None or not np.isfinite(alpha) or alpha < 0:
        alpha = 1.0  # safe fallback
    n_features = X.shape[1]
    I = np.eye(n_features)
    I[0, 0] = 0.0  # do not penalize intercept
    XtX = X.T @ X
    Xty = X.T @ y
    beta = np.linalg.solve(XtX + float(alpha) * I, Xty)
    return beta


def _kfold_indices(n: int, k: int, rng: np.random.RandomState) -> List[Tuple[np.ndarray, np.ndarray]]:
    idx = np.arange(n)
    rng.shuffle(idx)
    folds = np.array_split(idx, k)
    pairs = []
    for i in range(k):
        val = folds[i]
        train = np.hstack([folds[j] for j in range(k) if j != i]) if k > 1 else idx
        pairs.append((train, val))
    return pairs


def fit_ridge_cv(df_valid: pd.DataFrame, use_log: bool, kfolds: int, alphas: Optional[tuple]):
    """
    Cross-validate ridge across an alpha grid (includes 0.0 = OLS).
    FIX: guarantees non-empty alpha grid and non-None best_alpha.
    Drops any rows with NaNs in features or target before training.
    """
    feats = ["trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt"]
    data = df_valid[feats + ["fin_ann_inc1"]].astype(float)
    data = data.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how="any")
    if data.empty or data.shape[0] < 2:
        raise RuntimeError("Not enough clean rows to train regression after dropping NaNs/infs.")

    X = data[feats].values
    y = data["fin_ann_inc1"].values
    if use_log:
        X = np.log1p(X); y = np.log1p(y)

    X1 = np.c_[np.ones(X.shape[0]), X]

    # Ensure valid alpha grid
    if not alphas or len(alphas) == 0:
        alphas = (0.0, 0.1, 1.0, 10.0, 100.0, 1000.0)

    # Ensure valid kfolds
    k = max(2, min(kfolds, X1.shape[0]))  # at least 2, at most n
    rng = np.random.RandomState(42)
    splits = _kfold_indices(X1.shape[0], k, rng)

    best_alpha, best_mse = float(alphas[0]), np.inf  # initialize with first alpha
    for alpha in alphas:
        alpha = 0.0 if alpha == 0 else float(alpha)
        mses = []
        for tr, va in splits:
            beta = _ridge_closed_form(X1[tr], y[tr], alpha)
            pred = X1[va] @ beta
            mse = float(np.mean((y[va] - pred) ** 2))
            mses.append(mse)
        avg = float(np.mean(mses))
        if avg < best_mse:
            best_mse, best_alpha = avg, float(alpha)

    # Train final model with best_alpha
    beta = _ridge_closed_form(X1, y, best_alpha)
    model = {"beta": beta.tolist(), "feats": feats, "alpha": float(best_alpha), "use_log": bool(use_log), "cv_mse": float(best_mse)}
    return model


def predict_model(df: pd.DataFrame, model: Dict[str, Any]) -> np.ndarray:
    X = df[model["feats"]].astype(float).values
    if model["use_log"]:
        X = np.log1p(X)
    X1 = np.c_[np.ones(X.shape[0]), X]
    yhat = X1 @ np.array(model["beta"])
    return np.expm1(yhat) if model["use_log"] else yhat


# ==============================
# Rules (use calibrated ratios)
# ==============================
def expected_income_rules(row: pd.Series, ratios: Dict[str, Dict], mins: Dict[str, float]) -> float:
    r = ratios["__overall__"]
    cc_ratio = max(1e-9, r["cc_to_monthly_income_ratio"])
    trb_months = max(1e-9, r["trb_to_monthly_income_months"])
    cc_max = max(0.0, row.get("cc_max_lmt", 0.0) or 0.0)
    cc_tot = max(0.0, row.get("cc_tot_lmt", 0.0) or 0.0)
    trb = max(0.0, row.get("trb_bal_0625_uniq", 0.0) or 0.0)
    monthly = max(cc_max / cc_ratio, trb / trb_months, (cc_tot / cc_ratio) * 0.25)
    seg_floor = float(mins["__overall__"])
    return float(max(monthly * 12.0, 0.8 * seg_floor))


# ==============================
# Flagging
# ==============================
def flag_income(reported: float, expected: float, lower: float, upper: float) -> str:
    if pd.isna(reported) or reported <= 0:
        return "Missing"
    if reported < expected * lower or reported > expected * upper:
        return "Mismatch"
    return "OK"


# ==============================
# Plotting (one figure per chart)
# ==============================
def _save_scatter_with_fit(outdir: str, x: np.ndarray, y: np.ndarray, x_label: str, y_label: str,
                           pred_fn=None, fname: str = "scatter.png"):
    plt.figure()
    plt.scatter(x, y, alpha=0.4)
    if pred_fn is not None and np.isfinite(x).any():
        xs = np.linspace(np.nanpercentile(x, 1), np.nanpercentile(x, 99), 180)
        ys = [pred_fn(xv) for xv in xs]
        plt.plot(xs, ys)
    plt.xlabel(x_label); plt.ylabel(y_label)
    path = os.path.join(outdir, fname)
    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()
    return path


def _save_actual_vs_pred(outdir: str, actual: np.ndarray, pred: np.ndarray, fname: str):
    plt.figure()
    plt.scatter(pred, actual, alpha=0.4)
    lo = np.nanpercentile(np.concatenate([actual, pred]), 1)
    hi = np.nanpercentile(np.concatenate([actual, pred]), 99)
    plt.plot([lo, hi], [lo, hi])
    plt.xlabel("Predicted Income (₹)"); plt.ylabel("Actual Income (₹)")
    path = os.path.join(outdir, fname)
    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()
    return path


def _save_residuals(outdir: str, pred: np.ndarray, resid: np.ndarray, fname_scatter: str, fname_hist: str):
    plt.figure()
    plt.scatter(pred, resid, alpha=0.4)
    plt.axhline(0)
    plt.xlabel("Predicted Income (₹)"); plt.ylabel("Residuals (Actual - Pred)")
    p_sc = os.path.join(outdir, fname_scatter)
    plt.tight_layout(); plt.savefig(p_sc, dpi=140); plt.close()

    plt.figure()
    plt.hist(resid[~np.isnan(resid)], bins=40)
    plt.xlabel("Residuals"); plt.ylabel("Frequency")
    p_hist = os.path.join(outdir, fname_hist)
    plt.tight_layout(); plt.savefig(p_hist, dpi=140); plt.close()
    return p_sc, p_hist


def _save_corr_heatmap(outdir: str, df: pd.DataFrame, cols: List[str], fname: str = "corr_matrix.png"):
    corr = df[cols].corr(method="pearson")
    plt.figure()
    plt.imshow(corr, interpolation="nearest")
    plt.xticks(range(len(cols)), cols, rotation=45, ha="right")
    plt.yticks(range(len(cols)), cols)
    plt.colorbar()
    plt.title("Correlation Matrix")
    path = os.path.join(outdir, fname)
    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()
    return path


# ==============================
# Pipeline
# ==============================
def run_pipeline(
    df: pd.DataFrame,
    q_lo: float,
    q_hi: float,
    use_log_model: bool,
    kfolds: int,
    alpha_grid: tuple,
    mode: str,
    winsorize_for_model: bool,
    winsor_lo: float,
    winsor_hi: float,
    import_calib: Optional[Dict[str, Any]],
    outdir: str
) -> Tuple[pd.DataFrame, Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    if "final_cus_seg" not in df.columns:
        df["final_cus_seg"] = Defaults.segment_label
    df = clean_numeric_cols(df, PROFILE_COLS)

    # 1) Data profile
    profile = profile_vars(df, PROFILE_COLS)

    # 2) Modeling view (optional winsorization)
    model_df = df.copy()
    if winsorize_for_model:
        for c in PROFILE_COLS:
            if c in model_df.columns and model_df[c].notna().any():
                model_df[c] = winsorize_series(model_df[c], winsor_lo, winsor_hi)

    valid_mask = model_df["fin_ann_inc1"].notna() & (model_df["fin_ann_inc1"] > 0)
    df_valid = model_df.loc[valid_mask].copy()
    n_valid = int(df_valid.shape[0])

    if import_calib is not None:
        ratios = import_calib["ratios_by_seg"]
        bands = import_calib["bands_by_seg"]
        mins = import_calib["mins_by_seg"]
        calib_sheets = {"ratios_by_seg": pd.DataFrame(), "bands_by_seg": pd.DataFrame(), "overall": pd.DataFrame()}
    else:
        if n_valid < DEFAULTS.min_valid_incomes:
            raise RuntimeError(
                f"Insufficient valid income records for calibration/training "
                f"({n_valid} found; require >= {DEFAULTS.min_valid_incomes})."
            )
        ratios, bands, mins, calib_sheets = calibrate_from_data(df_valid, q_lo, q_hi)

    # 3) Expected income
    out = model_df.copy()
    model_info: Dict[str, Any]
    if mode == "regression":
        model = fit_ridge_cv(df_valid, use_log=use_log_model, kfolds=kfolds, alphas=alpha_grid)
        out["expected_income"] = predict_model(out, model)
        model_info = {"mode": "regression", **model}
    else:
        out["expected_income"] = [expected_income_rules(r, ratios, mins) for _, r in out.iterrows()]
        model_info = {"mode": "rules", "ratios": ratios["__overall__"]}

    # 4) Bounds & flags
    lower = float(bands["__overall__"]["band_lower"])
    upper = float(bands["__overall__"]["band_upper"])
    out["lower_bound"] = out["expected_income"] * lower
    out["upper_bound"] = out["expected_income"] * upper
    out["income_flag"] = [flag_income(rep, exp, lower, upper)
                          for rep, exp in zip(df["fin_ann_inc1"], out["expected_income"])]
    out["income_gap"] = out["expected_income"] - df["fin_ann_inc1"].fillna(0.0)

    # 5) Reporting frame
    cols_keep = ["cusid", "final_cus_seg", "inc_src",
                 "trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt",
                 "fin_ann_inc1", "expected_income", "lower_bound", "upper_bound",
                 "income_gap", "income_flag"]
    df_out = pd.concat([df, out[["expected_income", "lower_bound", "upper_bound", "income_gap", "income_flag"]]], axis=1)
    df_out = df_out[[c for c in cols_keep if c in df_out.columns]].copy()
    if "cusid" in df_out.columns:
        df_out["cusid"] = df_out["cusid"].astype("string")

    # 6) Summary
    summary = (
        df_out.groupby(["final_cus_seg", "income_flag"], dropna=False)
        .agg(customers=("cusid", "nunique"),
             avg_reported=("fin_ann_inc1", "mean"),
             avg_expected=("expected_income", "mean"),
             total_gap=("income_gap", "sum"))
        .reset_index()
    )

    # 7) Charts
    os.makedirs(outdir, exist_ok=True)
    charts = {}

    if model_info["mode"] == "regression":
        med_trb = float(np.nanmedian(model_df["trb_bal_0625_uniq"])) if "trb_bal_0625_uniq" in model_df else 0.0
        med_cmax = float(np.nanmedian(model_df["cc_max_lmt"])) if "cc_max_lmt" in model_df else 0.0
        med_ctot = float(np.nanmedian(model_df["cc_tot_lmt"])) if "cc_tot_lmt" in model_df else 0.0
        beta = np.array(model_info["beta"]); use_log = model_info["use_log"]

        def pred_with_x(xv, vary: str):
            trb = xv if vary == "trb" else med_trb
            cmax = xv if vary == "cmax" else med_cmax
            ctot = xv if vary == "ctot" else med_ctot
            vec = np.array([1.0, trb, cmax, ctot])
            if use_log:
                vec = np.array([1.0, np.log1p(trb), np.log1p(cmax), np.log1p(ctot)])
                yhat = vec @ beta
                return float(np.expm1(yhat))
            else:
                return float(vec @ beta)

        charts["income_vs_trb"] = _save_scatter_with_fit(
            outdir,
            x=model_df["trb_bal_0625_uniq"].values,
            y=df["fin_ann_inc1"].values,
            x_label="TRB (₹)", y_label="Income (₹)",
            pred_fn=lambda xv: pred_with_x(xv, "trb"),
            fname="income_vs_trb.png"
        )
        charts["income_vs_ccmax"] = _save_scatter_with_fit(
            outdir,
            x=model_df["cc_max_lmt"].values,
            y=df["fin_ann_inc1"].values,
            x_label="CC Max Limit (₹)", y_label="Income (₹)",
            pred_fn=lambda xv: pred_with_x(xv, "cmax"),
            fname="income_vs_ccmax.png"
        )
        charts["income_vs_cctot"] = _save_scatter_with_fit(
            outdir,
            x=model_df["cc_tot_lmt"].values,
            y=df["fin_ann_inc1"].values,
            x_label="CC Total Limit (₹)", y_label="Income (₹)",
            pred_fn=lambda xv: pred_with_x(xv, "ctot"),
            fname="income_vs_cctot.png"
        )
        y_pred = out["expected_income"].values
        y_act = df["fin_ann_inc1"].astype(float).values
        resid = y_act - y_pred
        charts["actual_vs_pred"] = _save_actual_vs_pred(outdir, y_act, y_pred, "actual_vs_pred.png")
        p_sc, p_hist = _save_residuals(outdir, y_pred, resid, "residuals_vs_pred.png", "residual_hist.png")
        charts["resid_vs_pred"] = p_sc
        charts["resid_hist"] = p_hist

    corr_cols = [c for c in ["fin_ann_inc1", "trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt"] if c in df.columns]
    if len(corr_cols) >= 2:
        charts["corr_matrix"] = _save_corr_heatmap(outdir, df, corr_cols, "corr_matrix.png")

    # 8) Write outputs
    out_csv = os.path.join(outdir, "premier_income_flags.csv")
    out_xlsx = os.path.join(outdir, "premier_income_flags.xlsx")
    df_out.to_csv(out_csv, index=False)

    with pd.ExcelWriter(out_xlsx, engine="xlsxwriter") as writer:
        df_out.to_excel(writer, index=False, sheet_name="flags")
        summary.to_excel(writer, index=False, sheet_name="summary")
        profile.to_excel(writer, index=False, sheet_name="data_profile")

        calib = calib_sheets
        if not calib["ratios_by_seg"].empty:
            calib["ratios_by_seg"].to_excel(writer, index=False, sheet_name="calib_ratios")
        if not calib["bands_by_seg"].empty:
            calib["bands_by_seg"].to_excel(writer, index=False, sheet_name="calib_bands")
        if not calib["overall"].empty:
            calib["overall"].to_excel(writer, index=False, sheet_name="calib_overall")

        wb = writer.book
        ws_flags = writer.sheets["flags"]
        num_fmt = wb.add_format({"num_format": "#,##0"})
        text_fmt = wb.add_format({"num_format": "@"})
        money_cols = ["trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt", "fin_ann_inc1",
                      "expected_income", "lower_bound", "upper_bound", "income_gap"]
        for i, col in enumerate(df_out.columns):
            ws_flags.set_column(i, i, max(12, min(28, len(col) + 2)), text_fmt if col == "cusid" else None)
        for col in money_cols:
            if col in df_out.columns:
                j = df_out.columns.get_loc(col)
                ws_flags.set_column(j, j, 16, num_fmt)

        ws_ch = wb.add_worksheet("charts")
        row, col = 1, 1
        for _, path in charts.items():
            if os.path.exists(path):
                ws_ch.insert_image(row, col, path)
                row += 22

    meta = {
        "mode": model_info["mode"],
        "use_log_model": use_log_model,
        "q_lo": q_lo, "q_hi": q_hi,
        "kfolds": kfolds, "alpha_grid": tuple(alpha_grid),
        "winsorized_for_model": winsorize_for_model,
        "winsor_lo": winsor_lo, "winsor_hi": winsor_hi,
        "calibration_rows": int(ratios["__overall__"]["n"])
    }
    with open(os.path.join(outdir, "meta.txt"), "w", encoding="utf-8") as f:
        for k, v in meta.items():
            f.write(f"{k}: {v}\n")

    details = {
        "ratios_by_seg": ratios,
        "bands_by_seg": bands,
        "mins_by_seg": mins,
        "summary_tables": calib_sheets,
        "profile": profile
    }
    files = {"out_csv": out_csv, "out_xlsx": out_xlsx, "charts": charts}
    return df_out, model_info, details, files


# ==============================
# Main (CLI & Spyder)
# ==============================
def main(args_list: Optional[List[str]] = None):
    ap = argparse.ArgumentParser(description="Premier Income Estimator — Production")
    ap.add_argument("--in", dest="in_path", required=True, help="Input file (.xlsx/.xls/.csv/.parquet)")
    ap.add_argument("--outdir", default="./out", help="Output directory")
    ap.add_argument("--mode", choices=["regression", "rules"], default="regression", help="Modeling mode")
    ap.add_argument("--q_lo", type=float, default=DEFAULTS.q_lo, help="Lower quantile for band calibration")
    ap.add_argument("--q_hi", type=float, default=DEFAULTS.q_hi, help="Upper quantile for band calibration")
    ap.add_argument("--kfolds", type=int, default=DEFAULTS.kfolds, help="CV folds for ridge/OLS")
    ap.add_argument("--use_log_model", action="store_true", help="Use log1p regression (default True)")
    ap.add_argument("--no_log_model", action="store_true", help="Disable log1p regression")
    ap.add_argument("--winsorize", action="store_true", help="Winsorize key vars for modeling only")
    ap.add_argument("--winsor_lo", type=float, default=0.025, help="Winsor lower quantile")
    ap.add_argument("--winsor_hi", type=float, default=0.975, help="Winsor upper quantile")
    ap.add_argument("--import_calib", default=None, help="Path to JSON calibration to reuse")
    ap.add_argument("--export_calib", default=None, help="Path to write learned calibration JSON")
    ap.add_argument("-v", "--verbose", action="count", default=1, help="Verbosity: -v (info), -vv (debug)")
    args = ap.parse_args(args_list)

    setup_logging(args.verbose)

    use_log = DEFAULTS.use_log_model
    if args.no_log_model: use_log = False
    if args.use_log_model: use_log = True

    try:
        df = read_input(args.in_path)
    except Exception as e:
        logging.exception("Failed to read input: %s", e)
        sys.exit(2)

    required = ["cusid", "cc_max_lmt", "cc_tot_lmt", "fin_ann_inc1", "inc_src", "trb_bal_0625_uniq"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        logging.error("Missing required columns: %s", missing)
        sys.exit(2)

    import_cal = None
    if args.import_calib:
        try:
            with open(args.import_calib, "r", encoding="utf-8") as f:
                import_cal = json.load(f)
        except Exception as e:
            logging.exception("Failed to import calibration JSON: %s", e)
            sys.exit(2)

    try:
        df_out, model_info, details, files = run_pipeline(
            df=df,
            q_lo=args.q_lo,
            q_hi=args.q_hi,
            use_log_model=use_log,
            kfolds=args.kfolds,
            alpha_grid=DEFAULTS.alpha_grid,  # always non-empty
            mode=args.mode,
            winsorize_for_model=args.winsorize,
            winsor_lo=args.winsor_lo,
            winsor_hi=args.winsor_hi,
            import_calib=import_cal,
            outdir=args.outdir
        )
    except Exception as e:
        logging.exception("Run failed: %s", e)
        sys.exit(2)

    if args.export_calib:
        try:
            payload = {
                "ratios_by_seg": details["ratios_by_seg"],
                "bands_by_seg": details["bands_by_seg"],
                "mins_by_seg": details["mins_by_seg"],
                "meta": {"source_file": os.path.basename(args.in_path),
                         "q_lo": args.q_lo, "q_hi": args.q_hi, "kfolds": args.kfolds}
            }
            with open(args.export_calib, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.exception("Failed to export calibration JSON: %s", e)
            sys.exit(2)

    logging.info("Output CSV: %s", files["out_csv"])
    logging.info("Output XLSX: %s", files["out_xlsx"])
    logging.info("Charts saved to: %s", args.outdir)


if __name__ == "__main__":
    # ---- Terminal usage (normal) ----
    # main()  # uncomment to use with real CLI args

    # ---- Spyder usage (simulate CLI here) ----
    # main([
    #     "--in", r"C:\path\to\mydata.xlsx",
    #     "--outdir", "./out",
    #     "--mode", "regression",
    #     # "--winsorize",
    # ])
    pass


































#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Premier Income Estimator — Production (Data-Driven, Profiling, Regression, Charts)
==================================================================================

What this script guarantees (fact-based, no guesses):
- Reads your live data (Excel/CSV/Parquet) and FIRST profiles actual ranges
  (min/max and P10/P50/P90) for: TRB, CC max limit, CC total limit, income.
- Calibrates expected-income bands strictly from your data (quantiles of
  reported/expected), not from assumptions.
- Trains a regression model to predict income from TRB + credit limits:
  Ridge with K-fold CV over an alpha grid (alpha=0 equals OLS).
- Flags each record: Missing / Mismatch / OK using calibrated bands.
- Exports: CSV + Excel (flags, summary, data_profile, calib tables, charts).
- Robust to outliers via optional winsorization (for modeling only).
- Spyder-friendly: main(args_list=None) so you can pass an argument list.

Required columns (any case; underscores/spaces ok, they are normalized):
  - cusid
  - cc_max_lmt            (max credit limit)
  - cc_tot_lmt            (sum of credit limits)
  - fin_ann_inc1          (reported annual income)
  - inc_src               (income source)
  - trb_bal_0625_uniq     (TRB)
  - final_cus_seg         (your data is Premier-only; fine)

CLI examples:
-------------
# Regression + calibrated bands + charts
python premier_income_estimator_final.py --in "mydata.xlsx" --outdir ./out --mode regression

# Rules-only (no regression), winsorize inputs for modeling robustness
python premier_income_estimator_final.py --in "mydata.csv" --outdir ./out_rules \
  --mode rules --winsorize --winsor_lo 0.025 --winsor_hi 0.975

# Freeze and reuse calibration (reproducible month-over-month)
python premier_income_estimator_final.py --in "this_month.csv" --outdir ./out \
  --mode rules --export_calib calib.json
python premier_income_estimator_final.py --in "next_month.csv" --outdir ./out \
  --mode rules --import_calib calib.json
"""
from __future__ import annotations

import argparse
import json
import logging
import os
import sys
from dataclasses import dataclass
from typing import Dict, Tuple, Optional, Any, List

import numpy as np
import pandas as pd

# Matplotlib (no seaborn, single-plot figures, no custom styles)
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt


# ==============================
# Config & Logging
# ==============================
@dataclass(frozen=True)
class Defaults:
    q_lo: float = 0.10                 # lower quantile for mismatch band calibration
    q_hi: float = 0.90                 # upper quantile for mismatch band calibration
    kfolds: int = 5                    # CV folds for ridge/OLS
    alpha_grid: tuple = (0.0, 0.1, 1.0, 10.0, 100.0, 1000.0)  # 0.0 -> OLS
    min_valid_incomes: int = 50        # minimum labeled rows required to calibrate/train
    use_log_model: bool = True         # log1p modeling (stable on skew)
    segment_label: str = "Premier"     # default label if final_cus_seg missing


DEFAULTS = Defaults()


def setup_logging(verbosity: int = 1):
    level = logging.WARNING
    if verbosity >= 2:
        level = logging.INFO
    if verbosity >= 3:
        level = logging.DEBUG
    logging.basicConfig(
        format="%(asctime)s | %(levelname)s | %(message)s",
        level=level,
        datefmt="%Y-%m-%d %H:%M:%S",
    )


# ==============================
# IO & Cleaning
# ==============================
def _to_num(x: Any) -> float:
    if x is None:
        return np.nan
    try:
        return float(x)
    except Exception:
        try:
            return float(str(x).replace(",", "").replace("₹", "").strip())
        except Exception:
            return np.nan


def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    # lower-case, strip, replace spaces with underscores
    df = df.copy()
    df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]
    # allow a few common variants
    rename = {
        "cc_max_limit": "cc_max_lmt",
        "cc_total_limit": "cc_tot_lmt",
        "trb": "trb_bal_0625_uniq",
        "income": "fin_ann_inc1",
    }
    df.rename(columns=rename, inplace=True)
    return df


def read_input(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path)[1].lower()
    if ext in (".xlsx", ".xls"):
        df = pd.read_excel(path, dtype={"cusid": "string"})
    elif ext in (".parquet", ".pq"):
        df = pd.read_parquet(path)
        if "cusid" in df.columns:
            df["cusid"] = df["cusid"].astype("string")
    else:
        df = pd.read_csv(path, dtype={"cusid": "string"})
    df = standardize_columns(df)
    if "cusid" in df.columns:
        df["cusid"] = df["cusid"].astype("string")
    return df


def clean_numeric_cols(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    df = df.copy()
    for c in cols:
        if c in df.columns:
            df[c] = df[c].apply(_to_num)
    return df


# ==============================
# Data Profiling (facts from live data)
# ==============================
PROFILE_COLS = ["cc_max_lmt", "cc_tot_lmt", "trb_bal_0625_uniq", "fin_ann_inc1"]

def profile_vars(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    rows = []
    for c in cols:
        if c not in df.columns:
            continue
        s = pd.to_numeric(df[c], errors="coerce")
        rows.append({
            "variable": c,
            "non_null": int(s.notna().sum()),
            "min": float(np.nanmin(s)) if s.notna().any() else np.nan,
            "p10": float(np.nanquantile(s, 0.10)) if s.notna().any() else np.nan,
            "p50": float(np.nanmedian(s)) if s.notna().any() else np.nan,
            "p90": float(np.nanquantile(s, 0.90)) if s.notna().any() else np.nan,
            "max": float(np.nanmax(s)) if s.notna().any() else np.nan,
        })
    return pd.DataFrame(rows)


def winsorize_series(s: pd.Series, lo_q: float, hi_q: float) -> pd.Series:
    s = s.astype(float)
    lo = np.nanquantile(s, lo_q)
    hi = np.nanquantile(s, hi_q)
    return s.clip(lower=lo, upper=hi)


# ==============================
# Calibration (purely data-driven)
# ==============================
def calibrate_from_data(df_valid: pd.DataFrame, q_lo: float, q_hi: float):
    """
    Uses ONLY rows with valid income to learn:
      - Ratios for rules: median(cc_max / monthly_income), median(TRB / monthly_income)
      - Segment minimum (floor) from median annual income (data-driven)
      - Bands from quantiles of (reported / expected_rules)
    """
    work = df_valid.copy()
    work["monthly_income"] = work["fin_ann_inc1"] / 12.0
    work = work[work["monthly_income"] > 0].copy()

    work["r_cc"] = work["cc_max_lmt"] / work["monthly_income"]
    work["r_trb"] = work["trb_bal_0625_uniq"] / work["monthly_income"]
    work.replace([np.inf, -np.inf], np.nan, inplace=True)

    # We keep a single "__overall__" entry since your data is Premier-only.
    cc_to_monthly = float(np.nanmedian(work["r_cc"].values))
    trb_to_months = float(np.nanmedian(work["r_trb"].values))
    seg_min_annual = float(np.nanmedian(work["monthly_income"].values) * 12.0 * 0.9)

    ratios = {"__overall__": {
        "cc_to_monthly_income_ratio": cc_to_monthly,
        "trb_to_monthly_income_months": trb_to_months,
        "n": int(work.shape[0])
    }}
    mins = {"__overall__": seg_min_annual}

    # Preliminary expected via rules to compute mismatch bands
    def expected_rules_row(row):
        cc_ratio = max(1e-9, cc_to_monthly)
        trb_months = max(1e-9, trb_to_months)
        cc_max = max(0.0, row.get("cc_max_lmt", 0.0) or 0.0)
        cc_tot = max(0.0, row.get("cc_tot_lmt", 0.0) or 0.0)
        trb = max(0.0, row.get("trb_bal_0625_uniq", 0.0) or 0.0)
        monthly = max(cc_max / cc_ratio, trb / trb_months, (cc_tot / cc_ratio) * 0.25)
        return monthly * 12.0

    work["expected_rules"] = [expected_rules_row(r) for _, r in work.iterrows()]
    eps = 1e-9
    rep_to_exp = work["fin_ann_inc1"] / (work["expected_rules"] + eps)
    band_lower = float(np.nanquantile(rep_to_exp.values, q_lo))
    band_upper = float(np.nanquantile(rep_to_exp.values, q_hi))
    bands = {"__overall__": {"band_lower": max(0.4, band_lower),
                             "band_upper": min(2.5, band_upper),
                             "n": int(work.shape[0])}}

    # Summary tables
    calib_ratios = pd.DataFrame([{
        "final_cus_seg": "Premier",
        "n": work.shape[0],
        "cc_to_monthly_income_ratio": cc_to_monthly,
        "trb_to_monthly_income_months": trb_to_months,
        "annual_income_median": float(np.nanmedian(work["monthly_income"]) * 12.0)
    }])
    calib_bands = pd.DataFrame([{
        "final_cus_seg": "Premier",
        "q_lo": band_lower, "q_hi": band_upper, "n": work.shape[0]
    }])
    calib_overall = pd.DataFrame({
        "metric": ["cc_to_monthly_income_ratio", "trb_to_monthly_income_months",
                   "band_lower", "band_upper", "n"],
        "value": [cc_to_monthly, trb_to_months, bands["__overall__"]["band_lower"],
                  bands["__overall__"]["band_upper"], work.shape[0]]
    })

    summary_tbls = {
        "ratios_by_seg": calib_ratios,
        "bands_by_seg": calib_bands,
        "overall": calib_overall
    }
    return ratios, bands, mins, summary_tbls


# ==============================
# Modeling (OLS/Ridge via closed-form + CV)
# ==============================
def _ridge_closed_form(X: np.ndarray, y: np.ndarray, alpha: float) -> np.ndarray:
    n_features = X.shape[1]
    I = np.eye(n_features)
    # do not penalize intercept (first column of ones)
    I[0, 0] = 0.0
    XtX = X.T @ X
    Xty = X.T @ y
    beta = np.linalg.solve(XtX + alpha * I, Xty)
    return beta


def _kfold_indices(n: int, k: int, rng: np.random.RandomState) -> List[Tuple[np.ndarray, np.ndarray]]:
    idx = np.arange(n)
    rng.shuffle(idx)
    folds = np.array_split(idx, k)
    pairs = []
    for i in range(k):
        val = folds[i]
        train = np.hstack([folds[j] for j in range(k) if j != i])
        pairs.append((train, val))
    return pairs


def fit_ridge_cv(df_valid: pd.DataFrame, use_log: bool, kfolds: int, alphas: tuple):
    feats = ["trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt"]
    X = df_valid[feats].astype(float).values
    y = df_valid["fin_ann_inc1"].astype(float).values
    if use_log:
        X = np.log1p(X); y = np.log1p(y)
    # add intercept
    X1 = np.c_[np.ones(X.shape[0]), X]

    rng = np.random.RandomState(42)
    splits = _kfold_indices(X1.shape[0], max(2, min(kfolds, X1.shape[0])), rng)

    best_alpha, best_mse = None, np.inf
    for alpha in alphas:
        mses = []
        for tr, va in splits:
            beta = _ridge_closed_form(X1[tr], y[tr], alpha)
            pred = X1[va] @ beta
            if use_log:
                pred = pred  # still in log space for MSE
                mse = float(np.mean((y[va] - pred) ** 2))
            else:
                mse = float(np.mean((y[va] - pred) ** 2))
            mses.append(mse)
        avg = float(np.mean(mses))
        if avg < best_mse:
            best_mse, best_alpha = avg, alpha

    # fit on full data with best alpha
    beta = _ridge_closed_form(X1, y, best_alpha)
    model = {
        "beta": beta.tolist(),
        "feats": feats,
        "alpha": float(best_alpha),
        "use_log": bool(use_log),
        "cv_mse": float(best_mse),
    }
    return model


def predict_model(df: pd.DataFrame, model: Dict[str, Any]) -> np.ndarray:
    X = df[model["feats"]].astype(float).values
    if model["use_log"]:
        X = np.log1p(X)
    X1 = np.c_[np.ones(X.shape[0]), X]
    yhat = X1 @ np.array(model["beta"])
    return np.expm1(yhat) if model["use_log"] else yhat


# ==============================
# Rules (use calibrated ratios)
# ==============================
def expected_income_rules(row: pd.Series, ratios: Dict[str, Dict], mins: Dict[str, float]) -> float:
    r = ratios["__overall__"]
    cc_ratio = max(1e-9, r["cc_to_monthly_income_ratio"])
    trb_months = max(1e-9, r["trb_to_monthly_income_months"])
    cc_max = max(0.0, row.get("cc_max_lmt", 0.0) or 0.0)
    cc_tot = max(0.0, row.get("cc_tot_lmt", 0.0) or 0.0)
    trb = max(0.0, row.get("trb_bal_0625_uniq", 0.0) or 0.0)
    monthly = max(cc_max / cc_ratio, trb / trb_months, (cc_tot / cc_ratio) * 0.25)
    # Floor relative to segment median (data-driven)
    seg_floor = float(mins["__overall__"])
    return float(max(monthly * 12.0, 0.8 * seg_floor))


# ==============================
# Flagging
# ==============================
def flag_income(reported: float, expected: float, lower: float, upper: float) -> str:
    if pd.isna(reported) or reported <= 0:
        return "Missing"
    if reported < expected * lower or reported > expected * upper:
        return "Mismatch"
    return "OK"


# ==============================
# Plotting (one figure per chart)
# ==============================
def _save_scatter_with_fit(outdir: str, x: np.ndarray, y: np.ndarray, x_label: str, y_label: str,
                           pred_fn=None, fname: str = "scatter.png"):
    plt.figure()
    plt.scatter(x, y, alpha=0.4)
    if pred_fn is not None and np.isfinite(x).any():
        xs = np.linspace(np.nanpercentile(x, 1), np.nanpercentile(x, 99), 180)
        ys = [pred_fn(xv) for xv in xs]
        plt.plot(xs, ys)
    plt.xlabel(x_label); plt.ylabel(y_label)
    path = os.path.join(outdir, fname)
    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()
    return path


def _save_actual_vs_pred(outdir: str, actual: np.ndarray, pred: np.ndarray, fname: str):
    plt.figure()
    plt.scatter(pred, actual, alpha=0.4)
    lo = np.nanpercentile(np.concatenate([actual, pred]), 1)
    hi = np.nanpercentile(np.concatenate([actual, pred]), 99)
    plt.plot([lo, hi], [lo, hi])
    plt.xlabel("Predicted Income (₹)"); plt.ylabel("Actual Income (₹)")
    path = os.path.join(outdir, fname)
    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()
    return path


def _save_residuals(outdir: str, pred: np.ndarray, resid: np.ndarray, fname_scatter: str, fname_hist: str):
    # scatter
    plt.figure()
    plt.scatter(pred, resid, alpha=0.4)
    plt.axhline(0)
    plt.xlabel("Predicted Income (₹)"); plt.ylabel("Residuals (Actual - Pred)")
    p1 = os.path.join(outdir, fname_scatter)
    plt.tight_layout(); plt.savefig(p1, dpi=140); plt.close()
    # hist
    plt.figure()
    plt.hist(resid[~np.isnan(resid)], bins=40)
    plt.xlabel("Residuals"); plt.ylabel("Frequency")
    p2 = os.path.join(outdir, fname_hist)
    plt.tight_layout(); plt.savefig(p2, dpi=140); plt.close()
    return p1, p2


def _save_corr_heatmap(outdir: str, df: pd.DataFrame, cols: List[str], fname: str = "corr_matrix.png"):
    corr = df[cols].corr(method="pearson")
    plt.figure()
    plt.imshow(corr, interpolation="nearest")
    plt.xticks(range(len(cols)), cols, rotation=45, ha="right")
    plt.yticks(range(len(cols)), cols)
    plt.colorbar()
    plt.title("Correlation Matrix")
    path = os.path.join(outdir, fname)
    plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()
    return path


# ==============================
# Pipeline
# ==============================
def run_pipeline(
    df: pd.DataFrame,
    q_lo: float,
    q_hi: float,
    use_log_model: bool,
    kfolds: int,
    alpha_grid: tuple,
    mode: str,
    winsorize_for_model: bool,
    winsor_lo: float,
    winsor_hi: float,
    import_calib: Optional[Dict[str, Any]],
    outdir: str
) -> Tuple[pd.DataFrame, Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    # Normalize / clean
    if "final_cus_seg" not in df.columns:
        df["final_cus_seg"] = Defaults.segment_label
    df = clean_numeric_cols(df, PROFILE_COLS)

    # 1) Data profile (audit facts)
    profile = profile_vars(df, PROFILE_COLS)

    # 2) Build modeling view (optional winsorization for robustness)
    model_df = df.copy()
    if winsorize_for_model:
        for c in PROFILE_COLS:
            if c in model_df.columns and model_df[c].notna().any():
                model_df[c] = winsorize_series(model_df[c], winsor_lo, winsor_hi)

    # Valid rows for calibration/training
    valid_mask = model_df["fin_ann_inc1"].notna() & (model_df["fin_ann_inc1"] > 0)
    df_valid = model_df.loc[valid_mask].copy()
    n_valid = int(df_valid.shape[0])

    if import_calib is not None:
        ratios = import_calib["ratios_by_seg"]
        bands = import_calib["bands_by_seg"]
        mins = import_calib["mins_by_seg"]
        calib_sheets = {"ratios_by_seg": pd.DataFrame(), "bands_by_seg": pd.DataFrame(), "overall": pd.DataFrame()}
    else:
        if n_valid < DEFAULTS.min_valid_incomes:
            raise RuntimeError(
                f"Insufficient valid income records for calibration/training "
                f"({n_valid} found; require >= {DEFAULTS.min_valid_incomes}). "
                f"Provide more labeled rows or import a prior calibration."
            )
        ratios, bands, mins, calib_sheets = calibrate_from_data(df_valid, q_lo, q_hi)

    # 3) Expected income
    out = model_df.copy()
    model_info: Dict[str, Any]
    if mode == "regression":
        model = fit_ridge_cv(df_valid, use_log=use_log_model, kfolds=kfolds, alphas=alpha_grid)
        out["expected_income"] = predict_model(out, model)
        model_info = {"mode": "regression", **model}
    else:
        out["expected_income"] = [expected_income_rules(r, ratios, mins) for _, r in out.iterrows()]
        model_info = {"mode": "rules", "ratios": ratios["__overall__"]}

    # 4) Bounds & flags
    lower = float(bands["__overall__"]["band_lower"])
    upper = float(bands["__overall__"]["band_upper"])
    out["lower_bound"] = out["expected_income"] * lower
    out["upper_bound"] = out["expected_income"] * upper
    out["income_flag"] = [flag_income(rep, exp, lower, upper)
                          for rep, exp in zip(df["fin_ann_inc1"], out["expected_income"])]
    out["income_gap"] = out["expected_income"] - df["fin_ann_inc1"].fillna(0.0)

    # 5) Assemble reporting frame (raw inputs + model outputs)
    cols_keep = ["cusid", "final_cus_seg", "inc_src",
                 "trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt",
                 "fin_ann_inc1", "expected_income", "lower_bound", "upper_bound",
                 "income_gap", "income_flag"]
    df_out = pd.concat([df, out[["expected_income", "lower_bound", "upper_bound", "income_gap", "income_flag"]]], axis=1)
    df_out = df_out[[c for c in cols_keep if c in df_out.columns]].copy()
    if "cusid" in df_out.columns:
        df_out["cusid"] = df_out["cusid"].astype("string")

    # 6) Summary table
    summary = (
        df_out.groupby(["final_cus_seg", "income_flag"], dropna=False)
        .agg(customers=("cusid", "nunique"),
             avg_reported=("fin_ann_inc1", "mean"),
             avg_expected=("expected_income", "mean"),
             total_gap=("income_gap", "sum"))
        .reset_index()
    )

    # 7) Charts directory
    os.makedirs(outdir, exist_ok=True)
    charts = {}

    # Regression visuals (only if regression mode)
    if model_info["mode"] == "regression":
        # Partial lines: vary one feature, hold others at medians of modeling view
        med_trb = float(np.nanmedian(model_df["trb_bal_0625_uniq"])) if "trb_bal_0625_uniq" in model_df else 0.0
        med_cmax = float(np.nanmedian(model_df["cc_max_lmt"])) if "cc_max_lmt" in model_df else 0.0
        med_ctot = float(np.nanmedian(model_df["cc_tot_lmt"])) if "cc_tot_lmt" in model_df else 0.0
        beta = np.array(model_info["beta"]); use_log = model_info["use_log"]

        def pred_with_x(xv, vary: str):
            trb = xv if vary == "trb" else med_trb
            cmax = xv if vary == "cmax" else med_cmax
            ctot = xv if vary == "ctot" else med_ctot
            vec = np.array([1.0, trb, cmax, ctot])
            if use_log:
                vec = np.array([1.0, np.log1p(trb), np.log1p(cmax), np.log1p(ctot)])
                yhat = vec @ beta
                return float(np.expm1(yhat))
            else:
                return float(vec @ beta)

        charts["income_vs_trb"] = _save_scatter_with_fit(
            outdir,
            x=model_df["trb_bal_0625_uniq"].values,
            y=df["fin_ann_inc1"].values,
            x_label="TRB (₹)", y_label="Income (₹)",
            pred_fn=lambda xv: pred_with_x(xv, "trb"),
            fname="income_vs_trb.png"
        )
        charts["income_vs_ccmax"] = _save_scatter_with_fit(
            outdir,
            x=model_df["cc_max_lmt"].values,
            y=df["fin_ann_inc1"].values,
            x_label="CC Max Limit (₹)", y_label="Income (₹)",
            pred_fn=lambda xv: pred_with_x(xv, "cmax"),
            fname="income_vs_ccmax.png"
        )
        charts["income_vs_cctot"] = _save_scatter_with_fit(
            outdir,
            x=model_df["cc_tot_lmt"].values,
            y=df["fin_ann_inc1"].values,
            x_label="CC Total Limit (₹)", y_label="Income (₹)",
            pred_fn=lambda xv: pred_with_x(xv, "ctot"),
            fname="income_vs_cctot.png"
        )
        y_pred = out["expected_income"].values
        y_act = df["fin_ann_inc1"].astype(float).values
        resid = y_act - y_pred
        charts["actual_vs_pred"] = _save_actual_vs_pred(outdir, y_act, y_pred, "actual_vs_pred.png")
        p_sc, p_hist = _save_residuals(outdir, y_pred, resid, "residuals_vs_pred.png", "residual_hist.png")
        charts["resid_vs_pred"] = p_sc
        charts["resid_hist"] = p_hist

    # Correlation heatmap (raw, always)
    corr_cols = [c for c in ["fin_ann_inc1", "trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt"] if c in df.columns]
    if len(corr_cols) >= 2:
        charts["corr_matrix"] = _save_corr_heatmap(outdir, df, corr_cols, "corr_matrix.png")

    # 8) Write outputs
    out_csv = os.path.join(outdir, "premier_income_flags.csv")
    out_xlsx = os.path.join(outdir, "premier_income_flags.xlsx")
    df_out.to_csv(out_csv, index=False)

    with pd.ExcelWriter(out_xlsx, engine="xlsxwriter") as writer:
        df_out.to_excel(writer, index=False, sheet_name="flags")
        summary.to_excel(writer, index=False, sheet_name="summary")
        profile.to_excel(writer, index=False, sheet_name="data_profile")
        # Calibration tables
        calib = calib_sheets
        if not calib["ratios_by_seg"].empty:
            calib["ratios_by_seg"].to_excel(writer, index=False, sheet_name="calib_ratios")
        if not calib["bands_by_seg"].empty:
            calib["bands_by_seg"].to_excel(writer, index=False, sheet_name="calib_bands")
        if not calib["overall"].empty:
            calib["overall"].to_excel(writer, index=False, sheet_name="calib_overall")

        # Formatting
        wb = writer.book
        ws_flags = writer.sheets["flags"]
        num_fmt = wb.add_format({"num_format": "#,##0"})
        text_fmt = wb.add_format({"num_format": "@"})
        money_cols = ["trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt", "fin_ann_inc1",
                      "expected_income", "lower_bound", "upper_bound", "income_gap"]
        for i, col in enumerate(df_out.columns):
            ws_flags.set_column(i, i, max(12, min(28, len(col) + 2)), text_fmt if col == "cusid" else None)
        for col in money_cols:
            if col in df_out.columns:
                j = df_out.columns.get_loc(col)
                ws_flags.set_column(j, j, 16, num_fmt)

        # Charts
        ws_ch = wb.add_worksheet("charts")
        row, col = 1, 1
        for _, path in charts.items():
            if os.path.exists(path):
                ws_ch.insert_image(row, col, path)
                row += 22

    meta = {
        "mode": model_info["mode"],
        "use_log_model": use_log_model,
        "q_lo": q_lo, "q_hi": q_hi,
        "kfolds": kfolds, "alpha_grid": alpha_grid,
        "winsorized_for_model": winsorize_for_model,
        "winsor_lo": winsor_lo, "winsor_hi": winsor_hi,
        "calibration_rows": int(ratios["__overall__"]["n"])
    }
    with open(os.path.join(outdir, "meta.txt"), "w", encoding="utf-8") as f:
        for k, v in meta.items():
            f.write(f"{k}: {v}\n")

    details = {
        "ratios_by_seg": ratios,
        "bands_by_seg": bands,
        "mins_by_seg": mins,
        "summary_tables": calib_sheets,
        "profile": profile
    }
    files = {"out_csv": out_csv, "out_xlsx": out_xlsx, "charts": charts}
    return df_out, model_info, details, files


# ==============================
# Main (CLI & Spyder)
# ==============================
def main(args_list: Optional[List[str]] = None):
    ap = argparse.ArgumentParser(description="Premier Income Estimator — Production")
    ap.add_argument("--in", dest="in_path", required=True, help="Input file (.xlsx/.xls/.csv/.parquet)")
    ap.add_argument("--outdir", default="./out", help="Output directory")
    ap.add_argument("--mode", choices=["regression", "rules"], default="regression", help="Modeling mode")
    ap.add_argument("--q_lo", type=float, default=DEFAULTS.q_lo, help="Lower quantile for band calibration")
    ap.add_argument("--q_hi", type=float, default=DEFAULTS.q_hi, help="Upper quantile for band calibration")
    ap.add_argument("--kfolds", type=int, default=DEFAULTS.kfolds, help="CV folds for ridge/OLS")
    ap.add_argument("--use_log_model", action="store_true", help="Use log1p regression (default True)")
    ap.add_argument("--no_log_model", action="store_true", help="Disable log1p regression")
    ap.add_argument("--winsorize", action="store_true", help="Winsorize key vars for modeling only")
    ap.add_argument("--winsor_lo", type=float, default=0.025, help="Winsor lower quantile")
    ap.add_argument("--winsor_hi", type=float, default=0.975, help="Winsor upper quantile")
    ap.add_argument("--import_calib", default=None, help="Path to JSON calibration to reuse")
    ap.add_argument("--export_calib", default=None, help="Path to write learned calibration JSON")
    ap.add_argument("-v", "--verbose", action="count", default=1, help="Verbosity: -v (info), -vv (debug)")
    args = ap.parse_args(args_list)

    setup_logging(args.verbose)

    use_log = DEFAULTS.use_log_model
    if args.no_log_model: use_log = False
    if args.use_log_model: use_log = True

    try:
        df = read_input(args.in_path)
    except Exception as e:
        logging.exception("Failed to read input: %s", e)
        sys.exit(2)

    required = ["cusid", "cc_max_lmt", "cc_tot_lmt", "fin_ann_inc1", "inc_src", "trb_bal_0625_uniq"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        logging.error("Missing required columns: %s", missing)
        sys.exit(2)

    # Import calibration if provided
    import_cal = None
    if args.import_calib:
        try:
            with open(args.import_calib, "r", encoding="utf-8") as f:
                import_cal = json.load(f)
        except Exception as e:
            logging.exception("Failed to import calibration JSON: %s", e)
            sys.exit(2)

    try:
        df_out, model_info, details, files = run_pipeline(
            df=df,
            q_lo=args.q_lo,
            q_hi=args.q_hi,
            use_log_model=use_log,
            kfolds=args.kfolds,
            alpha_grid=DEFAULTS.alpha_grid,
            mode=args.mode,
            winsorize_for_model=args.winsorize,
            winsor_lo=args.winsor_lo,
            winsor_hi=args.winsor_hi,
            import_calib=import_cal,
            outdir=args.outdir
        )
    except Exception as e:
        logging.exception("Run failed: %s", e)
        sys.exit(2)

    # Export calibration if requested
    if args.export_calib:
        try:
            payload = {
                "ratios_by_seg": details["ratios_by_seg"],
                "bands_by_seg": details["bands_by_seg"],
                "mins_by_seg": details["mins_by_seg"],
                "meta": {
                    "source_file": os.path.basename(args.in_path),
                    "q_lo": args.q_lo, "q_hi": args.q_hi,
                    "kfolds": args.kfolds
                }
            }
            with open(args.export_calib, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.exception("Failed to export calibration JSON: %s", e)
            sys.exit(2)

    logging.info("Output CSV: %s", files["out_csv"])
    logging.info("Output XLSX: %s", files["out_xlsx"])
    logging.info("Charts saved to: %s", args.outdir)


if __name__ == "__main__":
    # ---- Terminal usage (normal) ----
    # main()  # uncomment to use with real CLI args

    # ---- Spyder usage (simulate CLI here) ----
    # Provide your paths below, then press Run in Spyder:
    # main([
    #     "--in", r"C:\path\to\mydata.xlsx",
    #     "--outdir", "./out",
    #     "--mode", "regression",
    #     # "--winsorize",
    # ])
    pass


































#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Premier Income Estimator — Auto‑Calibrating + Profiling + Charts (Production)
=============================================================================

What it does (no hardcoding):
1) Profiles live data for key vars (min/max/P10/P50/P90).
2) Calibrates ratios/bands from your data (medians/quantiles).
3) Estimates expected income (regression or rules).
4) Flags Missing/Low/Mismatch/OK.
5) Exports Excel with tabs: flags, summary, data_profile, calib_*.
6) Generates & embeds charts (matplotlib only).

Required columns (CSV/Parquet):
- cusid, cc_max_lmt, cc_tot_lmt, fin_ann_inc1, inc_src, trb_bal_0625_uniq, final_cus_seg
(Note: you said data is Premier‑only; script works without filtering.)

Quick start:
-----------
# Calibrate+regress, lock per‑segment bands if sample ≥100, embed charts
python premier_income_estimator_pro.py --in data.csv --outdir ./out \
  --mode regression --lock_segment_bands

# Use rules only, winsorize inputs at profile P2.5/P97.5 for modeling
python premier_income_estimator_pro.py --in data.csv --outdir ./out_rules \
  --mode rules --winsorize --winsor_lo 0.025 --winsor_hi 0.975

# Reuse prior calibration (reproducible month-over-month)
python premier_income_estimator_pro.py --in next.csv --outdir ./out_fix \
  --mode rules --import_calib calib.json
"""
from __future__ import annotations

import argparse
import json
import logging
import os
import sys
from dataclasses import dataclass
from typing import Dict, Tuple, Optional, Any, List

import numpy as np
import pandas as pd

# Use non-interactive backend for servers
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt


# ==============================
# Configuration defaults
# ==============================
@dataclass(frozen=True)
class Defaults:
    segment: str = "Premier"
    min_income_annual: float = 1_800_000.0   # ₹/year (base floor, not hard threshold for profiling)
    band_lower: float = 0.65
    band_upper: float = 1.75
    q_lo: float = 0.10
    q_hi: float = 0.90
    min_records_per_segment: int = 100
    use_log_model: bool = True


DEFAULTS = Defaults()


# ==============================
# Logging
# ==============================
def setup_logging(verbosity: int = 1):
    level = logging.WARNING
    if verbosity >= 2:
        level = logging.INFO
    if verbosity >= 3:
        level = logging.DEBUG
    logging.basicConfig(
        format="%(asctime)s | %(levelname)s | %(message)s",
        level=level,
        datefmt="%Y-%m-%d %H:%M:%S",
    )


# ==============================
# Utilities
# ==============================
def _to_num(x: Any) -> float:
    if x is None:
        return np.nan
    try:
        return float(x)
    except Exception:
        try:
            return float(str(x).replace(",", "").replace("₹", "").strip())
        except Exception:
            return np.nan


def clean_numeric_cols(df: pd.DataFrame, cols) -> pd.DataFrame:
    for c in cols:
        if c in df.columns:
            df[c] = df[c].apply(_to_num)
    return df


def read_input(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path)[1].lower()
    if ext in [".parquet", ".pq"]:
        df = pd.read_parquet(path)
    else:
        df = pd.read_csv(path, dtype={"cusid": "string"})
    if "cusid" in df.columns:
        df["cusid"] = df["cusid"].astype("string")
    return df


# ==============================
# Data profiling (no assumptions)
# ==============================
PROFILE_COLS = ["cc_max_lmt", "cc_tot_lmt", "trb_bal_0625_uniq", "fin_ann_inc1"]

def profile_vars(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
    out = []
    for c in cols:
        if c not in df.columns:
            continue
        s = pd.to_numeric(df[c], errors="coerce")
        if s.dropna().empty:
            out.append({"variable": c, "min": np.nan, "p10": np.nan, "p50": np.nan,
                        "p90": np.nan, "max": np.nan, "non_null": 0})
            continue
        out.append({
            "variable": c,
            "min": float(np.nanmin(s)),
            "p10": float(np.nanquantile(s, 0.10)),
            "p50": float(np.nanmedian(s)),
            "p90": float(np.nanquantile(s, 0.90)),
            "max": float(np.nanmax(s)),
            "non_null": int(s.notna().sum())
        })
    return pd.DataFrame(out)


def winsorize_series(s: pd.Series, lo_q: float, hi_q: float) -> pd.Series:
    s = s.astype(float)
    lo = np.nanquantile(s, lo_q)
    hi = np.nanquantile(s, hi_q)
    return s.clip(lower=lo, upper=hi)


# ==============================
# Rule-based expected income
# ==============================
def expected_income_rules(row: pd.Series, ratios: Dict, mins: Dict, seg: str) -> float:
    seg_rat = ratios.get(seg, ratios.get("__overall__", {}))
    cc_ratio = max(1e-9, seg_rat.get("cc_to_monthly_income_ratio", 1.5))
    trb_months = max(1e-9, seg_rat.get("trb_to_monthly_income_months", 3.0))
    min_income = float(mins.get(seg, mins.get("__overall__", DEFAULTS.min_income_annual)))

    cc_max = max(0.0, row.get("cc_max_lmt", 0.0) or 0.0)
    cc_tot = max(0.0, row.get("cc_tot_lmt", 0.0) or 0.0)
    trb = max(0.0, row.get("trb_bal_0625_uniq", 0.0) or 0.0)

    monthly_from_cc = cc_max / cc_ratio
    monthly_from_trb = trb / trb_months
    monthly_from_cc_tot = (cc_tot / cc_ratio) * 0.25

    monthly_est = max(monthly_from_cc, monthly_from_trb, monthly_from_cc_tot)
    annual_est = monthly_est * 12.0
    return float(max(annual_est, 0.8 * min_income))


# ==============================
# Regression expected income
# ==============================
def fit_regression(df_train: pd.DataFrame, use_log: bool = True):
    feats = ["trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt"]
    X = df_train[feats].astype(float).values
    y = df_train["fin_ann_inc1"].astype(float).values
    if use_log:
        X = np.log1p(X)
        y = np.log1p(y)
    X_ = np.c_[np.ones(X.shape[0]), X]
    beta, *_ = np.linalg.lstsq(X_, y, rcond=None)
    y_hat = X_.dot(beta)
    resid = y - y_hat
    sigma = float(np.sqrt(np.mean(resid ** 2)))
    return beta, sigma, feats


def predict_regression(df: pd.DataFrame, beta, feats, use_log: bool = True) -> np.ndarray:
    X = df[feats].astype(float).values
    if use_log:
        X = np.log1p(X)
    X_ = np.c_[np.ones(X.shape[0]), X]
    y_hat = X_.dot(beta)
    return np.expm1(y_hat) if use_log else y_hat


# ==============================
# Calibration (data‑driven)
# ==============================
def calibrate(df: pd.DataFrame, q_lo: float, q_hi: float, base_min_income: float):
    work = df.copy()
    work["monthly_income"] = work["fin_ann_inc1"] / 12.0
    work = work[work["monthly_income"] > 0].copy()

    work["r_cc"] = work["cc_max_lmt"] / work["monthly_income"]
    work["r_trb"] = work["trb_bal_0625_uniq"] / work["monthly_income"]
    work.replace([np.inf, -np.inf], np.nan, inplace=True)

    def agg_seg(g):
        return pd.Series({
            "n": g.shape[0],
            "cc_to_monthly_income_ratio": np.nanmedian(g["r_cc"].values),
            "trb_to_monthly_income_months": np.nanmedian(g["r_trb"].values),
            "monthly_income_median": np.nanmedian(g["monthly_income"].values),
            "annual_income_median": np.nanmedian(g["monthly_income"].values) * 12.0
        })

    by_seg = work.groupby("final_cus_seg", dropna=False).apply(agg_seg).reset_index()
    overall = agg_seg(work)
    overall.name = "__overall__"

    mins_by_seg = {
        s: max(base_min_income, float(row["annual_income_median"] * 0.9))
        for s, row in by_seg.set_index("final_cus_seg").iterrows()
    }
    mins_by_seg["__overall__"] = max(base_min_income, float(overall["annual_income_median"] * 0.9))

    ratios_by_seg = {
        s: {
            "cc_to_monthly_income_ratio": float(row["cc_to_monthly_income_ratio"]),
            "trb_to_monthly_income_months": float(row["trb_to_monthly_income_months"]),
            "n": int(row["n"])
        }
        for s, row in by_seg.set_index("final_cus_seg").iterrows()
    }
    ratios_by_seg["__overall__"] = {
        "cc_to_monthly_income_ratio": float(overall["cc_to_monthly_income_ratio"]),
        "trb_to_monthly_income_months": float(overall["trb_to_monthly_income_months"]),
        "n": int(overall["n"])
    }

    work["expected_rules"] = [
        expected_income_rules(r, ratios_by_seg, mins_by_seg, r.get("final_cus_seg", "__overall__"))
        for _, r in work.iterrows()
    ]
    eps = 1e-9
    work["rep_to_exp"] = work["fin_ann_inc1"] / (work["expected_rules"] + eps)

    def band_agg(g):
        qlo = np.nanquantile(g["rep_to_exp"].values, q_lo)
        qhi = np.nanquantile(g["rep_to_exp"].values, q_hi)
        return pd.Series({"q_lo": float(qlo), "q_hi": float(qhi), "n": g.shape[0]})

    bands_seg = work.groupby("final_cus_seg", dropna=False).apply(band_agg).reset_index()
    bands_overall = band_agg(work)
    bands_overall.name = "__overall__"

    bands_by_seg = {
        s: {"band_lower": float(max(0.4, row["q_lo"])), "band_upper": float(min(2.5, row["q_hi"])), "n": int(row["n"])}
        for s, row in bands_seg.set_index("final_cus_seg").iterrows()
    }
    bands_by_seg["__overall__"] = {
        "band_lower": float(max(0.4, bands_overall["q_lo"])),
        "band_upper": float(min(2.5, bands_overall["q_hi"])),
        "n": int(bands_overall["n"])
    }

    # Summary tables for Excel
    tbl_ratios = by_seg.sort_values("n", ascending=False)
    tbl_bands = bands_seg.sort_values("n", ascending=False)
    tbl_overall = pd.DataFrame({
        "metric": ["cc_to_monthly_income_ratio", "trb_to_monthly_income_months", "band_lower", "band_upper", "n"],
        "value": [ratios_by_seg["__overall__"]["cc_to_monthly_income_ratio"],
                  ratios_by_seg["__overall__"]["trb_to_monthly_income_months"],
                  bands_by_seg["__overall__"]["band_lower"],
                  bands_by_seg["__overall__"]["band_upper"],
                  ratios_by_seg["__overall__"]["n"]]
    })

    summary = {
        "ratios_by_seg": tbl_ratios,
        "bands_by_seg": tbl_bands,
        "overall": tbl_overall
    }
    return ratios_by_seg, bands_by_seg, mins_by_seg, summary


# ==============================
# Flagging
# ==============================
def flag_income(reported: float, expected: float, lower: float, upper: float, min_income_annual: float) -> str:
    if pd.isna(reported) or reported <= 0:
        return "Missing"
    base = "Low" if reported < min_income_annual else "OK"
    if reported < expected * lower or reported > expected * upper:
        return "Mismatch"
    return base


# ==============================
# Plotting (matplotlib, one plot per fig)
# ==============================
def _save_scatter_with_fit(
    outdir: str, x: np.ndarray, y: np.ndarray, x_label: str, y_label: str,
    pred_fn=None, hold_vals: Optional[Dict[str, float]] = None, fname: str = "scatter.png"
):
    # Scatter
    plt.figure()
    plt.scatter(x, y, alpha=0.4)
    # Fitted curve along x if pred_fn provided (others held at medians)
    if pred_fn is not None:
        xs = np.linspace(np.nanpercentile(x, 1), np.nanpercentile(x, 99), 180)
        ys = []
        for xv in xs:
            ys.append(pred_fn(xv, hold_vals))
        plt.plot(xs, ys)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    path = os.path.join(outdir, fname)
    plt.tight_layout()
    plt.savefig(path, dpi=140)
    plt.close()
    return path


def _save_actual_vs_pred(outdir: str, actual: np.ndarray, pred: np.ndarray, fname: str = "actual_vs_pred.png"):
    plt.figure()
    plt.scatter(pred, actual, alpha=0.4)
    # 45-degree
    lo = np.nanpercentile(np.concatenate([actual, pred]), 1)
    hi = np.nanpercentile(np.concatenate([actual, pred]), 99)
    plt.plot([lo, hi], [lo, hi])
    plt.xlabel("Predicted Income (₹)")
    plt.ylabel("Actual Income (₹)")
    path = os.path.join(outdir, fname)
    plt.tight_layout()
    plt.savefig(path, dpi=140)
    plt.close()
    return path


def _save_residuals_vs_pred(outdir: str, pred: np.ndarray, resid: np.ndarray, fname: str = "residuals_vs_pred.png"):
    plt.figure()
    plt.scatter(pred, resid, alpha=0.4)
    plt.axhline(0)
    plt.xlabel("Predicted Income (₹)")
    plt.ylabel("Residuals (Actual - Pred)")
    path = os.path.join(outdir, fname)
    plt.tight_layout()
    plt.savefig(path, dpi=140)
    plt.close()
    return path


def _save_residual_hist(outdir: str, resid: np.ndarray, fname: str = "residual_hist.png"):
    plt.figure()
    plt.hist(resid[~np.isnan(resid)], bins=40)
    plt.xlabel("Residuals")
    plt.ylabel("Frequency")
    path = os.path.join(outdir, fname)
    plt.tight_layout()
    plt.savefig(path, dpi=140)
    plt.close()
    return path


def _save_corr_heatmap(outdir: str, df: pd.DataFrame, cols: List[str], fname: str = "corr_matrix.png"):
    corr = df[cols].corr(method="pearson")
    plt.figure()
    plt.imshow(corr, interpolation="nearest")
    plt.xticks(range(len(cols)), cols, rotation=45, ha="right")
    plt.yticks(range(len(cols)), cols)
    plt.colorbar()
    plt.title("Correlation Matrix")
    path = os.path.join(outdir, fname)
    plt.tight_layout()
    plt.savefig(path, dpi=140)
    plt.close()
    return path


# ==============================
# Pipeline
# ==============================
def run_pipeline(
    df: pd.DataFrame,
    cfg: Dict[str, Any],
    mode: str = "regression",
    lock_segment_bands: bool = False,
    import_calib: Optional[Dict[str, Any]] = None,
    winsorize: bool = False,
    winsor_lo: float = 0.025,
    winsor_hi: float = 0.975,
    outdir: str = "./out"
) -> Tuple[pd.DataFrame, Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    # Clean numerics
    df = clean_numeric_cols(df, PROFILE_COLS)
    if "final_cus_seg" not in df.columns:
        df["final_cus_seg"] = DEFAULTS.segment

    # 1) Data Profile (raw, for audit; this drives no hardcoding)
    profile = profile_vars(df, PROFILE_COLS)

    # Optional winsorization for MODELING ONLY (keeps raw for reporting)
    model_df = df.copy()
    if winsorize:
        for c in PROFILE_COLS:
            if c in model_df.columns and model_df[c].notna().any():
                model_df[c] = winsorize_series(model_df[c], winsor_lo, winsor_hi)

    # 2) Calibration on valid salaries (winsorized view if enabled)
    valid = model_df["fin_ann_inc1"].notna() & (model_df["fin_ann_inc1"] > 0)
    calib_df = model_df.loc[valid].copy()

    if import_calib is not None:
        ratios_by_seg = import_calib["ratios_by_seg"]
        bands_by_seg = import_calib["bands_by_seg"]
        mins_by_seg = import_calib["mins_by_seg"]
        calib_sheets = {"ratios_by_seg": pd.DataFrame(), "bands_by_seg": pd.DataFrame(), "overall": pd.DataFrame()}
    elif calib_df.empty or calib_df.shape[0] < 50:
        ratios_by_seg = {"__overall__": {"cc_to_monthly_income_ratio": 1.5, "trb_to_monthly_income_months": 3.0, "n": 0}}
        bands_by_seg = {"__overall__": {"band_lower": cfg["band_lower"], "band_upper": cfg["band_upper"], "n": 0}}
        mins_by_seg = {"__overall__": cfg["min_income_annual"]}
        calib_sheets = {"ratios_by_seg": pd.DataFrame(), "bands_by_seg": pd.DataFrame(), "overall": pd.DataFrame()}
    else:
        ratios_by_seg, bands_by_seg, mins_by_seg, calib_sheets = calibrate(
            calib_df, cfg["q_lo"], cfg["q_hi"], cfg["min_income_annual"]
        )

    def segkey(seg: str) -> str:
        if not lock_segment_bands:
            return "__overall__"
        n_ratio = ratios_by_seg.get(seg, {}).get("n", 0)
        n_band = bands_by_seg.get(seg, {}).get("n", 0)
        return seg if (n_ratio >= cfg["min_records_per_segment"] and n_band >= cfg["min_records_per_segment"]) else "__overall__"

    # 3) Expected income (regression on modeling view if enough data and not importing)
    df_calc = model_df.copy()
    if mode == "regression" and (calib_df.shape[0] >= 50) and (import_calib is None):
        beta, sigma, feats = fit_regression(calib_df, use_log=cfg["use_log_model"])
        df_calc["expected_income"] = predict_regression(df_calc, beta, feats, use_log=cfg["use_log_model"])
        model_info = {"mode": "regression", "beta": beta.tolist(), "sigma": sigma, "feats": feats}
    else:
        df_calc["expected_income"] = [
            expected_income_rules(r, ratios_by_seg, mins_by_seg, r.get("final_cus_seg", "__overall__"))
            for _, r in df_calc.iterrows()
        ]
        model_info = {"mode": "rules" if import_calib is None else "rules(imported_bands)"}

    # 4) Bounds & flags (using calibrated bands)
    lowers, uppers, mins_used = [], [], []
    for _, r in df_calc.iterrows():
        seg = r.get("final_cus_seg", "__overall__")
        sk = segkey(seg)
        band = bands_by_seg.get(sk, bands_by_seg["__overall__"])
        lower = float(band["band_lower"])
        upper = float(band["band_upper"])
        min_income = float(mins_by_seg.get(sk, mins_by_seg["__overall__"]))
        lowers.append(lower); uppers.append(upper); mins_used.append(min_income)

    df_calc["lower_bound"] = df_calc["expected_income"] * np.array(lowers)
    df_calc["upper_bound"] = df_calc["expected_income"] * np.array(uppers)
    df_calc["min_income_used"] = mins_used

    # Flags are evaluated on REPORTED income (raw), not winsorized
    df_calc["income_flag"] = [
        flag_income(rep, exp, lo, up, mn)
        for rep, exp, lo, up, mn in zip(df["fin_ann_inc1"], df_calc["expected_income"], lowers, uppers, mins_used)
    ]
    df_calc["income_gap"] = df_calc["expected_income"] - df["fin_ann_inc1"].fillna(0.0)

    # 5) Charts (saved PNGs and embedded into Excel)
    charts = {}
    os.makedirs(outdir, exist_ok=True)

    # Helper for regression partial fit lines
    hold = {
        "trb_bal_0625_uniq": float(np.nanmedian(model_df["trb_bal_0625_uniq"])) if "trb_bal_0625_uniq" in model_df else 0.0,
        "cc_max_lmt": float(np.nanmedian(model_df["cc_max_lmt"])) if "cc_max_lmt" in model_df else 0.0,
        "cc_tot_lmt": float(np.nanmedian(model_df["cc_tot_lmt"])) if "cc_tot_lmt" in model_df else 0.0
    }

    if model_info["mode"] == "regression":
        beta = np.array(model_info["beta"])
        use_log = cfg["use_log_model"]

        def pred_partial(x_val: float, hold_vals: Dict[str, float], vary: str) -> float:
            # Build one row with vary=x_val and others held
            trb = x_val if vary == "trb_bal_0625_uniq" else hold_vals["trb_bal_0625_uniq"]
            cmax = x_val if vary == "cc_max_lmt" else hold_vals["cc_max_lmt"]
            ctot = x_val if vary == "cc_tot_lmt" else hold_vals["cc_tot_lmt"]
            X = np.array([1.0, trb, cmax, ctot])
            if use_log:
                X = np.array([1.0, np.log1p(trb), np.log1p(cmax), np.log1p(ctot)])
                yhat = np.dot(X, beta)
                return float(np.expm1(yhat))
            else:
                return float(np.dot(X, beta))

        # Scatter + fitted curve for each predictor
        charts["income_vs_trb"] = _save_scatter_with_fit(
            outdir,
            x=model_df["trb_bal_0625_uniq"].values,
            y=df["fin_ann_inc1"].values,
            x_label="TRB (₹)",
            y_label="Income (₹)",
            pred_fn=lambda xv, _: pred_partial(xv, hold, "trb_bal_0625_uniq"),
            hold_vals=hold,
            fname="income_vs_trb.png"
        )
        charts["income_vs_ccmax"] = _save_scatter_with_fit(
            outdir,
            x=model_df["cc_max_lmt"].values,
            y=df["fin_ann_inc1"].values,
            x_label="CC Max Limit (₹)",
            y_label="Income (₹)",
            pred_fn=lambda xv, _: pred_partial(xv, hold, "cc_max_lmt"),
            hold_vals=hold,
            fname="income_vs_ccmax.png"
        )
        charts["income_vs_cctot"] = _save_scatter_with_fit(
            outdir,
            x=model_df["cc_tot_lmt"].values,
            y=df["fin_ann_inc1"].values,
            x_label="CC Total Limit (₹)",
            y_label="Income (₹)",
            pred_fn=lambda xv, _: pred_partial(xv, hold, "cc_tot_lmt"),
            hold_vals=hold,
            fname="income_vs_cctot.png"
        )

        # Actual vs Predicted, Residuals, Histogram
        y_pred = df_calc["expected_income"].values
        y_act = df["fin_ann_inc1"].astype(float).values
        resid = y_act - y_pred
        charts["actual_vs_pred"] = _save_actual_vs_pred(outdir, y_act, y_pred, "actual_vs_pred.png")
        charts["resid_vs_pred"] = _save_residuals_vs_pred(outdir, y_pred, resid, "residuals_vs_pred.png")
        charts["resid_hist"] = _save_residual_hist(outdir, resid, "residual_hist.png")

    # Correlation matrix (raw)
    corr_cols = ["fin_ann_inc1", "trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt"]
    charts["corr_matrix"] = _save_corr_heatmap(outdir, df, [c for c in corr_cols if c in df.columns], "corr_matrix.png")

    # 6) Prepare final outputs (use raw df for reporting columns; expected/bounds/flags from df_calc)
    keep = [
        "cusid", "final_cus_seg", "inc_src",
        "trb_bal_0625_uniq", "cc_max_lmt", "cc_tot_lmt",
        "fin_ann_inc1", "expected_income", "lower_bound", "upper_bound",
        "income_gap", "income_flag"
    ]
    out_df = pd.concat([df, df_calc[["expected_income","lower_bound","upper_bound","income_gap","income_flag"]]], axis=1)
    existing = [c for c in keep if c in out_df.columns]
    out_df = out_df[existing].copy()
    if "cusid" in out_df.columns:
        out_df["cusid"] = out_df["cusid"].astype("string")

    # Summary
    summary = (
        out_df.groupby(["final_cus_seg", "income_flag"], dropna=False)
        .agg(customers=("cusid", "nunique"),
             avg_reported=("fin_ann_inc1", "mean"),
             avg_expected=("expected_income", "mean"),
             total_gap=("income_gap", "sum"))
        .reset_index()
    )

    details = {
        "ratios_by_seg": ratios_by_seg,
        "bands_by_seg": bands_by_seg,
        "mins_by_seg": mins_by_seg,
        "calib_sheets": calib_sheets,
        "profile": profile
    }

    # 7) Save Excel + embed charts
    out_csv = os.path.join(outdir, "premier_income_flags.csv")
    out_xlsx = os.path.join(outdir, "premier_income_flags.xlsx")
    out_df.to_csv(out_csv, index=False)

    with pd.ExcelWriter(out_xlsx, engine="xlsxwriter") as writer:
        # Data sheets
        out_df.to_excel(writer, index=False, sheet_name="flags")
        summary.to_excel(writer, index=False, sheet_name="summary")
        profile.to_excel(writer, index=False, sheet_name="data_profile")

        # Calibration sheets
        if not details["calib_sheets"]["ratios_by_seg"].empty:
            details["calib_sheets"]["ratios_by_seg"].to_excel(writer, index=False, sheet_name="calib_ratios")
        if not details["calib_sheets"]["bands_by_seg"].empty:
            details["calib_sheets"]["bands_by_seg"].to_excel(writer, index=False, sheet_name="calib_bands")
        if not details["calib_sheets"]["overall"].empty:
            details["calib_sheets"]["overall"].to_excel(writer, index=False, sheet_name="calib_overall")

        # Basic formatting
        wb = writer.book
        ws_flags = writer.sheets["flags"]
        num_fmt = wb.add_format({"num_format": "#,##0"})
        text_fmt = wb.add_format({"num_format": "@"})
        money_cols = ["trb_bal_0625_uniq","cc_max_lmt","cc_tot_lmt","fin_ann_inc1","expected_income","lower_bound","upper_bound","income_gap"]

        for i, col in enumerate(out_df.columns):
            ws_flags.set_column(i, i, max(12, min(28, len(col)+2)), text_fmt if col=="cusid" else None)
        for col in money_cols:
            if col in out_df.columns:
                idx = out_df.columns.get_loc(col)
                ws_flags.set_column(idx, idx, 16, num_fmt)

        # Charts sheet with embedded PNGs
        ws_ch = wb.add_worksheet("charts")
        row, col = 1, 1
        for name, path in charts.items():
            if os.path.exists(path):
                ws_ch.insert_image(row, col, path)
                row += 22  # space between images

    meta = {
        "mode": model_info["mode"],
        "use_log_model": cfg["use_log_model"],
        "lock_segment_bands": lock_segment_bands,
        "winsorized_for_model": winsorize,
        "winsor_lo": winsor_lo,
        "winsor_hi": winsor_hi,
        "q_lo": cfg["q_lo"],
        "q_hi": cfg["q_hi"],
        "min_income_annual": cfg["min_income_annual"],
        "min_records_per_segment": cfg["min_records_per_segment"]
    }
    with open(os.path.join(outdir, "meta.txt"), "w", encoding="utf-8") as f:
        for k, v in meta.items():
            f.write(f"{k}: {v}\n")

    # Return everything for potential callers
    return out_df, model_info, details, {"charts": charts, "out_csv": out_csv, "out_xlsx": out_xlsx}


# ==============================
# CLI
# ==============================
def main():
    ap = argparse.ArgumentParser(description="Premier Income Estimator — Production (Profiling + Charts)")
    ap.add_argument("--in", dest="in_path", required=True, help="Input CSV/Parquet path")
    ap.add_argument("--outdir", default="./out", help="Output directory")
    ap.add_argument("--mode", choices=["regression","rules"], default="regression", help="Estimation mode")
    ap.add_argument("--lock_segment_bands", action="store_true", help="Use segment-wise bands/ratios if sample is sufficient")
    ap.add_argument("--min_records_per_segment", type=int, default=DEFAULTS.min_records_per_segment)
    ap.add_argument("--q_lo", type=float, default=DEFAULTS.q_lo, help="Lower quantile for band calibration")
    ap.add_argument("--q_hi", type=float, default=DEFAULTS.q_hi, help="Upper quantile for band calibration")
    ap.add_argument("--min_income_annual", type=float, default=DEFAULTS.min_income_annual, help="Base minimum annual income (₹)")
    ap.add_argument("--use_log_model", action="store_true", help="Use log1p regression (default True)")
    ap.add_argument("--no_log_model", action="store_true", help="Disable log1p regression")
    ap.add_argument("--import_calib", default=None, help="Path to load calibration JSON")
    ap.add_argument("--export_calib", default=None, help="Path to write learned calibration JSON")
    ap.add_argument("--winsorize", action="store_true", help="Winsorize key vars for modeling (keeps raw for reporting)")
    ap.add_argument("--winsor_lo", type=float, default=0.025, help="Winsor lower quantile")
    ap.add_argument("--winsor_hi", type=float, default=0.975, help="Winsor upper quantile")
    ap.add_argument("-v", "--verbose", action="count", default=1, help="Verbosity: -v (info), -vv (debug)")
    args = ap.parse_args()

    setup_logging(args.verbose)

    cfg: Dict[str, Any] = {
        "segment": DEFAULTS.segment,
        "min_income_annual": args.min_income_annual,
        "band_lower": DEFAULTS.band_lower,
        "band_upper": DEFAULTS.band_upper,
        "q_lo": args.q_lo,
        "q_hi": args.q_hi,
        "min_records_per_segment": args.min_records_per_segment,
        "use_log_model": DEFAULTS.use_log_model,
    }
    if args.no_log_model:
        cfg["use_log_model"] = False
    if args.use_log_model:
        cfg["use_log_model"] = True

    # Read
    try:
        df = read_input(args.in_path)
    except Exception as e:
        logging.exception("Failed to read input: %s", e)
        sys.exit(2)

    required = ["cusid","cc_max_lmt","cc_tot_lmt","fin_ann_inc1","inc_src","trb_bal_0625_uniq","final_cus_seg"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        logging.error("Missing required columns: %s", missing)
        sys.exit(2)

    # Optional import calibration
    import_calib = None
    if args.import_calib:
        try:
            with open(args.import_calib, "r", encoding="utf-8") as f:
                import_calib = json.load(f)
        except Exception as e:
            logging.exception("Failed to import calibration JSON: %s", e)
            sys.exit(2)

    # Run pipeline
    out_df, model_info, details, files = run_pipeline(
        df=df,
        cfg=cfg,
        mode=args.mode,
        lock_segment_bands=args.lock_segment_bands,
        import_calib=import_calib,
        winsorize=args.winsorize,
        winsor_lo=args.winsor_lo,
        winsor_hi=args.winsor_hi,
        outdir=args.outdir
    )

    # Optional export calibration
    if args.export_calib:
        try:
            calib_payload = {
                "ratios_by_seg": details["ratios_by_seg"],
                "bands_by_seg": details["bands_by_seg"],
                "mins_by_seg": details["mins_by_seg"],
                "meta": {
                    "source_file": os.path.basename(args.in_path),
                    "mode": model_info.get("mode"),
                    "q_lo": cfg["q_lo"],
                    "q_hi": cfg["q_hi"],
                    "min_records_per_segment": cfg["min_records_per_segment"]
                }
            }
            with open(args.export_calib, "w", encoding="utf-8") as f:
                json.dump(calib_payload, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.exception("Failed to export calibration JSON: %s", e)
            sys.exit(2)

    logging.info("Output CSV: %s", files["out_csv"])
    logging.info("Output XLSX: %s", files["out_xlsx"])
    logging.info("Charts saved to: %s", args.outdir)


if __name__ == "__main__":
    main()


























Sub CreateFinalPivotTables()

    Dim wsData As Worksheet, wsPivot As Worksheet
    Dim i As Integer, startRow As Long
    Dim baseVars As Variant, varName As String
    Dim pc As PivotCache, pt As PivotTable
    Dim dataRange As Range
    Dim colCheck As Boolean

    ' ✅ Speed & memory optimization
    Application.ScreenUpdating = False
    Application.Calculation = xlCalculationManual
    Application.EnableEvents = False

    ' Set source sheet
    Set wsData = ThisWorkbook.Sheets("RawData")

    ' Clear/create pivot sheet
    On Error Resume Next
    Application.DisplayAlerts = False
    Worksheets("All_Pivots").Delete
    Application.DisplayAlerts = True
    On Error GoTo 0

    Set wsPivot = ThisWorkbook.Sheets.Add(After:=ThisWorkbook.Sheets(ThisWorkbook.Sheets.Count))
    wsPivot.Name = "All_Pivots"

    ' Full fixed range (A1:AH644743)
    Set dataRange = wsData.Range("A1:AH644743")

    ' ✅ Extracted base variables from columns B to L
    baseVars = Array("Entertainment", "Groceries", "Medical", "Other_Regular", "Persona_Finances", _
                     "Restaurant", "Shopping", "Transportation", "Travel", "Utility_prim", "Utility_sec")

    startRow = 1

    ' Loop over each base variable
    For i = LBound(baseVars) To UBound(baseVars)
        varName = baseVars(i)

        colCheck = ColumnExists(wsData, varName) And _
                   ColumnExists(wsData, varName & "_test") And _
                   ColumnExists(wsData, varName & "_train") And _
                   ColumnExists(wsData, "ACCT")

        If Not colCheck Then
            wsPivot.Cells(startRow, 1).Value = "❌ Skipped: " & varName & " – Missing column(s)"
            startRow = startRow + 2
            GoTo NextLoop
        End If

        ' Create PivotCache from full range
        Set pc = ThisWorkbook.PivotCaches.Create( _
            SourceType:=xlDatabase, _
            SourceData:=dataRange.Address(, , xlR1C1, True))

        ' Create PivotTable
        Set pt = pc.CreatePivotTable( _
            TableDestination:=wsPivot.Cells(startRow, 1), _
            TableName:="Pivot_" & varName)

        On Error GoTo PivotError

        With pt
            ' Row field
            .PivotFields(varName).Orientation = xlRowField
            .PivotFields(varName).Position = 1

            ' Column field
            .PivotFields(varName & "_test").Orientation = xlColumnField
            .PivotFields(varName & "_test").Position = 1

            ' Data field: count of ACCT
            With .PivotFields("ACCT")
                .Orientation = xlDataField
                .Function = xlCount
                .Name = "Count of ACCT"
            End With

            ' Show as % of row total
            .DataPivotField.ShowDataAs = xlPercentOfRow
            .DataPivotField.NumberFormat = "0.0%"

            ' Filter field
            .PivotFields(varName & "_train").Orientation = xlPageField
            .PivotFields(varName & "_train").Position = 1
        End With

        ' Update position for next pivot
        startRow = startRow + pt.TableRange2.Rows.Count + 3
        GoTo NextLoop

PivotError:
        wsPivot.Cells(startRow, 1).Value = "⚠️ Pivot failed for: " & varName
        startRow = startRow + 2
        Resume Next

NextLoop:
    Next i

    ' ✅ Reset Excel settings
    Application.ScreenUpdating = True
    Application.Calculation = xlCalculationAutomatic
    Application.EnableEvents = True

    MsgBox "✅ All pivots created successfully in 'All_Pivots' sheet!", vbInformation

End Sub

' Check if column exists in header
Function ColumnExists(ws As Worksheet, colName As String) As Boolean
    ColumnExists = Not IsError(Application.Match(colName, ws.Range("A1:AH1"), 0))
End Function

















Sub CreateOrderedPivotTables()

    Dim wsData As Worksheet, wsPivot As Worksheet
    Dim i As Integer, startRow As Long
    Dim baseVars As Variant, varName As String
    Dim pc As PivotCache, pt As PivotTable
    Dim dataRange As Range
    Dim colCheck As Boolean

    ' ✅ Performance optimization
    Application.ScreenUpdating = False
    Application.Calculation = xlCalculationManual
    Application.EnableEvents = False

    ' Set source sheet
    Set wsData = ThisWorkbook.Sheets("RawData")

    ' Create/clean output sheet
    On Error Resume Next
    Application.DisplayAlerts = False
    Worksheets("All_Pivots").Delete
    Application.DisplayAlerts = True
    On Error GoTo 0

    Set wsPivot = ThisWorkbook.Sheets.Add(After:=ThisWorkbook.Sheets(ThisWorkbook.Sheets.Count))
    On Error Resume Next
    wsPivot.Name = "All_Pivots"
    On Error GoTo 0

    ' ✅ Use fixed range (reduce if memory fails)
    Set dataRange = wsData.Range("A1:AH644743")

    ' ✅ Ordered list of base variables (no suffix)
    baseVars = Array( _
        "Entertainment", _
        "Groceries", _
        "Medical", _
        "Other_Regular", _
        "Persona_Finances", _
        "Restaurant", _
        "Shopping", _
        "Transportation", _
        "Travel", _
        "Utility_prim", _
        "Utility_sec" _
    )

    startRow = 1

    ' Loop through variables
    For i = LBound(baseVars) To UBound(baseVars)
        varName = baseVars(i)

        colCheck = ColumnExists(wsData, varName) And _
                   ColumnExists(wsData, varName & "_test") And _
                   ColumnExists(wsData, "ACCT")

        If Not colCheck Then
            wsPivot.Cells(startRow, 1).Value = "❌ Skipped: " & varName & " – Missing column(s)"
            startRow = startRow + 2
            GoTo NextLoop
        End If

        ' ✅ Create Pivot Cache (R1C1 format avoids range issues)
        Set pc = ThisWorkbook.PivotCaches.Create( _
            SourceType:=xlDatabase, _
            SourceData:=dataRange.Address(, , xlR1C1, True))

        ' ✅ Create Pivot Table
        Set pt = pc.CreatePivotTable( _
            TableDestination:=wsPivot.Cells(startRow, 1), _
            TableName:="Pivot_" & varName)

        With pt
            ' Row Field
            .PivotFields(varName).Orientation = xlRowField
            .PivotFields(varName).Position = 1

            ' Column Field
            .PivotFields(varName & "_test").Orientation = xlColumnField
            .PivotFields(varName & "_test").Position = 1

            ' Value Field
            With .PivotFields("ACCT")
                .Orientation = xlDataField
                .Function = xlCount
                .Name = "Count of ACCT"
            End With

            ' Show as % of Row
            .DataPivotField.ShowDataAs = xlPercentOfRow
            .DataPivotField.NumberFormat = "0.0%"

            ' Filter Field (optional)
            If ColumnExists(wsData, varName & "_train") Then
                .PivotFields(varName & "_train").Orientation = xlPageField
                .PivotFields(varName & "_train").Position = 1
            End If
        End With

        ' Update start row for next pivot
        startRow = startRow + pt.TableRange2.Rows.Count + 3

NextLoop:
    Next i

    ' ✅ Restore settings
    Application.ScreenUpdating = True
    Application.Calculation = xlCalculationAutomatic
    Application.EnableEvents = True

    MsgBox "✅ All ordered pivot tables created in 'All_Pivots' successfully!", vbInformation

End Sub

' ✅ Column existence checker
Function ColumnExists(ws As Worksheet, colName As String) As Boolean
    ColumnExists = Not IsError(Application.Match(colName, ws.Range("A1:AH1"), 0))
End Function





















Sub CreateOrderedPivotTables()

    Dim wsData As Worksheet, wsPivot As Worksheet
    Dim i As Integer, startRow As Long
    Dim baseVars As Variant, varName As String
    Dim pc As PivotCache, pt As PivotTable
    Dim dataRange As Range
    Dim colCheck As Boolean

    ' Set source sheet
    Set wsData = ThisWorkbook.Sheets("RawData")

    ' Recreate output sheet
    On Error Resume Next
    Application.DisplayAlerts = False
    Worksheets("All_Pivots").Delete
    Application.DisplayAlerts = True
    On Error GoTo 0
    Set wsPivot = ThisWorkbook.Sheets.Add
    wsPivot.Name = "All_Pivots"

    ' Explicit data range
    Set dataRange = wsData.Range("A1:AH644743")

    ' ✅ Ordered list of variables (base only)
    baseVars = Array( _
        "Entertainment", _
        "Groceries", _
        "Medical", _
        "Other_Regular", _
        "Persona_Finances", _
        "Restaurant", _
        "Shopping", _
        "Transportation", _
        "Travel", _
        "Utility_prim", _
        "Utility_sec" _
    )

    startRow = 1

    ' Loop through ordered variables
    For i = LBound(baseVars) To UBound(baseVars)
        varName = baseVars(i)

        colCheck = ColumnExists(wsData, varName) And _
                   ColumnExists(wsData, varName & "_test") And _
                   ColumnExists(wsData, "ACCT")

        If Not colCheck Then
            wsPivot.Cells(startRow, 1).Value = "❌ Skipped: " & varName & " – Missing column(s)"
            startRow = startRow + 2
            GoTo NextLoop
        End If

        ' Create pivot cache
        Set pc = ThisWorkbook.PivotCaches.Create( _
            SourceType:=xlDatabase, _
            SourceData:=dataRange.Address(, , xlR1C1, True))

        ' Create pivot table
        Set pt = pc.CreatePivotTable( _
            TableDestination:=wsPivot.Cells(startRow, 1), _
            TableName:="Pivot_" & varName)

        With pt
            .PivotFields(varName).Orientation = xlRowField
            .PivotFields(varName).Position = 1

            .PivotFields(varName & "_test").Orientation = xlColumnField
            .PivotFields(varName & "_test").Position = 1

            With .PivotFields("ACCT")
                .Orientation = xlDataField
                .Function = xlCount
                .Name = "Count of ACCT"
            End With

            .DataPivotField.ShowDataAs = xlPercentOfRow
            .DataPivotField.NumberFormat = "0.0%"

            If ColumnExists(wsData, varName & "_train") Then
                .PivotFields(varName & "_train").Orientation = xlPageField
                .PivotFields(varName & "_train").Position = 1
            End If
        End With

        startRow = startRow + pt.TableRange2.Rows.Count + 3

NextLoop:
    Next i

    MsgBox "✅ Ordered Pivot tables created successfully!", vbInformation

End Sub

' Column existence checker
Function ColumnExists(ws As Worksheet, colName As String) As Boolean
    ColumnExists = Not IsError(Application.Match(colName, ws.Range("A1:AH1"), 0))
End Function

















Sub CreatePivotTablesForAllVars()

    Dim wsData As Worksheet, wsPivot As Worksheet
    Dim lastRow As Long, i As Integer, startRow As Long
    Dim baseVars As Variant, varName As String
    Dim pc As PivotCache, pt As PivotTable
    Dim dataRange As Range

    ' Update with your sheet name
    Set wsData = ThisWorkbook.Sheets("RawData")

    ' Create/clear output sheet
    On Error Resume Next
    Application.DisplayAlerts = False
    Worksheets("All_Pivots").Delete
    Application.DisplayAlerts = True
    On Error GoTo 0
    Set wsPivot = ThisWorkbook.Sheets.Add
    wsPivot.Name = "All_Pivots"
    
    ' Get last row
    lastRow = wsData.Cells(wsData.Rows.Count, "A").End(xlUp).Row
    Set dataRange = wsData.Range("A1").CurrentRegion

    ' List of base variables (no suffix)
    baseVars = Array("Entertainment", "Groceries", "Medical", "Other_Regular", "Persona_Finances", _
                     "Restaurant", "Shopping", "Transportation", "Travel", "Utility_sec", "Utility_prim")

    startRow = 1

    For i = LBound(baseVars) To UBound(baseVars)
        varName = baseVars(i)

        ' Create Pivot Cache
        Set pc = ThisWorkbook.PivotCaches.Create( _
            SourceType:=xlDatabase, _
            SourceData:=dataRange)

        ' Create Pivot Table
        Set pt = pc.CreatePivotTable( _
            TableDestination:=wsPivot.Cells(startRow, 1), _
            TableName:="Pivot_" & varName)

        With pt
            .PivotFields(varName).Orientation = xlRowField
            .PivotFields(varName).Position = 1

            .PivotFields(varName & "_test").Orientation = xlColumnField
            .PivotFields(varName & "_test").Position = 1

            .PivotFields("ACCT").Orientation = xlDataField
            .PivotFields("ACCT").Function = xlCount
            .PivotFields("ACCT").NumberFormat = "0.0%"

            .DataPivotField.ShowDataAs = xlPercentOfRow

            On Error Resume Next ' In case train field is missing
            .PivotFields(varName & "_train").Orientation = xlPageField
            .PivotFields(varName & "_train").Position = 1
            On Error GoTo 0
        End With

        ' Set next pivot position
        startRow = startRow + pt.TableRange2.Rows.Count + 3

    Next i

    MsgBox "All pivot tables created successfully in sheet 'All_Pivots'.", vbInformation

End Sub

















%macro create_pivots(input_ds=your_dataset, output_file='pivot_output.xlsx');

    /* List of base variables (without _test/_train) */
    %let base_vars = Entertainment Groceries Medical Other_Regular Persona_Finances Restaurant Shopping Transportation Travel Utility_sec Utility_prim;

    /* Loop through each base variable */
    %do i = 1 %to %sysfunc(countw(&base_vars));
        %let var = %scan(&base_vars, &i);
        %let var_test = &var._test;
        %let var_train = &var._train;

        /* Step 1: Create raw table with all values */
        proc sql;
            create table pivot_&var as
            select 
                &var as row_val,
                &var_test as col_val,
                count(ACCT) as count_acct
            from &input_ds
            group by &var, &var_test;
        quit;

        /* Step 2: Transpose for columns */
        proc sort data=pivot_&var;
            by row_val col_val;
        run;

        proc transpose data=pivot_&var out=trans_&var(drop=_name_) prefix=col_;
            by row_val;
            id col_val;
            var count_acct;
        run;

        /* Step 3: Add row totals and compute percentages */
        data final_&var;
            set trans_&var;
            array cols[*] col_:;
            row_total = sum(of col_:);

            /* Calculate % of row total */
            %do j = 1 %to 100;
                %if %sysfunc(exist(final_&var)) %then %do;
                    if n(col_&j) then pct_&j = (col_&j / row_total) * 100;
                %end;
            %end;
        run;

        /* Step 4: Export with filter column */
        data export_&var;
            set final_&var;
            length &var_train $50;
            set &input_ds(keep=&var_train);
        run;

    %end;

    /* Step 5: Export all to Excel with one sheet per variable */
    ods excel file=&output_file options(sheet_interval='proc');

    %do i = 1 %to %sysfunc(countw(&base_vars));
        %let var = %scan(&base_vars, &i);
        proc print data=export_&var label;
            title "Pivot for &var";
        run;
    %end;

    ods excel close;

%mend;

/* Call the macro */
%create_pivots(input_ds=your_dataset, output_file='pivot_output.xlsx');












%macro stack_cube_data;

    /* Define mapping of dataset name month to revenue suffix (MMYY) */
    %let n = 12;
    %let dsname1 = jan24; %let revcode1 = 0124;
    %let dsname2 = feb24; %let revcode2 = 0224;
    %let dsname3 = mar24; %let revcode3 = 0324;
    %let dsname4 = apr24; %let revcode4 = 0424;
    %let dsname5 = may24; %let revcode5 = 0524;
    %let dsname6 = jun24; %let revcode6 = 0624;
    %let dsname7 = jul24; %let revcode7 = 0724;
    %let dsname8 = aug24; %let revcode8 = 0824;
    %let dsname9 = sep24; %let revcode9 = 0924;
    %let dsname10 = oct24; %let revcode10 = 1024;
    %let dsname11 = nov24; %let revcode11 = 1124;
    %let dsname12 = dec24; %let revcode12 = 1224;

    data all_cubes;
        length month $6;
        set
        %do i = 1 %to &n;
            %let dsn = &&dsname&i;
            %let rev = &&revcode&i;
            cube.cust_cube_&dsn._data (in=in_&i rename=(pil_revenue_&rev=pil_revenue))
        %end;
        ;

        /* Assign correct month label based on dataset */
        %do i = 1 %to &n;
            if in_&i then month = "&&dsname&i";
        %end;

        keep cusid final_cus_segment pil_revenue month;
    run;

%mend;

%stack_cube_data;













%macro stack_cube_data;
    data all_cubes;
        set
        %let months = jan24 feb24 mar24 apr24 may24 jun24 
                     jul24 aug24 sep24 oct24 nov24 dec24;

        %do i = 1 %to %sysfunc(countw(&months));
            %let m = %scan(&months, &i);
            cube.cust_cube_&m._data (rename=(pil_revenue_&m=pil_revenue))
        %end;
        ;
        month = lowcase(put(input(scan(vname(pil_revenue), -1, '_'), monyy5.), monyy5.));
        keep cusid final_cus_segment pil_revenue month;
    run;
%mend;

%stack_cube_data;










/* === 1. Setup Month List === */
%let months = jan24 feb24 mar24 apr24 may24 jun24 
              jul24 aug24 sep24 oct24 nov24 dec24;

/* === 2. Prepare PIL Data from work.pil_master === */
data pil;
    set pil_master;
    account_open_date = input(account_open_date, date9.);
    format account_open_date date9.;
    account_open_month = lowcase(put(account_open_date, monyy5.));       /* e.g. jan24 */
    month_before_open_date = intnx('month', account_open_date, -1, 'same');
    month_before_open = lowcase(put(month_before_open_date, monyy5.));   /* e.g. dec23 */
run;

/* === 3. Stack Cube Data from libref CUBE === */
data all_cubes;
    set
    %do i = 1 %to %sysfunc(countw(&months));
        %let m = %scan(&months, &i);
        cube.cust_cube_&m._data (in=a rename=(pil_revenue_&m=pil_revenue))
    %end;
    ;
    month = lowcase(scan(vname(pil_revenue), -1, '_'));  /* or just "&m" if static */
    keep cusid final_cus_segment pil_revenue month;
run;

/* === 4. Merge segment from month before account open === */
proc sql;
    create table pil_seg as
    select a.*, b.final_cus_segment as segment_before_open
    from pil a
    left join all_cubes b
    on a.custid = b.cusid and a.month_before_open = b.month;
quit;

/* === 5. Add Month Rank for PIL and CUBE months === */
data pil_seg;
    set pil_seg;
    open_rank = input(account_open_month, monyy5.);
    format open_rank monyy5.;
run;

data cubes_rank;
    set all_cubes;
    month_rank = input(month, monyy5.);
    format month_rank monyy5.;
run;

/* === 6. Join and Filter Revenue from Open Month to Dec-2024 === */
proc sql;
    create table revenue_joined as
    select a.custid, b.pil_revenue
    from pil_seg a
    inner join cubes_rank b
    on a.custid = b.cusid and b.month_rank >= a.open_rank;
quit;

proc sql;
    create table revenue_sum as
    select custid, sum(pil_revenue) as pil_revenue_from_open_to_dec
    from revenue_joined
    group by custid;
quit;

/* === 7. Final Merge to Get Segment and Revenue === */
proc sql;
    create table final_output as
    select a.custid, a.segment_before_open, b.pil_revenue_from_open_to_dec
    from pil_seg a
    left join revenue_sum b
    on a.custid = b.custid;
quit;

/* === 8. Export if Needed === */
proc export data=final_output
    outfile="/your/export/path/final_pil_segment_summary.csv"
    dbms=csv
    replace;
run;









import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

# ---------------------------
# 1. Load and preprocess
# ---------------------------
df = pd.read_csv("recomtrain_test_data.csv")
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df.dropna(subset=['Date'], inplace=True)
df['ACCT'] = df['ACCT'].astype(str)

split_date = pd.to_datetime("2024-12-31")
test_end_date = pd.to_datetime("2025-03-31")
train_df = df[df['Date'] < split_date]
test_df = df[(df['Date'] >= split_date) & (df['Date'] <= test_end_date)]
sectors = sorted(df['Sector'].unique())

# ---------------------------
# 2. Amount + Frequency Matrix
# ---------------------------
def build_amount_freq(df, sectors):
    amt = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
    freq = df.groupby(['ACCT', 'Sector']).size().unstack(fill_value=0)
    return amt.reindex(columns=sectors, fill_value=0), freq.reindex(columns=sectors, fill_value=0)

def get_weighted_matrix(amt, freq):
    scaler = MinMaxScaler()
    amt_norm = pd.DataFrame(scaler.fit_transform(amt), index=amt.index, columns=amt.columns)
    freq_norm = pd.DataFrame(scaler.fit_transform(freq), index=freq.index, columns=freq.columns)
    amt_var = amt_norm.var(axis=1).mean()
    freq_var = freq_norm.var(axis=1).mean()
    total = amt_var + freq_var
    amt_wt = freq_var / total
    freq_wt = amt_var / total
    combined = amt_wt * amt_norm + freq_wt * freq_norm
    return combined, amt_wt, freq_wt, amt_var, freq_var

# ---------------------------
# 3. Hot Encoding
# ---------------------------
def get_hot_encoding(df, sectors, suffix):
    binary = df.groupby(['ACCT', 'Sector']).size().unstack(fill_value=0)
    binary[binary > 0] = 1
    binary = binary.reindex(columns=sectors, fill_value=0).reset_index()
    binary.columns = ['ACCT'] + [f"{col}_{suffix}" for col in binary.columns if col != 'ACCT']
    return binary

# ---------------------------
# 4. Recommendation
# ---------------------------
def get_recommendations(matrix, similarity):
    norm = pd.DataFrame(MinMaxScaler().fit_transform(matrix), index=matrix.index, columns=matrix.columns)
    scores = norm.dot(similarity)
    return scores.rank(axis=1, method='min', ascending=False).astype(int).reset_index()

# ---------------------------
# 5. Main Logic
# ---------------------------
train_amt, train_freq = build_amount_freq(train_df, sectors)
test_amt, test_freq = build_amount_freq(test_df, sectors)
train_matrix, train_amt_wt, train_freq_wt, train_amt_var, train_freq_var = get_weighted_matrix(train_amt, train_freq)
test_matrix, *_ = get_weighted_matrix(test_amt, test_freq)

sector_sim = pd.DataFrame(cosine_similarity(train_matrix.T), index=sectors, columns=sectors)

train_hot = get_hot_encoding(train_df, sectors, "train")
test_hot = get_hot_encoding(test_df, sectors, "test")

train_recom = get_recommendations(train_matrix, sector_sim)
test_recom = get_recommendations(test_matrix, sector_sim)

# ---------------------------
# 6. Merge Recommendations + Hot Encoding
# ---------------------------
train_combined = train_recom.merge(train_hot, on='ACCT', how='left').merge(test_hot, on='ACCT', how='left')
test_combined = test_recom.merge(test_hot, on='ACCT', how='left').merge(train_hot, on='ACCT', how='left')

# ---------------------------
# 7. Weight Summary
# ---------------------------
weight_summary = pd.DataFrame({
    'Metric': ['Mean Variance (Amount)', 'Mean Variance (Frequency)', 'Weight (Amount)', 'Weight (Frequency)'],
    'Value': [train_amt_var, train_freq_var, train_amt_wt, train_freq_wt]
})

# ---------------------------
# 8. Export to Excel
# ---------------------------
output_path = "Recommender_Output_Final.xlsx"

with pd.ExcelWriter(output_path, engine="xlsxwriter") as writer:
    train_recom.to_excel(writer, sheet_name="Train_Recommendations", index=False)
    test_recom.to_excel(writer, sheet_name="Test_Recommendations", index=False)
    train_hot.to_excel(writer, sheet_name="Train_Hot_Encoding", index=False)
    test_hot.to_excel(writer, sheet_name="Test_Hot_Encoding", index=False)
    train_combined.to_excel(writer, sheet_name="Train_Combined", index=False)
    test_combined.to_excel(writer, sheet_name="Test_Combined", index=False)
    weight_summary.to_excel(writer, sheet_name="Weight_Summary", index=False)

print(f"✅ File saved at: {output_path}")

























import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

# Load and prepare data
df = pd.read_csv("recomtrain_test_data.csv")
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df.dropna(subset=['Date'], inplace=True)
df['ACCT'] = df['ACCT'].astype(str)

split_date = pd.to_datetime("2024-12-31")
test_end_date = pd.to_datetime("2025-03-31")
train_df = df[df['Date'] < split_date]
test_df = df[(df['Date'] >= split_date) & (df['Date'] <= test_end_date)]
sectors = sorted(df['Sector'].unique())

# Create amount and frequency matrices
def build_amount_freq(df, sectors):
    amt = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
    freq = df.groupby(['ACCT', 'Sector']).size().unstack(fill_value=0)
    return amt.reindex(columns=sectors, fill_value=0), freq.reindex(columns=sectors, fill_value=0)

# Normalize + calculate variance-based weights
def get_weighted_matrix(amt, freq):
    scaler = MinMaxScaler()
    amt_norm = pd.DataFrame(scaler.fit_transform(amt), index=amt.index, columns=amt.columns)
    freq_norm = pd.DataFrame(scaler.fit_transform(freq), index=freq.index, columns=freq.columns)

    amt_var = amt_norm.var(axis=1).mean()
    freq_var = freq_norm.var(axis=1).mean()
    total = amt_var + freq_var
    amt_wt = freq_var / total
    freq_wt = amt_var / total

    combined = amt_wt * amt_norm + freq_wt * freq_norm
    return combined, amt_wt, freq_wt, amt_var, freq_var

# One-hot encoding
def get_hot_encoding(df, sectors):
    binary = df.groupby(['ACCT', 'Sector']).size().unstack(fill_value=0)
    binary[binary > 0] = 1
    return binary.reindex(columns=sectors, fill_value=0).reset_index()

# Recommendation ranks
def get_recommendations(matrix, similarity):
    norm = pd.DataFrame(MinMaxScaler().fit_transform(matrix), index=matrix.index, columns=matrix.columns)
    scores = norm.dot(similarity)
    return scores.rank(axis=1, method='min', ascending=False).astype(int).reset_index()

# Sector-wise rank vs test-hot summary
def get_sector_summary(test_recom, test_hot, train_hot, sectors):
    summary = {}
    for s in sectors:
        temp = test_recom[['ACCT', s]].rename(columns={s: 'Rank'})
        temp = temp.merge(test_hot[['ACCT', s]].rename(columns={s: 'Used_in_Test'}), on='ACCT')
        temp = temp.merge(train_hot[['ACCT', s]].rename(columns={s: 'Used_in_Train'}), on='ACCT')
        pivot = pd.pivot_table(temp, index='Rank', columns='Used_in_Test', values='ACCT', aggfunc='count', fill_value=0).reset_index()
        summary[s] = pivot
    return summary

# Build matrices
train_amt, train_freq = build_amount_freq(train_df, sectors)
test_amt, test_freq = build_amount_freq(test_df, sectors)

train_matrix, train_amt_wt, train_freq_wt, train_amt_var, train_freq_var = get_weighted_matrix(train_amt, train_freq)
test_matrix, *_ = get_weighted_matrix(test_amt, test_freq)

# Similarity & encodings
sector_sim = pd.DataFrame(cosine_similarity(train_matrix.T), index=sectors, columns=sectors)
train_hot = get_hot_encoding(train_df, sectors)
test_hot = get_hot_encoding(test_df, sectors)

# Recommendations
train_recom = get_recommendations(train_matrix, sector_sim)
test_recom = get_recommendations(test_matrix, sector_sim)

# Summary sheets
summary_tables = get_sector_summary(test_recom, test_hot, train_hot, sectors)

# Export
with pd.ExcelWriter("Recommender_Output.xlsx", engine="xlsxwriter") as writer:
    train_recom.to_excel(writer, sheet_name="Train_Recommendations", index=False)
    test_recom.to_excel(writer, sheet_name="Test_Recommendations", index=False)
    train_hot.to_excel(writer, sheet_name="Train_Hot_Encoding", index=False)
    test_hot.to_excel(writer, sheet_name="Test_Hot_Encoding", index=False)

    pd.DataFrame({
        'Metric': ['Mean Variance (Amount)', 'Mean Variance (Frequency)', 'Weight (Amount)', 'Weight (Frequency)'],
        'Value': [train_amt_var, train_freq_var, train_amt_wt, train_freq_wt]
    }).to_excel(writer, sheet_name="Weight_Summary", index=False)

    for s, tbl in summary_tables.items():
        tbl.to_excel(writer, sheet_name=s[:31], index=False)

print("✅ All outputs written to 'Recommender_Output.xlsx'")



























Subject: Urgent: Incomplete TID Mapping for Palladium Mall Campaign

Hi [Recipient’s Name],

We’ve observed a critical issue regarding the Palladium Mall campaign. While validating recent customer transactions, we identified that a spend at Superdry (Palladium) was considered eligible for the offer. However, the TID (Terminal ID) associated with this transaction does not match any of the TIDs currently shared with us for Palladium Mall outlets.

This raises a concern that we might not have the complete list of TIDs associated with outlets participating in the campaign. If any eligible TIDs are missing, it could result in:
	•	Incorrect exclusion of valid customer spends,
	•	Inaccurate offer qualification, and
	•	Potential customer dissatisfaction and escalations.

Request you to please urgently re-verify and share the full and final list of TIDs for all outlets in Palladium Mall that are covered under this campaign. If not already done, please ensure the list includes every terminal in use, especially for high-footfall brands like Superdry.

Looking forward to your immediate response, as this may impact ongoing campaign execution and offer fulfillment.

Best regards,
[Your Name]
[Your Designation]













Planner_Clean =
SUMMARIZE(
    Planner,
    Planner[People_Soft_ID],
    "Planner_Name", MAX(Planner[Planner_Name])  // Or use FIRSTNONBLANK(...)
)

RM_Map_Master :=
SELECTCOLUMNS (
    FILTER (
        ADDCOLUMNS (
            DISTINCT (
                UNION (
                    SELECTCOLUMNS('Base File', "People_Soft_ID", 'Base File'[People_Soft_ID]),
                    SELECTCOLUMNS(Planner_Clean, "People_Soft_ID", Planner_Clean[People_Soft_ID])
                )
            ),
            "Planner_Name",
                COALESCE(
                    LOOKUPVALUE(
                        Planner_Clean[Planner_Name],
                        Planner_Clean[People_Soft_ID], [People_Soft_ID]
                    ),
                    "Unassigned"
                )
        ),
        NOT ISBLANK([People_Soft_ID])
    ),
    "People_Soft_ID", [People_Soft_ID],
    "Planner_Name", [Planner_Name]
)




RM_Map_Master :=
SELECTCOLUMNS (
    FILTER (
        ADDCOLUMNS (
            DISTINCT (
                UNION (
                    SELECTCOLUMNS('Base File', "People_Soft_ID", 'Base File'[People_Soft_ID]),
                    SELECTCOLUMNS(Planner, "People_Soft_ID", Planner[People_Soft_ID])
                )
            ),
            "Planner_Name",
                COALESCE(
                    LOOKUPVALUE(
                        Planner[Planner_Name],
                        Planner[People_Soft_ID], [People_Soft_ID]
                    ),
                    "Unassigned"
                )
        ),
        NOT ISBLANK([People_Soft_ID])
    ),
    "People_Soft_ID", [People_Soft_ID],
    "Planner_Name", [Planner_Name]
)




RM_Map_Master :=
VAR All_IDs =
    UNION(
        SELECTCOLUMNS('Base File', "People_Soft_ID", 'Base File'[People_Soft_ID]),
        SELECTCOLUMNS(Planner, "People_Soft_ID", Planner[People_Soft_ID])
    )

RETURN
SELECTCOLUMNS(
    ADDCOLUMNS(
        DISTINCT(All_IDs),
        "Planner_Name",
            COALESCE(
                CALCULATE(
                    MAX(Planner[Planner_Name])
                ),
                "Unassigned"
            )
    ),
    "People_Soft_ID", [People_Soft_ID],
    "Planner_Name", [Planner_Name]
)







RM_Map =
SELECTCOLUMNS (
    FILTER (
        ADDCOLUMNS (
            INTERSECT (
                VALUES(Planner[People_Soft_ID]),
                VALUES('Base File'[People_Soft_ID])
            ),
            "Planner_Name", 
                COALESCE(
                    CALCULATE(
                        MAX(Planner[Planner_Name])
                    ),
                    "Unassigned"
                )
        ),
        NOT ISBLANK([People_Soft_ID])
    ),
    "People_Soft_ID", [People_Soft_ID],
    "Planner_Name", [Planner_Name]
)






RM_Map =
SELECTCOLUMNS (
    FILTER (
        ADDCOLUMNS (
            VALUES(Planner[People_Soft_ID]),
            "Planner_Name", CALCULATE(
                MAX(Planner[Planner_Name])
            )
        ),
        NOT ISBLANK([People_Soft_ID])
    ),
    "People_Soft_ID", [People_Soft_ID],
    "Planner_Name", [Planner_Name]
)






RM_Map =
ADDCOLUMNS(
    SUMMARIZE(Planner, Planner[People_Soft_ID]),
    "Planner_Name",
    COALESCE(MAX(Planner[Planner_Name]), "Unassigned")
)




RM_Map =
ADDCOLUMNS(
    SUMMARIZE(Planner, Planner[People_Soft_ID]),
    "Planner_Name",
    COALESCE(MAX(Planner[Planner_Name]), "Unassigned")
)







Casa Open (Universal) :=
VAR IsRowLevel = ISINSCOPE(Planner[Planner_Name])
RETURN
    IF(
        IsRowLevel,
        SUM('Base File'[casa_open]),
        SUMX(
            VALUES(Planner[Planner_Name]),
            CALCULATE(SUM('Base File'[casa_open]))
        )
    )



Casa Grow (Safe) :=
VAR IsRowLevel = ISINSCOPE(Planner[Planner_Name])
VAR GroupSelected = SELECTEDVALUE('Base File'[Group])
VAR UseSafeMode = 
    GroupSelected = "DBMI_BAS"
        && CALCULATE(
            COUNTROWS('Base File'),
            FILTER(
                'Base File',
                ISBLANK(
                    LOOKUPVALUE(
                        Planner[Planner_Name],
                        Planner[People_Soft_ID], 'Base File'[People_Soft_ID]
                    )
                )
            )
        ) > 0

RETURN
    SWITCH(
        TRUE(),
        IsRowLevel, SUM('Base File'[casa_grow]),  -- Row values
        UseSafeMode, SUMX(VALUES(Planner[Planner_Name]), CALCULATE(SUM('Base File'[casa_grow]))),  -- Total fix
        SUM('Base File'[casa_grow])  -- Default for all other cases
    )










Has Unmatched RM :=
VAR CountUnmatched =
    CALCULATE(
        COUNTROWS(BaseTable),
        FILTER(
            BaseTable,
            BaseTable[Group] = "DBMI_BAS"
                && ISBLANK(
                    LOOKUPVALUE(
                        Planner[RM_Name],
                        Planner[ps_id], BaseTable[ps_id]
                    )
                )
        )
    )
RETURN
    IF(
        CountUnmatched > 0,
        "❌ Issue: Unmatched RM Exists",
        "✅ Clean"
    )











Has Unmatched RM :=
VAR CountUnmatched =
    CALCULATE(
        COUNTROWS(BaseTable),
        FILTER(
            BaseTable,
            BaseTable[Group] = "DBMI_BAS"
                && ISBLANK(
                    LOOKUPVALUE(
                        Planner[RM_Name],
                        Planner[ps_id], BaseTable[ps_id]
                    )
                )
        )
    )
RETURN
    IF(
        CountUnmatched > 0,
        "❌ Issue: Unmatched RM Exists",
        "✅ Clean"
    )Has Unmatched RM :=
VAR CountUnmatched =
    CALCULATE(
        COUNTROWS(BaseTable),
        FILTER(
            BaseTable,
            BaseTable[Group] = "DBMI_BAS"
                && ISBLANK(RELATED(Planner[RM_Name]))
        )
    )
RETURN
    IF(
        CountUnmatched > 0,
        "❌ Issue: Unmatched RM Exists",
        "✅ Clean"
    )








Has Unmatched RM :=
IF(
    CALCULATE(
        COUNTROWS(BaseTable),
        BaseTable[Group] = "DBMI_BAS",
        ISBLANK(RELATED(Planner[RM_Name]))
    ) > 0,
    "❌ Issue: Unmatched RM Exists",
    "✅ Clean"
)











EVALUATE
FILTER (
    ADDCOLUMNS (
        BaseTable,
        "Matched RM", RELATED(Planner[RM_Name])
    ),
    ISBLANK ( [Matched RM] )
        && BaseTable[Group] = "DBMI_BAS"
)





Casa Grow Final :=
IF(
    ISINSCOPE(Planner[RM_Name]),
    SUM(BaseTable[casa_grow]),
    SUMX(
        VALUES(Planner[RM_Name]),
        CALCULATE(SUM(BaseTable[casa_grow]))
    )
)




import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules

# Step 1: Load data
df = pd.read_csv("your_file.csv")  # Replace with your actual file path
df['Date'] = pd.to_datetime(df['Date'])

# Step 2: No date filtering (entire dataset will be used)

# Step 3: Filter accounts with at least 5 transactions
txn_count = df.groupby('ACCT')['Sector'].count()
valid_accts = txn_count[txn_count >= 5].index
df_filtered = df[df['ACCT'].isin(valid_accts)]

# Step 4: Create basket
basket = df_filtered.groupby(['ACCT', 'Sector'])['Date'].count().unstack().fillna(0)
basket = basket.applymap(lambda x: 1 if x > 0 else 0)

# Step 5: Apriori frequent itemsets
frequent_items = apriori(basket, min_support=0.001, use_colnames=True)

# Step 6: Generate all rules
rules_all = association_rules(frequent_items, metric="lift", min_threshold=1.0)

# Step 7: Keep only rules with target consequents
target_consequents = {'Entertainment', 'Travel', 'Shopping', 'Groceries'}
rules_targeted = rules_all[rules_all['consequents'].apply(
    lambda x: len(x) == 1 and list(x)[0] in target_consequents
)].copy()

# Step 8: Identify rule level
rules_targeted['antecedent_len'] = rules_targeted['antecedents'].apply(lambda x: len(x))

# Step 9: Create three rule sets
rules_1 = rules_targeted[rules_targeted['antecedent_len'] == 1].sort_values(by='lift', ascending=False).head(20)
rules_2 = rules_targeted[rules_targeted['antecedent_len'] == 2].sort_values(by='lift', ascending=False).head(20)
rules_3 = rules_targeted[rules_targeted['antecedent_len'] == 3].sort_values(by='lift', ascending=False).head(20)

# Step 10: Combine for overall top 20
rules_combined = pd.concat([rules_1, rules_2, rules_3])
rules_overall_top20 = rules_combined.sort_values(by='lift', ascending=False).head(20)

# Step 11: Clean formatting
for df in [rules_1, rules_2, rules_3, rules_overall_top20, rules_all]:
    df['antecedents'] = df['antecedents'].apply(lambda x: ', '.join(sorted(x)))
    df['consequents'] = df['consequents'].apply(lambda x: ', '.join(sorted(x)))

# Step 12: Export to Excel with all 5 outputs
with pd.ExcelWriter("final_mba_rules_all_5_outputs.xlsx") as writer:
    rules_1[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name="Rule_1_X_to_Y", index=False)
    rules_2[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name="Rule_2_XY_to_Z", index=False)
    rules_3[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name="Rule_3_XYZ_to_A", index=False)
    rules_all[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name="Normal_MBA_All", index=False)
    rules_overall_top20[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name="Overall_Top20_Rules", index=False)









import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules

# Step 1: Load data
df = pd.read_csv("your_file.csv")  # Replace with actual path
df['Date'] = pd.to_datetime(df['Date'])

# Step 2: Filter last 3 months
end_date = df['Date'].max()
start_date = end_date - pd.DateOffset(months=3)
df_recent = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]

# Step 3: Filter accounts with at least 5 transactions
txn_count = df_recent.groupby('ACCT')['Sector'].count()
valid_accts = txn_count[txn_count >= 5].index
df_recent = df_recent[df_recent['ACCT'].isin(valid_accts)]

# Step 4: Create basket
basket = df_recent.groupby(['ACCT', 'Sector'])['Date'].count().unstack().fillna(0)
basket = basket.applymap(lambda x: 1 if x > 0 else 0)

# Step 5: Apriori itemsets
frequent_items = apriori(basket, min_support=0.001, use_colnames=True)

# Step 6: Generate all rules
rules_all = association_rules(frequent_items, metric="lift", min_threshold=1.0)

# Step 7: Filter for only target consequents
target_consequents = {'Entertainment', 'Travel', 'Shopping', 'Groceries'}
rules_targeted = rules_all[rules_all['consequents'].apply(
    lambda x: len(x) == 1 and list(x)[0] in target_consequents
)].copy()

# Step 8: Add antecedent length
rules_targeted['antecedent_len'] = rules_targeted['antecedents'].apply(lambda x: len(x))

# Step 9: Create 3 rule levels
rules_1 = rules_targeted[rules_targeted['antecedent_len'] == 1].sort_values(by='lift', ascending=False).head(20)
rules_2 = rules_targeted[rules_targeted['antecedent_len'] == 2].sort_values(by='lift', ascending=False).head(20)
rules_3 = rules_targeted[rules_targeted['antecedent_len'] == 3].sort_values(by='lift', ascending=False).head(20)

# Step 10: Combine Top 20 from all 3 into 1 list and sort by lift
rules_combined = pd.concat([rules_1, rules_2, rules_3])
rules_overall_top20 = rules_combined.sort_values(by='lift', ascending=False).head(20)

# Step 11: Clean string formatting
for df in [rules_1, rules_2, rules_3, rules_overall_top20, rules_all]:
    df['antecedents'] = df['antecedents'].apply(lambda x: ', '.join(sorted(x)))
    df['consequents'] = df['consequents'].apply(lambda x: ', '.join(sorted(x)))

# Step 12: Export all to Excel
with pd.ExcelWriter("final_mba_rules_all_5_outputs.xlsx") as writer:
    rules_1[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name="Rule_1_X_to_Y", index=False)
    rules_2[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name="Rule_2_XY_to_Z", index=False)
    rules_3[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name="Rule_3_XYZ_to_A", index=False)
    rules_all[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name="Normal_MBA_All", index=False)
    rules_overall_top20[['antecedents', 'consequents', 'support', 'confidence', 'lift']].to_excel(writer, sheet_name="Overall_Top20_Rules", index=False)












# Merge with spend summary
sector_spend = df.groupby('sector')['amount'].sum().reset_index()

# Filter out zero-spend sectors
valid_sectors = sector_spend[sector_spend['amount'] > 0]['sector']
df = df[df['sector'].isin(valid_sectors)]





import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import os

# --------------------------
# Step 1: Load Data
# --------------------------
# Replace 'your_file.csv' with your actual filename
df = pd.read_csv("your_file.csv", parse_dates=['date'])

# --------------------------
# Step 2: Filter Last 3 Months & Active Customers
# --------------------------
latest_date = df['date'].max()
three_months_ago = latest_date - pd.DateOffset(months=3)
df = df[df['date'] >= three_months_ago]

acct_txn_counts = df.groupby('ACCT')['date'].nunique()
valid_accts = acct_txn_counts[acct_txn_counts >= 5].index
df = df[df['ACCT'].isin(valid_accts)]

# --------------------------
# Step 3: Create Basket per ACCT
# --------------------------
baskets = df.groupby('ACCT')['sector'].apply(list).tolist()

# --------------------------
# Step 4: Transaction Encoding
# --------------------------
te = TransactionEncoder()
te_array = te.fit(baskets).transform(baskets)
df_encoded = pd.DataFrame(te_array, columns=te.columns_)

# --------------------------
# Step 5: Frequent Itemsets & Rules
# --------------------------
frequent_items = apriori(df_encoded, min_support=0.02, use_colnames=True)
rules = association_rules(frequent_items, metric='lift', min_threshold=1.0)

rules['antecedents'] = rules['antecedents'].apply(lambda x: sorted(list(x)))
rules['consequents'] = rules['consequents'].apply(lambda x: sorted(list(x)))
rules['antecedents_str'] = rules['antecedents'].apply(lambda x: ', '.join(x))
rules['consequents_str'] = rules['consequents'].apply(lambda x: ', '.join(x))
rules['score'] = rules['lift'] * rules['confidence']

# --------------------------
# Step 6: Keep Only 1-Sector Consequents
# --------------------------
rules = rules[rules['consequents'].apply(lambda x: len(x) == 1)]

# --------------------------
# Step 7: Remove A↔B Duplicate Pairs (Keep Strongest Only)
# --------------------------
rules['rule_key'] = rules.apply(
    lambda row: tuple(sorted([row['antecedents_str'], row['consequents_str']])),
    axis=1
)
rules = rules.sort_values(by='lift', ascending=False)
rules = rules.drop_duplicates(subset='rule_key')

# --------------------------
# Step 8: Build Tiered Rules
# --------------------------
top20_classic = rules.nlargest(20, 'lift').reset_index(drop=True)
rule1 = rules[rules['antecedents'].apply(len) == 1].nlargest(20, 'lift').reset_index(drop=True)
rule2 = rules[rules['antecedents'].apply(len) == 2].nlargest(20, 'lift').reset_index(drop=True)
rule3 = rules[rules['antecedents'].apply(len) == 3].nlargest(20, 'lift').reset_index(drop=True)

combined_rules = pd.concat([rule1, rule2, rule3], ignore_index=True)
top20_combined = combined_rules.nlargest(20, 'lift').reset_index(drop=True)

# --------------------------
# Step 9: Save Outputs
# --------------------------
output_dir = "market_basket_outputs"
os.makedirs(output_dir, exist_ok=True)

rule1.to_csv(f"{output_dir}/top20_rule1_X_to_Y.csv", index=False)
rule2.to_csv(f"{output_dir}/top20_rule2_XY_to_Z.csv", index=False)
rule3.to_csv(f"{output_dir}/top20_rule3_XYZ_to_A.csv", index=False)
top20_classic.to_csv(f"{output_dir}/top20_classic_rules.csv", index=False)
top20_combined.to_csv(f"{output_dir}/top20_combined_rules.csv", index=False)

print("✅ Final outputs saved in folder: market_basket_outputs")












rules = association_rules(frequent_items, metric='lift', min_threshold=1.0)

# ✅ Keep only 1-item consequents
rules = rules[rules['consequents'].apply(lambda x: len(x) == 1)]








# --------------------------
# Step 7: Export to Output Folder
# --------------------------
output_dir = "market_basket_outputs"
os.makedirs(output_dir, exist_ok=True)

rule1.to_csv(f"{output_dir}/top20_rule1_X_to_Y.csv", index=False)
rule2.to_csv(f"{output_dir}/top20_rule2_XY_to_Z.csv", index=False)
rule3.to_csv(f"{output_dir}/top20_rule3_XYZ_to_A.csv", index=False)
top20_classic.to_csv(f"{output_dir}/top20_classic_rules.csv", index=False)
top20_combined.to_csv(f"{output_dir}/top20_combined_rules.csv", index=False)

print("✅ All 5 rule sets exported to folder: market_basket_outputs")





import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import os

# --------------------------
# Step 1: Load Your Data
# --------------------------
# Your CSV must contain: Customer_ID, Transaction_Date, Sector_Level
df = pd.read_csv("your_file.csv", parse_dates=['Transaction_Date'])

# --------------------------
# Step 2: Apply Filters
# --------------------------
# Filter: Last 3 months only
latest_date = df['Transaction_Date'].max()
three_months_ago = latest_date - pd.DateOffset(months=3)
df = df[df['Transaction_Date'] >= three_months_ago]

# Filter: Customers with ≥ 5 transactions
cust_counts = df.groupby('Customer_ID')['Transaction_Date'].nunique()
valid_customers = cust_counts[cust_counts >= 5].index
df = df[df['Customer_ID'].isin(valid_customers)]

# --------------------------
# Step 3: Create Sector Basket per Customer
# --------------------------
baskets = df.groupby('Customer_ID')['Sector_Level'].apply(list).tolist()

# --------------------------
# Step 4: Transaction Encoding
# --------------------------
te = TransactionEncoder()
te_data = te.fit(baskets).transform(baskets)
df_encoded = pd.DataFrame(te_data, columns=te.columns_)

# --------------------------
# Step 5: Frequent Itemsets & Rules
# --------------------------
frequent_items = apriori(df_encoded, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_items, metric='lift', min_threshold=1.0)

# Clean up and prep string columns
rules['antecedents'] = rules['antecedents'].apply(lambda x: sorted(list(x)))
rules['consequents'] = rules['consequents'].apply(lambda x: sorted(list(x)))
rules['antecedents_str'] = rules['antecedents'].apply(lambda x: ', '.join(x))
rules['consequents_str'] = rules['consequents'].apply(lambda x: ', '.join(x))
rules['score'] = rules['lift'] * rules['confidence']

# --------------------------
# Step 6: Rule Segmentations
# --------------------------

# 1. Classic MBA: Top 20 by lift
top20_classic = rules.nlargest(20, 'lift').reset_index(drop=True)

# 2. Rule 1 (X ➝ Y): Antecedents with 1 item
rule1 = rules[rules['antecedents'].apply(len) == 1].nlargest(20, 'lift').reset_index(drop=True)

# 3. Rule 2 (X & Y ➝ Z): Antecedents with 2 items
rule2 = rules[rules['antecedents'].apply(len) == 2].nlargest(20, 'lift').reset_index(drop=True)

# 4. Rule 3 (X, Y, Z ➝ A): Antecedents with 3 items
rule3 = rules[rules['antecedents'].apply(len) == 3].nlargest(20, 'lift').reset_index(drop=True)

# 5. Combined Best 20 from Rule1, Rule2, Rule3
combined_rules = pd.concat([rule1, rule2, rule3], ignore_index=True)
top20_combined = combined_rules.nlargest(20, 'lift').reset_index(drop=True)

# --------------------------
# Step 7: Export to CSV
# --------------------------
rule1.to_csv("top20_rule1_X_to_Y.csv", index=False)
rule2.to_csv("top20_rule2_XY_to_Z.csv", index=False)
rule3.to_csv("top20_rule3_XYZ_to_A.csv", index=False)
top20_classic.to_csv("top20_classic_rules.csv", index=False)
top20_combined.to_csv("top20_combined_rules.csv", index=False)

print("✅ All 5 rule sets exported successfully:")
print("- top20_rule1_X_to_Y.csv")
print("- top20_rule2_XY_to_Z.csv")
print("- top20_rule3_XYZ_to_A.csv")
print("- top20_classic_rules.csv")
print("- top20_combined_rules.csv")












import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
from pyvis.network import Network
import os

# ---------------------------
# Step 1: Load Your Data
# ---------------------------
# The dataset must contain: Customer_ID, Transaction_Date, Sector_Level
df = pd.read_csv("your_file.csv", parse_dates=['Transaction_Date'])

# ---------------------------
# Step 2: Filter Data
# ---------------------------

# Keep only last 3 months of data
latest_date = df['Transaction_Date'].max()
cutoff_date = latest_date - pd.DateOffset(months=3)
df = df[df['Transaction_Date'] >= cutoff_date]

# Keep only customers with ≥ 5 transactions
cust_txn_counts = df.groupby('Customer_ID')['Transaction_Date'].nunique()
valid_customers = cust_txn_counts[cust_txn_counts >= 5].index
df = df[df['Customer_ID'].isin(valid_customers)]

# ---------------------------
# Step 3: Create Basket per Customer
# ---------------------------
basket_data = df.groupby('Customer_ID')['Sector_Level'].apply(list).tolist()

# ---------------------------
# Step 4: One-Hot Encode Transactions
# ---------------------------
te = TransactionEncoder()
te_data = te.fit(basket_data).transform(basket_data)
df_encoded = pd.DataFrame(te_data, columns=te.columns_)

# ---------------------------
# Step 5: Generate Frequent Itemsets and Rules
# ---------------------------
frequent_items = apriori(df_encoded, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_items, metric='lift', min_threshold=1.0)

# Prepare rule strings for filtering and visualization
rules['antecedents'] = rules['antecedents'].apply(lambda x: sorted(list(x)))
rules['consequents'] = rules['consequents'].apply(lambda x: sorted(list(x)))
rules['antecedents_str'] = rules['antecedents'].apply(lambda x: ', '.join(x))
rules['consequents_str'] = rules['consequents'].apply(lambda x: ', '.join(x))
rules['score'] = rules['lift'] * rules['confidence']

# ---------------------------
# Step 6: Extract 3 Rule Levels
# ---------------------------

# Rule 1: If X ➝ Y
rule1 = rules[rules['antecedents'].apply(len) == 1].nlargest(20, 'lift').reset_index(drop=True)

# Rule 2: If X & Y ➝ Z
rule2 = rules[rules['antecedents'].apply(len) == 2].nlargest(20, 'lift').reset_index(drop=True)

# Rule 3: If X, Y & Z ➝ A
rule3 = rules[rules['antecedents'].apply(len) == 3].nlargest(20, 'lift').reset_index(drop=True)

# ---------------------------
# Step 7: Export Rules to CSV
# ---------------------------
rule1.to_csv("top20_rule1_X_to_Y.csv", index=False)
rule2.to_csv("top20_rule2_XY_to_Z.csv", index=False)
rule3.to_csv("top20_rule3_XYZ_to_A.csv", index=False)

# ---------------------------
# Step 8: PyVis Network Visualization
# ---------------------------
def create_pyvis_graph(rule_df, output_file):
    g = Network(height="600px", width="100%", notebook=False, directed=True)
    for _, row in rule_df.iterrows():
        ant = row['antecedents_str']
        con = row['consequents_str']
        g.add_node(ant, label=ant)
        g.add_node(con, label=con)
        g.add_edge(ant, con, title=f"Confidence: {row['confidence']:.2f} | Lift: {row['lift']:.2f}")
    g.show(output_file)

# Create and save interactive network graphs
create_pyvis_graph(rule1, "rule1_X_to_Y.html")
create_pyvis_graph(rule2, "rule2_XY_to_Z.html")
create_pyvis_graph(rule3, "rule3_XYZ_to_A.html")

print("✅ Market Basket Analysis completed successfully.")















import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
from pyvis.network import Network
import os

# -----------------------
# Step 1: Load Data
# -----------------------
# Sample input: Transaction_ID, Item
df = pd.read_csv("transactions.csv")  # Replace with your actual file

# Convert transactions into list of item lists
basket_list = df.groupby("Transaction_ID")['Item'].apply(list).tolist()

# -----------------------
# Step 2: Encode Transaction Data
# -----------------------
te = TransactionEncoder()
te_array = te.fit(basket_list).transform(basket_list)
df_encoded = pd.DataFrame(te_array, columns=te.columns_)

# -----------------------
# Step 3: Frequent Itemsets & Rules
# -----------------------
frequent_items = apriori(df_encoded, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_items, metric="lift", min_threshold=1.0)

# Clean and prepare rule columns
rules['antecedents'] = rules['antecedents'].apply(lambda x: sorted(list(x)))
rules['consequents'] = rules['consequents'].apply(lambda x: sorted(list(x)))
rules['antecedents_str'] = rules['antecedents'].apply(lambda x: ', '.join(x))
rules['consequents_str'] = rules['consequents'].apply(lambda x: ', '.join(x))
rules['score'] = rules['lift'] * rules['confidence']

# -----------------------
# Step 4: Extract Top 20 Rules per Level
# -----------------------
# Rule 1: If X ➔ Y
rule1 = rules[rules['antecedents'].apply(len) == 1].nlargest(20, 'lift').reset_index(drop=True)

# Rule 2: If X & Y ➔ Z
rule2 = rules[rules['antecedents'].apply(len) == 2].nlargest(20, 'lift').reset_index(drop=True)

# Rule 3: If X & Y & Z ➔ A
rule3 = rules[rules['antecedents'].apply(len) == 3].nlargest(20, 'lift').reset_index(drop=True)

# -----------------------
# Step 5: Export Top Rules
# -----------------------
rule1.to_csv("top20_rule1_X_to_Y.csv", index=False)
rule2.to_csv("top20_rule2_XY_to_Z.csv", index=False)
rule3.to_csv("top20_rule3_XYZ_to_A.csv", index=False)

# -----------------------
# Step 6: Generate PyVis Graphs
# -----------------------
def create_pyvis_graph(rule_df, output_html):
    g = Network(height="600px", width="100%", notebook=False, directed=True)
    for _, row in rule_df.iterrows():
        ant = row['antecedents_str']
        con = row['consequents_str']
        g.add_node(ant, label=ant)
        g.add_node(con, label=con)
        g.add_edge(ant, con, title=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}")
    g.show(output_html)

create_pyvis_graph(rule1, "rule1_X_to_Y.html")
create_pyvis_graph(rule2, "rule2_XY_to_Z.html")
create_pyvis_graph(rule3, "rule3_XYZ_to_A.html")

print("✅ Done! Top 20 rules per level exported and visualized.")















# 8. Enhanced Sankey Diagram Function
# ------------------------------------
def plot_sankey(rules_df, title):
    if rules_df.empty:
        print(f"No rules to display for {title}.")
        return

    # Prepare nodes: sorted for clean order
    antecedent_nodes = sorted(set().union(*rules_df['antecedents']))
    consequent_nodes = sorted(set().union(*rules_df['consequents']))
    nodes = antecedent_nodes + [n for n in consequent_nodes if n not in antecedent_nodes]

    node_map = {k: v for v, k in enumerate(nodes)}
    sources, targets, values, customdata = [], [], [], []

    for _, row in rules_df.iterrows():
        for antecedent in row['antecedents']:
            for consequent in row['consequents']:
                sources.append(node_map[antecedent])
                targets.append(node_map[consequent])
                values.append(row['confidence'])
                customdata.append(f"Lift: {row['lift']:.2f}, Support: {row['support']:.2f}")

    # Assign colors: blue for antecedents, orange for consequents
    colors = ['#1f77b4' if n in antecedent_nodes else '#ff7f0e' for n in nodes]

    fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=20,
            thickness=25,
            line=dict(color="black", width=0.5),
            label=nodes,
            color=colors,
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            hovertemplate='Conf: %{value:.2f}<br>%{customdata}<extra></extra>',
            customdata=customdata,
            color="rgba(150,150,150,0.5)"
        ))])

    fig.update_layout(
        title_text=title,
        font_size=12,
        height=700,
        width=1200,
        plot_bgcolor='white',
        paper_bgcolor='white'
    )
    fig.show()

# ------------------------------------
# 9. Filter and Display Each Rule Level
# ------------------------------------

# A. If X ➔ Y
rules_XY = rules[(rules['antecedents'].apply(len) == 1) &
                 (rules['consequents'].apply(len) == 1)]
print("✅ If X ➔ Y Rules")
print(rules_XY[['antecedents','consequents','support','confidence','lift']])
plot_sankey(rules_XY, "If X ➔ Y Sankey Diagram")

# B. If X and Y ➔ Z
rules_XY_Z = rules[(rules['antecedents'].apply(len) == 2) &
                   (rules['consequents'].apply(len) == 1)]
print("\n✅ If X and Y ➔ Z Rules")
print(rules_XY_Z[['antecedents','consequents','support','confidence','lift']])
plot_sankey(rules_XY_Z, "If X and Y ➔ Z Sankey Diagram")

# C. If X and Y and Z ➔ A
rules_XYZ_A = rules[(rules['antecedents'].apply(len) == 3) &
                    (rules['consequents'].apply(len) == 1)]
print("\n✅ If X and Y and Z ➔ A Rules")
print(rules_XYZ_A[['antecedents','consequents','support','confidence','lift']])
plot_sankey(rules_XYZ_A, "If X and Y and Z ➔ A Sankey Diagram")








# ------------------------------------
# 1. Import Required Libraries
# ------------------------------------
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import networkx as nx
import plotly.graph_objects as go

# ------------------------------------
# 2. User-defined Thresholds
# ------------------------------------
min_lift = float(input("Enter minimum lift threshold (e.g. 1.2): "))
min_confidence = float(input("Enter minimum confidence threshold (e.g. 0.6): "))

# ------------------------------------
# 3. Load Data
# ------------------------------------
df = pd.read_csv('your_data.csv')  # Replace with your file path

# ------------------------------------
# 4. Data Preprocessing
# ------------------------------------
df['MT_EFF_DATE'] = pd.to_datetime(df['MT_EFF_DATE'])

# Filter last 3 months
max_date = df['MT_EFF_DATE'].max()
start_date = max_date - pd.DateOffset(months=3)
filtered_df = df[df['MT_EFF_DATE'] >= start_date].copy()

# Filter customers with ≥5 transactions
cust_txn_counts = filtered_df.groupby('ACCT').size()
eligible_customers = cust_txn_counts[cust_txn_counts >= 5].index
filtered_df = filtered_df[filtered_df['ACCT'].isin(eligible_customers)].copy()

# Create basket as customer level
filtered_df['basket_id'] = filtered_df['ACCT'].astype(str)

# Group sectors per customer and deduplicate
basket_sector = filtered_df.groupby('basket_id')['sector'].apply(lambda x: list(set(x))).tolist()

# ------------------------------------
# 5. One-Hot Encoding
# ------------------------------------
te = TransactionEncoder()
te_ary = te.fit(basket_sector).transform(basket_sector)
basket_df = pd.DataFrame(te_ary, columns=te.columns_)

# ------------------------------------
# 6. Apriori Frequent Itemsets
# ------------------------------------
frequent_itemsets = apriori(basket_df, min_support=0.01, use_colnames=True, max_len=4)

# ------------------------------------
# 7. Generate Association Rules
# ------------------------------------
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# Filter by user-defined lift and confidence
rules = rules[(rules['lift'] >= min_lift) & (rules['confidence'] >= min_confidence)]

# ------------------------------------
# 8. Filter Rule Levels
# ------------------------------------

# A. If X ➔ Y
rules_XY = rules[(rules['antecedents'].apply(len) == 1) &
                 (rules['consequents'].apply(len) == 1)]

# B. If X and Y ➔ Z
rules_XY_Z = rules[(rules['antecedents'].apply(len) == 2) &
                   (rules['consequents'].apply(len) == 1)]

# C. If X and Y and Z ➔ A
rules_XYZ_A = rules[(rules['antecedents'].apply(len) == 3) &
                    (rules['consequents'].apply(len) == 1)]

# ------------------------------------
# 9. Display Outputs
# ------------------------------------
print("✅ If X ➔ Y Rules")
print(rules_XY[['antecedents','consequents','support','confidence','lift']])

print("\n✅ If X and Y ➔ Z Rules")
print(rules_XY_Z[['antecedents','consequents','support','confidence','lift']])

print("\n✅ If X and Y and Z ➔ A Rules")
print(rules_XYZ_A[['antecedents','consequents','support','confidence','lift']])

# ------------------------------------
# 10. Interactive Plotly Network Graph Function
# ------------------------------------
def plotly_network_graph(rules_df, title):
    G = nx.DiGraph()

    # Build NetworkX graph
    for _, row in rules_df.iterrows():
        for antecedent in row['antecedents']:
            for consequent in row['consequents']:
                G.add_edge(antecedent, consequent,
                           weight=row['lift'],
                           label=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}")

    pos = nx.spring_layout(G)

    edge_x = []
    edge_y = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x += [x0, x1, None]
        edge_y += [y0, y1, None]

    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=2, color='#888'),
        hoverinfo='none',
        mode='lines')

    node_x = []
    node_y = []
    node_text = []
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_text.append(node)

    node_trace = go.Scatter(
        x=node_x, y=node_y,
        mode='markers+text',
        hoverinfo='text',
        marker=dict(
            showscale=False,
            color='skyblue',
            size=20,
            line_width=2),
        text=node_text,
        textposition="bottom center"
    )

    fig = go.Figure(data=[edge_trace, node_trace],
                    layout=go.Layout(
                        title=title,
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=20,l=5,r=5,t=40),
                        xaxis=dict(showgrid=False, zeroline=False),
                        yaxis=dict(showgrid=False, zeroline=False))
                   )

    fig.show()

# ------------------------------------
# 11. Generate Interactive Graphs
# ------------------------------------

# If X ➔ Y
plotly_network_graph(rules_XY, "If X ➔ Y Rules Network Graph (Plotly)")

# If X and Y ➔ Z
plotly_network_graph(rules_XY_Z, "If X and Y ➔ Z Rules Network Graph (Plotly)")

# If X and Y and Z ➔ A
plotly_network_graph(rules_XYZ_A, "If X and Y and Z ➔ A Rules Network Graph (Plotly)")





















# -------------------------------
# 1. Import Required Libraries
# -------------------------------
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
from pyvis.network import Network

# -------------------------------
# 2. Load Data
# -------------------------------
df = pd.read_csv('your_data.csv')  # Replace with your actual file path

# -------------------------------
# 3. Data Preprocessing
# -------------------------------
# Convert 'MT_EFF_DATE' to datetime
df['MT_EFF_DATE'] = pd.to_datetime(df['MT_EFF_DATE'])

# Filter data for last 3 months
max_date = df['MT_EFF_DATE'].max()
start_date = max_date - pd.DateOffset(months=3)
filtered_df = df[df['MT_EFF_DATE'] >= start_date].copy()

# Filter customers with at least 5 transactions
cust_txn_counts = filtered_df.groupby('ACCT').size()
eligible_customers = cust_txn_counts[cust_txn_counts >= 5].index
filtered_df = filtered_df[filtered_df['ACCT'].isin(eligible_customers)].copy()

# Define basket as customer-level
filtered_df['basket_id'] = filtered_df['ACCT'].astype(str)

# Group sectors per customer and deduplicate
basket_sector = filtered_df.groupby('basket_id')['sector'].apply(lambda x: list(set(x))).tolist()

# -------------------------------
# 4. One-Hot Encoding
# -------------------------------
te = TransactionEncoder()
te_ary = te.fit(basket_sector).transform(basket_sector)
basket_df = pd.DataFrame(te_ary, columns=te.columns_)

# -------------------------------
# 5. Apriori Frequent Itemsets
# -------------------------------
frequent_itemsets = apriori(basket_df, min_support=0.01, use_colnames=True, max_len=4)

# -------------------------------
# 6. Generate Association Rules
# -------------------------------
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# -------------------------------
# 7. Filter Different Rule Levels
# -------------------------------

# A. If X ➔ Y
rules_XY = rules[(rules['antecedents'].apply(len) == 1) &
                 (rules['consequents'].apply(len) == 1)]

# B. If X and Y ➔ Z
rules_XY_Z = rules[(rules['antecedents'].apply(len) == 2) &
                   (rules['consequents'].apply(len) == 1)]

# C. If X and Y and Z ➔ A
rules_XYZ_A = rules[(rules['antecedents'].apply(len) == 3) &
                    (rules['consequents'].apply(len) == 1)]

# -------------------------------
# 8. Display Rule Outputs
# -------------------------------
print("✅ If X ➔ Y Rules")
print(rules_XY[['antecedents','consequents','support','confidence','lift']])

print("\n✅ If X and Y ➔ Z Rules")
print(rules_XY_Z[['antecedents','consequents','support','confidence','lift']])

print("\n✅ If X and Y and Z ➔ A Rules")
print(rules_XYZ_A[['antecedents','consequents','support','confidence','lift']])

# -------------------------------
# 9. Interactive Graph Function Using PyVis
# -------------------------------
def interactive_rules_graph(rules_df, filename, title):
    net = Network(height='600px', width='100%', notebook=True, directed=True)
    net.force_atlas_2based()

    # Add nodes and edges
    for _, row in rules_df.iterrows():
        for antecedent in row['antecedents']:
            net.add_node(antecedent, label=antecedent)
        for consequent in row['consequents']:
            net.add_node(consequent, label=consequent)
        for antecedent in row['antecedents']:
            for consequent in row['consequents']:
                net.add_edge(antecedent, consequent,
                             title=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}",
                             value=row['confidence'])

    net.show_buttons()
    net.show(filename)
    print(f"✅ {title} graph saved as {filename}")

# -------------------------------
# 10. Generate Interactive Graphs for Each Rule Level
# -------------------------------

# If X ➔ Y
interactive_rules_graph(rules_XY, 'rules_XY_graph.html', "If X ➔ Y")

# If X and Y ➔ Z
interactive_rules_graph(rules_XY_Z, 'rules_XY_Z_graph.html', "If X and Y ➔ Z")

# If X and Y and Z ➔ A
interactive_rules_graph(rules_XYZ_A, 'rules_XYZ_A_graph.html', "If X and Y and Z ➔ A")






















# After date filtering

# Calculate transaction count per customer
cust_txn_counts = filtered_df.groupby('cusid').size()

# Filter customers with at least 5 transactions
eligible_customers = cust_txn_counts[cust_txn_counts >= 5].index

# Filter dataset
filtered_df = filtered_df[filtered_df['cusid'].isin(eligible_customers)].copy()

print(f"Using {len(eligible_customers)} customers with >=5 transactions for analysis")











# 1. Import Libraries
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import networkx as nx
import matplotlib.pyplot as plt

# 2. Load Data
df = pd.read_csv('your_data.csv')  # Replace with your actual file path

# 3. Convert 'date' column to datetime (good practice for consistency)
df['date'] = pd.to_datetime(df['date'])

# 4. Filter Data for Desired Time Period (Example: Last 3 months from max date)
max_date = df['date'].max()
start_date = max_date - pd.DateOffset(months=3)

filtered_df = df[df['date'] >= start_date].copy()

print(f"Using data from {start_date.date()} to {max_date.date()}")

# ✔️ Alternatively, for custom period selection:
# start_date = pd.to_datetime('2025-04-01')
# end_date = pd.to_datetime('2025-06-30')
# filtered_df = df[(df['date'] >= start_date) & (df['date'] <= end_date)].copy()

# 5. Define Basket as Customer Level for Overall Affinity
filtered_df['basket_id'] = filtered_df['cusid'].astype(str)

# 6. Group into unique sector lists per customer (deduplicated)
basket_sector = filtered_df.groupby('basket_id')['sector'].apply(lambda x: list(set(x))).tolist()

# 7. One-Hot Encoding
te = TransactionEncoder()
te_ary = te.fit(basket_sector).transform(basket_sector)
basket_df = pd.DataFrame(te_ary, columns=te.columns_)

# 8. Apply Apriori Algorithm
frequent_itemsets = apriori(basket_df, min_support=0.005, use_colnames=True)

# 9. Generate Association Rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# 10. Filter Strong Rules (example: Lift > 1.2, Confidence > 0.3)
strong_rules = rules[(rules['lift'] > 1.2) & (rules['confidence'] > 0.3)]

# Display Strong Rules
print("Strong Association Rules:")
print(strong_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# 11. Visualization using NetworkX
G = nx.DiGraph()

# Add edges with lift as edge attribute
for _, row in strong_rules.iterrows():
    for antecedent in row['antecedents']:
        for consequent in row['consequents']:
            G.add_edge(antecedent, consequent, weight=row['lift'], label=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}")

# Draw Network Graph
plt.figure(figsize=(12,8))
pos = nx.spring_layout(G, k=0.5)

nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='skyblue')
nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20)
nx.draw_networkx_labels(G, pos, font_size=10, font_family="sans-serif")

edge_labels = nx.get_edge_attributes(G, 'label')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9)

plt.title("Overall Customer Purchase Affinity - Strong Rules Network Graph")
plt.axis('off')
plt.show()
























# 1. Import Libraries
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import networkx as nx
import matplotlib.pyplot as plt

# 2. Load Data
df = pd.read_csv('your_data.csv')  # Replace with your file path

# 3. Convert 'date' column to datetime
df['date'] = pd.to_datetime(df['date'])

# 4. Filter Data for Desired Time Period
# Example: Filter last 3 months from max date in dataset

# Calculate max date
max_date = df['date'].max()

# Calculate start date for last 3 months
start_date = max_date - pd.DateOffset(months=3)

# Apply filter
filtered_df = df[df['date'] >= start_date].copy()

print(f"Using data from {start_date.date()} to {max_date.date()}")

# ✔️ Alternatively, for custom period selection:
# start_date = pd.to_datetime('2025-04-01')
# end_date = pd.to_datetime('2025-06-30')
# filtered_df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]

# 5. Create Basket ID
filtered_df['basket_id'] = filtered_df['cusid'].astype(str) + "_" + filtered_df['date'].astype(str)

# 6. Group into transaction lists
basket_sector = filtered_df.groupby('basket_id')['sector'].apply(list).tolist()

# 7. One-Hot Encoding
te = TransactionEncoder()
te_ary = te.fit(basket_sector).transform(basket_sector)
basket_df = pd.DataFrame(te_ary, columns=te.columns_)

# 8. Apply Apriori
frequent_itemsets = apriori(basket_df, min_support=0.01, use_colnames=True)

# 9. Generate Association Rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# 10. Filter Strong Rules: Lift > 1.2, Confidence > 0.6
strong_rules = rules[(rules['lift'] > 1.2) & (rules['confidence'] > 0.6)]

# Display Strong Rules
print("Strong Association Rules:")
print(strong_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# 11. Visualization using NetworkX
G = nx.DiGraph()

# Add edges with lift as edge attribute
for _, row in strong_rules.iterrows():
    for antecedent in row['antecedents']:
        for consequent in row['consequents']:
            G.add_edge(antecedent, consequent, weight=row['lift'], label=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}")

# Draw Network Graph
plt.figure(figsize=(12,8))
pos = nx.spring_layout(G, k=0.5)

nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='skyblue')
nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20)
nx.draw_networkx_labels(G, pos, font_size=10, font_family="sans-serif")

edge_labels = nx.get_edge_attributes(G, 'label')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9)

plt.title("Market Basket Analysis Strong Rules Network Graph")
plt.axis('off')
plt.show()









# 1. Import Libraries
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import networkx as nx
import matplotlib.pyplot as plt

# 2. Load Data
df = pd.read_csv('your_data.csv')  # Replace with your actual file

# 3. Create Basket ID
df['basket_id'] = df['cusid'].astype(str) + "_" + df['date'].astype(str)

# 4. Group into transaction lists
basket_sector = df.groupby('basket_id')['sector'].apply(list).tolist()

# 5. One-Hot Encoding
te = TransactionEncoder()
te_ary = te.fit(basket_sector).transform(basket_sector)
basket_df = pd.DataFrame(te_ary, columns=te.columns_)

# 6. Apply Apriori
frequent_itemsets = apriori(basket_df, min_support=0.01, use_colnames=True)

# 7. Generate Association Rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# 8. Filter Strong Rules: Lift > 1.2, Confidence > 0.6
strong_rules = rules[(rules['lift'] > 1.2) & (rules['confidence'] > 0.6)]

# Display Strong Rules
print("Strong Association Rules:")
print(strong_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# 9. Visualization using NetworkX
G = nx.DiGraph()

# Add edges with lift as edge attribute
for _, row in strong_rules.iterrows():
    for antecedent in row['antecedents']:
        for consequent in row['consequents']:
            G.add_edge(antecedent, consequent, weight=row['lift'], label=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}")

# Draw Network Graph
plt.figure(figsize=(12,8))
pos = nx.spring_layout(G, k=0.5)  # k controls distance between nodes

# Draw nodes and edges
nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='skyblue')
nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20)
nx.draw_networkx_labels(G, pos, font_size=10, font_family="sans-serif")

# Draw edge labels
edge_labels = nx.get_edge_attributes(G, 'label')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9)

plt.title("Market Basket Analysis Strong Rules Network Graph")
plt.axis('off')
plt.show()




















Sub WaterfallPivotWithUpdatedConditions()
    Dim ws As Worksheet
    Set ws = ThisWorkbook.Sheets("Sheet1") ' Change if needed

    Dim pvt As PivotTable
    Set pvt = ws.PivotTables(1)

    Dim pf As PivotField
    Dim i As Long
    Dim filterVal As String

    Application.ScreenUpdating = False ' For faster execution

    For i = 1 To 62
        Set pf = pvt.PageFields(i)

        ' Set filter value based on conditions
        Select Case i
            Case 38
                filterVal = "Not Part of"
            Case 51 To 54
                filterVal = "ELIGIBLE"
            Case 60 To 61
                filterVal = "1"
            Case 62
                filterVal = "OTHR"
            Case Else
                filterVal = "0"
        End Select

        ' Apply the filter; skip silently if value not found
        On Error Resume Next
        pf.CurrentPage = filterVal
        On Error GoTo 0

        ' Refresh pivot after applying each filter
        pvt.RefreshTable

        ' Copy sum cust and acct from A65 and B65 to E & F in row i
        ws.Cells(i, "E").Value = ws.Range("A65").Value
        ws.Cells(i, "F").Value = ws.Range("B65").Value

        ' Log applied filter value in column D
        ws.Cells(i, "D").Value = filterVal
    Next i

    Application.ScreenUpdating = True

    MsgBox "Waterfall extraction complete with updated conditions.", vbInformation
End Sub















Sub WaterfallPivotWithExceptions()
    Dim ws As Worksheet
    Set ws = ThisWorkbook.Sheets("Sheet1") ' Change if needed

    Dim pvt As PivotTable
    Set pvt = ws.PivotTables(1)

    Dim pf As PivotField
    Dim i As Long
    Dim filterVal As String

    For i = 1 To 61
        Set pf = pvt.PageFields(i)

        ' Set filter value based on conditions
        Select Case i
            Case 38
                filterVal = "Not Part of"
            Case 51 To 54
                filterVal = "ELIGIBLE"
            Case Else
                filterVal = "0"
        End Select

        ' Try applying the filter (skip if value not found)
        On Error Resume Next
        pf.CurrentPage = filterVal
        On Error GoTo 0

        ' Refresh pivot after each change
        pvt.RefreshTable

        ' Copy sum cust and acct to columns E and F
        ws.Cells(i, "E").Value = ws.Range("A64").Value
        ws.Cells(i, "F").Value = ws.Range("B64").Value

        ' Optional: log applied filter value in column D
        ws.Cells(i, "D").Value = filterVal
    Next i

    MsgBox "Waterfall extraction complete.", vbInformation
End Sub









Sub WaterfallPivotFilterCopy()
    Dim ws As Worksheet
    Set ws = ThisWorkbook.Sheets("Sheet1") ' Update if needed

    Dim pvt As PivotTable
    Set pvt = ws.PivotTables(1) ' Assuming one pivot table

    Dim i As Long
    Dim pf As PivotField

    ' Loop through filters in A1 to A61
    For i = 1 To 61
        Set pf = pvt.PageFields(i) ' Gets the ith Page filter

        ' Apply filter = "0"
        On Error Resume Next
        pf.CurrentPage = "0"
        On Error GoTo 0

        ' Refresh pivot after each filter update
        pvt.RefreshTable

        ' Copy A64 (cust) and B64 (acct) to E & F of current row
        ws.Cells(i, "E").Value = ws.Range("A64").Value
        ws.Cells(i, "F").Value = ws.Range("B64").Value
    Next i

    MsgBox "Waterfall copy completed for all filters set to 0", vbInformation
End Sub











@echo off
echo [%DATE% %TIME%] === STEP 1: Running Python Web Scraping ===
"C:\Python311\python.exe" "C:\Scripts\python_script_1.py"
IF %ERRORLEVEL% NEQ 0 (
    echo Python Script 1 failed. Exiting.
    exit /b %ERRORLEVEL%
)

echo [%DATE% %TIME%] === STEP 2: Running SAS Processing ===
"C:\Program Files\SASHome\SASFoundation\9.4\sas.exe" -sysin "C:\Scripts\sas_script.sas" -log "C:\Logs\sas_script_%DATE:/=-%.log"
IF %ERRORLEVEL% NEQ 0 (
    echo SAS Script failed. Exiting.
    exit /b %ERRORLEVEL%
)

echo [%DATE% %TIME%] === STEP 3: Running SharePoint Upload Python Script ===
"C:\Python311\python.exe" "C:\Scripts\python_script_2.py"
IF %ERRORLEVEL% NEQ 0 (
    echo Python Script 2 failed. Exiting.
    exit /b %ERRORLEVEL%
)

echo [%DATE% %TIME%] === ALL STEPS COMPLETED SUCCESSFULLY ===








def generate_hot_encoding(df, sectors):
    interaction_matrix = df.pivot_table(index='ACT', columns='Sector', values='TRAN_AMT', aggfunc='sum')
    interaction_matrix = interaction_matrix.notnull().astype(int)
    for sector in sectors:
        if sector not in interaction_matrix.columns:
            interaction_matrix[sector] = 0
    return interaction_matrix[sorted(sectors)]












def generate_hot_encoding(df, sectors):
    df = df[['ACT', 'Sector']].dropna()
    df['Sector'] = df['Sector'].astype(str).str.strip()

    hot_encoded = pd.get_dummies(df, columns=['Sector'], prefix='', prefix_sep='_') \
                     .groupby('ACT').max()

    # Align with master sector list
    for sector in sectors:
        if sector not in hot_encoded.columns:
            hot_encoded[sector] = 0

    # Ensure column order
    return hot_encoded[sorted(sectors)]





import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

# ----------------------
# Load and Preprocess
# ----------------------
def load_and_preprocess_data(filepath):
    df = pd.read_csv(filepath)
    df['MT_EFF_DATE'] = pd.to_datetime(df['MT_EFF_DATE'], errors='coerce')
    df.dropna(subset=['MT_EFF_DATE'], inplace=True)
    return df

# ----------------------
# Interaction Matrix
# ----------------------
def generate_interaction_matrix(df, sectors, metric='amount'):
    if metric == 'amount':
        matrix = df.groupby(['ACT', 'Sector'])['TRAN_AMT'].sum().unstack(fill_value=0)
    elif metric == 'frequency':
        matrix = df.groupby(['ACT', 'Sector']).size().unstack(fill_value=0)
    else:
        raise ValueError("Metric must be 'amount' or 'frequency'")
    
    for sector in sectors:
        if sector not in matrix.columns:
            matrix[sector] = 0
    return matrix[sorted(sectors)].astype(np.float32)

# ----------------------
# Normalize Matrix
# ----------------------
def scale_matrix(matrix):
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(matrix)
    return pd.DataFrame(scaled, index=matrix.index, columns=matrix.columns)

# ----------------------
# One-hot Encoding
# ----------------------
def generate_hot_encoding(df, sectors):
    hot_encoded = pd.get_dummies(df[['ACT', 'Sector']], columns=['Sector'], prefix='', prefix_sep='_') \
                    .groupby('ACT').max()
    for sector in sectors:
        if sector not in hot_encoded.columns:
            hot_encoded[sector] = 0
    return hot_encoded[sorted(sectors)]

# ----------------------
# Combine Cosine Similarities
# ----------------------
def combine_similarity(amount_matrix, frequency_matrix, weight_amount=0.6):
    sim_amt = cosine_similarity(amount_matrix.T)
    sim_freq = cosine_similarity(frequency_matrix.T)
    return weight_amount * sim_amt + (1 - weight_amount) * sim_freq

# ----------------------
# Rank Recommendations
# ----------------------
def get_recommendation_ranks(test_matrix, sector_similarity):
    scores = np.dot(test_matrix, sector_similarity)
    ranks = (-scores).argsort(axis=1).argsort(axis=1) + 1
    return pd.DataFrame(ranks, index=test_matrix.index, columns=test_matrix.columns)

# ----------------------
# Rank Actual Transaction
# ----------------------
def generate_transaction_ranks(df, sectors):
    matrix = generate_interaction_matrix(df, sectors, metric='amount')
    return matrix.rank(axis=1, method='max', ascending=False).astype(int)

# ----------------------
# Main Execution
# ----------------------
def run_dual_matrix_recommender(filepath, split_date_str, output_prefix):
    # Load & prep
    df = load_and_preprocess_data(filepath)
    split_date = pd.to_datetime(split_date_str)
    sectors = sorted(df['Sector'].dropna().unique())

    # Split train/test
    train_df = df[df['MT_EFF_DATE'] < split_date]
    test_df = df[df['MT_EFF_DATE'] >= split_date]

    # Matrices
    train_amt_matrix = generate_interaction_matrix(train_df, sectors, metric='amount')
    train_freq_matrix = generate_interaction_matrix(train_df, sectors, metric='frequency')
    test_amt_matrix = generate_interaction_matrix(test_df, sectors, metric='amount')

    # Normalize
    train_amt_scaled = scale_matrix(train_amt_matrix)
    train_freq_scaled = scale_matrix(train_freq_matrix)
    test_amt_scaled = scale_matrix(test_amt_matrix)

    # Combined similarity
    sector_similarity = combine_similarity(train_amt_scaled, train_freq_scaled, weight_amount=0.6)

    # Hot encodings
    train_hot_encoding = generate_hot_encoding(train_df, sectors)
    test_hot_encoding = generate_hot_encoding(test_df, sectors)

    # Ranks
    test_recommendations = get_recommendation_ranks(test_amt_scaled, sector_similarity)
    test_transaction_ranks = generate_transaction_ranks(test_df, sectors)

    # Export
    test_recommendations.to_csv(f'{output_prefix}_recommendations.csv')

    with pd.ExcelWriter(f'{output_prefix}_full_output.xlsx', engine='openpyxl') as writer:
        train_hot_encoding.to_excel(writer, sheet_name='Train Hot Encoding')
        test_hot_encoding.to_excel(writer, sheet_name='Test Hot Encoding')
        test_recommendations.to_excel(writer, sheet_name='Recommendations')
        test_transaction_ranks.to_excel(writer, sheet_name='Actual Transaction Ranks')

    print(f"✅ Recommendation engine completed using both amount & frequency (weight 60/40).")
    print(f"📁 Files saved to: {output_prefix}_recommendations.csv and {output_prefix}_full_output.xlsx")

# Example run
run_dual_matrix_recommender(
    filepath='Z:/WORK/Persona Data/CSAT_LV_SPEND1.csv',
    split_date_str='2024-12-31',
    output_prefix='Z:/WORK/Persona Data/FinalRecommender_June18'
)



















/* Step 1: Setup options and define remote controller */
options nomprint nosymbolgen NETENCRYPTALGORITHM=TRIPLEDES noconnectmetaconnection;

%let control = aspsas2-cnt-eng.hk.hsbc 7551;
options remote=control;

/* Step 2: Safely terminate and re-establish a clean remote connection */
signoff control;
signon control user="your_user_id" password="your_password";   /* Replace with secure credentials or metadata binding */

/* Step 3: RSUBMIT block to handle binary file transfer */
rsubmit;

/* Define source ZIP file on HK SAS server (do NOT use encoding with binary files) */
filename rfile "/sasdata/hsbc/dil/INM/IMCE/external_data_transfer/GupShup/landing/inbound/hsbc_premier_23rdto26thApril2025";

/* Define destination ZIP file on INM SAS server */
filename lfile "/appvol/mix_nas/inm/INMDNA/IMCE/sandbox/Shikhar/WA_auto/hsbc_premier_23rdto26thApril2025.zip";

/* Perform secure binary download */
proc download infile=rfile outfile=lfile binary;
run;

endrsubmit;

/* Step 4: Close the session */
signoff control;










/* Step 1: General Options */
options nomprint nosymbolgen NETENCRYPTALGORITHM=TRIPLEDES noconnectmetaconnection;

/* Step 2: Define remote server and port */
%let control = aspsas2-cnt-eng.hk.hsbc 7551;
options remote=control;

/* Step 3: Safely sign off any existing session and sign on with credentials */
signoff control;
signon control user="your_user_id" password="your_password";  /* 🔒 Use secure method in production */

/* Step 4: Begin RSUBMIT to HK Grid */
rsubmit;

/* Define source file path on HK SAS server */
filename rfile "/sasdata/hsbc/dil/INM/IMCE/external_data_transfer/GupShup/landing/inbound/hsbc_premier_23rdto26thApril2025";

/* Define destination path on INM SAS server */
filename lfile "/appvol/mix_nas/inm/INMDNA/IMCE/sandbox/Shikhar/WA_auto/hsbc_premier_23rdto26thApril2025.zip";

/* Download the file from remote to local in binary mode */
proc download infile=rfile outfile=lfile binary;
run;

endrsubmit;

/* Step 5: Close the remote connection */
signoff control;







%macro transfer_file(remote_path=, local_path=, user=, password=);
    options nomprint nosymbolgen NETENCRYPTALGORITHM=TRIPLEDES;

    %let control=aspsas2-cnt-eng.hk.hsbc 7551;
    options remote=control;

    signoff control;
    signon control user="&user" password="&password";

    rsubmit;
        filename rfile "&remote_path";
        filename lfile "&local_path";
        proc download infile=rfile outfile=lfile binary;
        run;
    endrsubmit;

    signoff control;
%mend;

%transfer_file(
    remote_path=/sasdata/hsbc/dil/INM/IMCE/external_data_transfer/GupShup/landing/inbound/hsbc_premier_23rdto26thApril2025,
    local_path=/appvol/mix_nas/inm/INMDNA/IMCE/sandbox/Shikhar/WA_auto/hsbc_premier_23rdto26thApril2025.zip,
    user=your_user,
    password=your_password
);















Sub MultiFileLookupAllDirections()
    Dim wsConfig As Worksheet, wsMaster As Worksheet
    Dim lookupKey As String, variablesToExtract() As String
    Dim fDialog As FileDialog, folderPath As String
    Dim fileName As String, wbSource As Workbook, wsSource As Worksheet
    Dim headerDict As Object, resultDict As Object, tempDict As Object
    Dim keyVal As Variant, varName As Variant
    Dim masterKeyColLetter As String, masterKeyCol As Long
    Dim lastRow As Long, i As Long, j As Long, outputColStart As Long
    Dim srcLastRow As Long, srcLastCol As Long
    Dim rowArr As Variant, headers As Variant

    Application.ScreenUpdating = False
    Application.DisplayAlerts = False
    Application.EnableEvents = False

    ' Setup sheets
    Set wsConfig = ThisWorkbook.Sheets("Config")
    Set wsMaster = ThisWorkbook.Sheets("MasterList")

    ' Read config
    lookupKey = Trim(wsConfig.Range("B1").Value)
    variablesToExtract = Split(wsConfig.Range("B2").Value, ",")
    For i = 0 To UBound(variablesToExtract)
        variablesToExtract(i) = Trim(variablesToExtract(i))
    Next i

    ' Prompt for folder
    Set fDialog = Application.FileDialog(msoFileDialogFolderPicker)
    With fDialog
        .Title = "Select Folder Containing Excel Files"
        If .Show <> -1 Then
            MsgBox "No folder selected. Exiting.", vbExclamation
            Exit Sub
        End If
        folderPath = .SelectedItems(1) & "\"
    End With

    ' Prompt for master column
    masterKeyColLetter = InputBox("Enter the column letter (A-Z) in 'MasterList' that contains the lookup key:", "Select Lookup Column", "A")
    If masterKeyColLetter = "" Then Exit Sub
    masterKeyCol = Range(masterKeyColLetter & "1").Column

    ' Collect lookup values
    Dim lookupDict As Object
    Set lookupDict = CreateObject("Scripting.Dictionary")
    lastRow = wsMaster.Cells(wsMaster.Rows.Count, masterKeyCol).End(xlUp).Row
    For i = 2 To lastRow
        keyVal = Trim(wsMaster.Cells(i, masterKeyCol).Value)
        If keyVal <> "" Then
            lookupDict(CStr(keyVal)) = i ' store row number
        End If
    Next i

    ' Prepare result dictionary
    Set resultDict = CreateObject("Scripting.Dictionary")

    ' Loop files
    fileName = Dir(folderPath & "*.xls*")
    Do While fileName <> ""
        Set wbSource = Workbooks.Open(folderPath & fileName, False, True)

        For Each wsSource In wbSource.Sheets
            srcLastRow = wsSource.Cells(wsSource.Rows.Count, 1).End(xlUp).Row
            srcLastCol = wsSource.Cells(1, wsSource.Columns.Count).End(xlToLeft).Column
            headers = wsSource.Range(wsSource.Cells(1, 1), wsSource.Cells(1, srcLastCol)).Value

            ' Build header dictionary
            Set headerDict = CreateObject("Scripting.Dictionary")
            For j = 1 To srcLastCol
                If Trim(headers(1, j)) <> "" Then
                    headerDict(Trim(headers(1, j))) = j
                End If
            Next j

            ' Continue only if lookupKey column exists
            If headerDict.exists(lookupKey) Then
                Dim keyCol As Long: keyCol = headerDict(lookupKey)

                For i = 2 To srcLastRow
                    rowArr = wsSource.Range(wsSource.Cells(i, 1), wsSource.Cells(i, srcLastCol)).Value
                    keyVal = rowArr(1, keyCol)
                    If lookupDict.exists(CStr(keyVal)) Then
                        If Not resultDict.exists(CStr(keyVal)) Then
                            Set tempDict = CreateObject("Scripting.Dictionary")
                            For Each varName In variablesToExtract
                                If headerDict.exists(varName) Then
                                    tempDict(varName) = rowArr(1, headerDict(varName))
                                End If
                            Next varName
                            resultDict(CStr(keyVal)) = tempDict
                        End If
                    End If
                Next i
            End If
        Next wsSource

        wbSource.Close SaveChanges:=False
        fileName = Dir
    Loop

    ' Output to MasterList
    outputColStart = masterKeyCol + 1
    For j = 0 To UBound(variablesToExtract)
        wsMaster.Cells(1, outputColStart + j).Value = variablesToExtract(j)
    Next j

    For Each keyVal In resultDict.Keys
        i = lookupDict(keyVal)
        Set tempDict = resultDict(keyVal)
        For j = 0 To UBound(variablesToExtract)
            varName = variablesToExtract(j)
            If tempDict.exists(varName) Then
                wsMaster.Cells(i, outputColStart + j).Value = tempDict(varName)
            End If
        Next j
    Next keyVal

    MsgBox "Lookup complete. Data written to 'MasterList'.", vbInformation

    Application.ScreenUpdating = True
    Application.DisplayAlerts = True
    Application.EnableEvents = True
End Sub





















proc univariate data=your_dataset noprint;
    var propensity_score;
    output out=percentile_cutoffs
        pctlpts = 10 20 30 40 50 60 70 80 90
        pctlpre = P_;
run;

data final_with_deciles;
    if _N_ = 1 then set percentile_cutoffs; /* Load percentiles once */
    set your_dataset;

    if propensity_score <= P_10 then decile = 1;
    else if propensity_score <= P_20 then decile = 2;
    else if propensity_score <= P_30 then decile = 3;
    else if propensity_score <= P_40 then decile = 4;
    else if propensity_score <= P_50 then decile = 5;
    else if propensity_score <= P_60 then decile = 6;
    else if propensity_score <= P_70 then decile = 7;
    else if propensity_score <= P_80 then decile = 8;
    else if propensity_score <= P_90 then decile = 9;
    else decile = 10;
run;

















proc rank data=your_dataset out=ranked_dataset groups=10;
    var propensity_score;
    ranks decile;
run;

data ranked_dataset;
    set ranked_dataset;
    decile = decile + 1; /* To make deciles 1 to 10 instead of 0 to 9 */
run;












/* Step 1: Filter Card Only customers from Dec 2023 */
proc sql;
    create table dec23_card_only as
    select cusid, final_cus_seg as seg_dec23
    from dec23_data
    where final_cus_seg = "Card Only";
quit;

/* Step 2: Filter active customers from Dec 2024 */
proc sql;
    create table dec24_active as
    select cusid, final_cus_seg as seg_dec24
    from dec24_data
    where active_dec24 = 1;
quit;

/* Step 3: Join and build transition labels */
proc sql;
    create table card_only_transitions as
    select 
        a.cusid,
        a.seg_dec23,
        coalesce(b.seg_dec24, "Dropped") as seg_dec24,
        cats(a.seg_dec23, " → ", coalesce(b.seg_dec24, "Dropped")) as segment_transition
    from dec23_card_only a
    left join dec24_active b
    on a.cusid = b.cusid;
quit;

/* Step 4: Summary counts by transition */
proc freq data=card_only_transitions;
    tables segment_transition / nocum nopercent;
run;








proc sql noprint;
    select max(tran_date) into :last_date from trans_data;
quit;

/* Step 2: Compute activity summary using last_date as anchor */
proc sql;
    create table cust_activity_summary as
    select 
        cust_id,
        count(distinct intnx('month', tran_date, 0, 'b')) as active_months,

        case 
            when max(intnx('month', tran_date, 0, 'b')) >= intnx('month', &last_date, -3, 'b') 
                then "3M Active"
            when max(intnx('month', tran_date, 0, 'b')) >= intnx('month', &last_date, -6, 'b') 
                then "6M Active"
            when max(intnx('month', tran_date, 0, 'b')) >= intnx('month', &last_date, -9, 'b') 
                then "9M Active"
            else "Inactive"
        end as recent_activity_label
    from trans_data
    group by cust_id;
quit;












import py7zr
import os

def zip_csv_with_py7zr(csv_file_path, zip_file_path, password):
    if not os.path.exists(csv_file_path):
        raise FileNotFoundError(f"{csv_file_path} does not exist.")

    with py7zr.SevenZipFile(zip_file_path, 'w', password=password) as archive:
        archive.write(csv_file_path, arcname=os.path.basename(csv_file_path))

    print(f"✅ AES-encrypted 7z file created: {os.path.abspath(zip_file_path)}")

# === USAGE ===
csv_file = 'sample.csv'
zip_file = 'secure_sample.7z'
password = 'StrongAES123'

zip_csv_with_py7zr(csv_file, zip_file, password)










import zipfile
import os

def zip_with_basic_password(csv_file_path, zip_file_path, password):
    if not os.path.exists(csv_file_path):
        raise FileNotFoundError(f"{csv_file_path} does not exist.")

    with zipfile.ZipFile(zip_file_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:
        # Write file and apply password
        zf.setpassword(password.encode())
        zf.write(csv_file_path, arcname=os.path.basename(csv_file_path))

    print(f"✅ Basic password-protected ZIP created: {zip_file_path}")

# === USAGE ===
csv_file = 'sample.csv'
zip_file = 'sample_protected.zip'
password = 'Basic123'

zip_with_basic_password(csv_file, zip_file, password)










/*********************************************************************
*  SAS Automated Workflow with Dual-Key Deduplication                *
*  - Direct SFTP folder access                                       *
*  - Master log tracks both mobile_number and cusid                  *
**********************************************************************/

/* --- PARAMETERS --- */
%let today = %sysfunc(today(), yymmddn8.);
%let yyyymmdd = %sysfunc(putn(&today, yymmddn8.));
%let local_zip = /sftp/inbox/daily_&yyyymmdd..zip;
%let extract_dir = /sas/data/extract/&yyyymmdd.;
%let extract_file = &extract_dir./inputfile.xlsx;
%let output_dir = /sas/data/output/&yyyymmdd.;
%let output_xlsx = &output_dir./matched_customers.xlsx;
%let output_zip = &output_dir./matched_customers.zip;
%let pwd = YourPassword123;
%let eligible_base = /sas/data/base/eligible_base.sas7bdat;
%let log_file = /sas/data/logs/mobile_master_log.sas7bdat;
%let process_log = /sas/data/logs/master_process_log.sas7bdat;
%let email_to = receiver@email.com;

/* --- 1. Create output/extract dirs if not exist --- */
options noxwait;
x "mkdir -p &extract_dir";
x "mkdir -p &output_dir";

/* --- 2. Unzip file --- */
x "unzip -o &local_zip -d &extract_dir";

/* --- 3. Import Excel --- */
proc import datafile="&extract_file"
  out=raw_data dbms=xlsx replace;
  getnames=yes;
run;

/* --- 4. Filter: OTP=YES and T&C=YES --- */
data filtered;
  set raw_data;
  where upcase(otp)='YES' and upcase(terms_and_conditions)='YES';
run;

/* --- 5. Ensure master log exists --- */
%if %sysfunc(exist(mobile_master_log))=0 %then %do;
  data mobile_master_log;
    length mobile_number $20 cusid $20;
    stop;
  run;
%end;

/* --- 6. Join to eligible base for cusid --- */
proc sql;
  create table matched_base as
  select a.*, b.cusid
  from filtered a
  inner join eligible_base b
    on a.mobile_number=b.mobile_number and a.dob=b.dob;
quit;

/* --- 7. Remove already-processed (dedupe by mobile or cusid) --- */
proc sql;
  create table new_customers as
  select *
  from matched_base
  where not exists (
    select 1 from mobile_master_log
    where matched_base.mobile_number = mobile_master_log.mobile_number
       or matched_base.cusid = mobile_master_log.cusid
  );
quit;

/* --- 8. Append both keys to the master log --- */
data to_append;
  set new_customers(keep=mobile_number cusid);
run;

proc append base=mobile_master_log data=to_append force;
run;

/* --- 9. Export final output --- */
proc export data=new_customers
  outfile="&output_xlsx"
  dbms=xlsx replace;
run;

/* --- 10. Password-protect and Zip --- */
x "zip -j -P &pwd &output_zip &output_xlsx";

/* --- 11. Collect Stats for Summary --- */
proc sql noprint;
  select count(*) into :rec_infile from raw_data;
  select count(*) into :filtered_infile from filtered;
  select count(*) into :matched_count from matched_base;
  select count(*) into :final_count from new_customers;
quit;

%let dedup_count = %eval(&matched_count - &final_count);

/* --- 12. Email With Attachment --- */
filename mymail email
  to=("&email_to")
  subject="Daily Eligible Customer File: &yyyymmdd"
  attach=("&output_zip");

data _null_;
  file mymail;
  put "Summary for &yyyymmdd:";
  put "Total customers in file: &rec_infile";
  put "OTP=YES & T&C=YES: &filtered_infile";
  put "After mobile & DOB match: &matched_count";
  put "Duplicates removed (by mobile or cusid): &dedup_count";
  put "Final unique eligible: &final_count";
run;

/* --- 13. Log the Process --- */
data log_today;
  length file_name $100 status $10;
  format date date9.;
  date = today();
  file_name = "daily_&yyyymmdd..zip";
  total_received = &rec_infile;
  filtered_valid = &filtered_infile;
  matched_base = &matched_count;
  duplicates = &dedup_count;
  final_unique = &final_count;
  status = "SUCCESS";
run;

%if %sysfunc(exist(master_process_log))=0 %then %do;
  data master_process_log;
    length file_name $100 status $10;
    format date date9.;
    stop;
  run;
%end;

proc append base=master_process_log data=log_today force;
run;






















view: spend_view2 {
  sql_table_name: `your_project.your_dataset.SPEND_VIEW_2`

  # === DATE GROUP ===
  dimension_group: mt_posting_date {
    type: time
    timeframes: [raw, date, week, month, quarter, year]
    label: "Posting Date"
    group_label: "Transaction Dates"
    sql: ${TABLE}.MT_POSTING_DATE ;;
  }

  # === BASE DIMENSIONS ===
  dimension: acct {
    type: string
    label: "Account Number"
    group_label: "Account Info"
    sql: ${TABLE}.acct ;;
  }
  dimension: mcc_code {
    type: string
    label: "MCC Code"
    group_label: "Merchant Info"
    sql: CAST(${TABLE}.MCC_CODE AS STRING) ;;
  }
  dimension: description {
    type: string
    label: "MCC Description"
    group_label: "Merchant Info"
    sql: ${TABLE}.Description ;;
  }
  dimension: merchant_details {
    type: string
    label: "Merchant Details"
    group_label: "Merchant Info"
    sql: ${TABLE}.Merchant_Details ;;
  }
  dimension: org {
    type: number
    label: "Org ID"
    group_label: "Product Info"
    sql: ${TABLE}.ORG ;;
  }
  dimension: transaction_type {
    type: string
    label: "Transaction Type"
    group_label: "Txn Details"
    sql: ${TABLE}.TRANSACTION_TYPE ;;
  }
  dimension: spend_type {
    type: string
    label: "Spend Type"
    group_label: "Txn Details"
    sql: ${TABLE}.SPEND_TYPE ;;
  }
  dimension: segment_name {
    type: string
    label: "Segment Name"
    group_label: "Merchant Info"
    sql: ${TABLE}.Segment_Name ;;
  }
  dimension: mt_type {
    type: string
    label: "MT Type"
    group_label: "Product Info"
    sql: ${TABLE}.mt_type ;;
  }
  dimension: spend_place {
    type: string
    label: "Txn Geography"
    group_label: "Txn Details"
    sql: ${TABLE}.Spend_Place ;;
  }
  dimension: product {
    type: string
    label: "Product"
    group_label: "Product Info"
    sql: ${TABLE}.Product ;;
  }
  dimension: spend_amount {
    type: number
    label: "Spend Amount"
    group_label: "Txn Amounts"
    sql: ${TABLE}.Spend_Amount ;;
  }
  dimension: reversal_amount {
    type: number
    label: "Reversal Amount"
    group_label: "Txn Amounts"
    sql: ${TABLE}.Reversal_Amount ;;
  }
  dimension: net_transaction {
    type: number
    label: "Net Transaction"
    group_label: "Txn Amounts"
    sql: ${TABLE}.Net_Transaction ;;
  }
  dimension: spend_amount_cmb {
    type: number
    label: "Spend Amount CMB"
    group_label: "Txn Amounts"
    sql: ${TABLE}.SPEND_AMOUNT_CMB ;;
  }
  dimension: spend_amount_rbmw {
    type: number
    label: "Spend Amount RBMW"
    group_label: "Txn Amounts"
    sql: ${TABLE}.SPEND_AMOUNT_RBMW ;;
  }
  dimension: mt_ref_nbr {
    type: string
    label: "Txn Reference No."
    group_label: "Txn Info"
    primary_key: yes
    sql: ${TABLE}.MT_REF_NBR ;;
  }
  dimension: brand {
    type: string
    label: "Brand"
    group_label: "Merchant Info"
    sql: ${TABLE}.BRAND ;;
  }
  dimension: final_category {
    type: string
    label: "Final Category"
    group_label: "Merchant Info"
    sql: ${TABLE}.FINAL_CATEGORY ;;
  }
  dimension: category_general_desc {
    type: string
    label: "Category General Desc"
    group_label: "Merchant Info"
    sql: ${TABLE}.CATEGORY_GENERAL_DESC ;;
  }
  dimension: category_desc {
    type: string
    label: "Category Desc"
    group_label: "Merchant Info"
    sql: ${TABLE}.CATEGORY_DESC ;;
  }

  # === DERIVED DIMENSIONS ===
  dimension: merchant_name {
    type: string
    label: "Merchant Name"
    group_label: "Merchant Info"
    sql: SUBSTRING(${TABLE}.Merchant_Details, 1, 24) ;;
  }
  dimension: merchant_city {
    type: string
    label: "Merchant City"
    group_label: "Merchant Info"
    sql: REPLACE(SUBSTRING(${TABLE}.Merchant_Details, 24, 14), " ", "") ;;
  }
  dimension: merchant_country {
    type: string
    label: "Merchant Country"
    group_label: "Merchant Info"
    sql: SUBSTRING(${TABLE}.Merchant_Details, 38, 2) ;;
  }
  dimension: mcc_combo {
    type: string
    label: "MCC Combo"
    group_label: "Merchant Info"
    sql: CONCAT(${mcc_code}, " - ", ${description}) ;;
  }

  # === DATE UTILITIES ===
  dimension: today {
    type: date
    label: "Today's Date"
    group_label: "Date Utilities"
    sql: current_date() ;;
  }
  dimension: day {
    type: number
    label: "Current Day"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%e', current_date()) AS INT64) ;;
  }
  dimension: mon {
    type: number
    label: "Current Month"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%m', current_date()) AS INT64) ;;
  }
  dimension: yr {
    type: number
    label: "Current Year"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%Y', current_date()) AS INT64) ;;
  }
  dimension: mt_day {
    type: number
    label: "Txn Day"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%e', ${mt_posting_date_date}) AS INT64) ;;
  }
  dimension: mt_mon {
    type: number
    label: "Txn Month"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%m', ${mt_posting_date_date}) AS INT64) ;;
  }
  dimension: mt_yr {
    type: number
    label: "Txn Year"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%Y', ${mt_posting_date_date}) AS INT64) ;;
  }

  # === PARAMETERS & DYNAMIC MEASURES ===
  parameter: metric_selector {
    label: "Metric Selector"
    group_label: "Switches"
    allowed_value: { label: "Net Spends in Cr" value: "net" }
    allowed_value: { label: "Gross Spends in Cr" value: "gross" }
  }
  measure: monthly_trend_metric {
    type: number
    label: "Monthly Trend Metric"
    group_label: "Switches"
    description: "Net or Gross Spends in Cr as per selector"
    sql: 
      CASE
        WHEN {% parameter metric_selector %} = 'net' THEN ${total_net_transaction}
        WHEN {% parameter metric_selector %} = 'gross' THEN ${total_spend}
        ELSE NULL
      END ;;
  }
  parameter: wordcloud_metric_selector {
    label: "Word Cloud Metric"
    group_label: "Switches"
    allowed_value: { label: "Net Spends in Cr" value: "net" }
    allowed_value: { label: "Gross Spends in Cr" value: "gross" }
  }
  measure: wordcloud_metric {
    type: number
    label: "Word Cloud Metric"
    group_label: "Switches"
    sql: 
      CASE
        WHEN {% parameter wordcloud_metric_selector %} = 'net' THEN ${total_net_transaction}
        WHEN {% parameter wordcloud_metric_selector %} = 'gross' THEN ${total_spend}
        ELSE NULL
      END ;;
  }

  # === CORE MEASURES ===
  measure: total_spend {
    type: sum
    group_label: "Core Spend"
    label: "Gross Spends in Cr"
    sql: ${spend_amount}/10000000 ;;
    value_format: "#,##0.00"
  }
  measure: total_reversal {
    type: sum
    group_label: "Core Spend"
    label: "Reversals in Cr"
    sql: ${reversal_amount}/10000000 ;;
    value_format: "#,##0.00"
  }
  measure: total_net_transaction {
    type: sum
    group_label: "Core Spend"
    label: "Net Spends in Cr"
    sql: ${net_transaction}/10000000 ;;
    value_format: "#,##0.00"
  }
  measure: average_spend {
    type: average
    group_label: "Core Spend"
    label: "Avg Gross Spend/Txn"
    sql: ${spend_amount} ;;
    value_format: "#,##0.00"
  }
  measure: average_net_transaction {
    type: average
    group_label: "Core Spend"
    label: "Avg Net Spend/Txn"
    sql: ${net_transaction} ;;
    value_format: "#,##0.00"
  }
  measure: transaction_count {
    type: count
    group_label: "Core Spend"
    label: "Txn Count"
  }
  measure: unique_accts_cnt {
    type: count_distinct
    group_label: "Core Spend"
    label: "Unique Accounts"
    sql: ${acct} ;;
  }

  # --- MTD MEASURES ---
  measure: total_net_transactions_mtd {
    type: sum
    group_label: "MTD"
    label: "MTD Net Spends in Cr"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }
  measure: total_spends_mtd {
    type: sum
    group_label: "MTD"
    label: "MTD Gross Spends in Cr"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${spend_amount}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }
  measure: total_reversal_mtd {
    type: sum
    group_label: "MTD"
    label: "MTD Reversals in Cr"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${reversal_amount}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }
  measure: avg_net_transactions_mtd {
    type: average
    group_label: "MTD"
    label: "MTD Avg Net Spend/Txn"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction} ELSE NULL END ;;
    value_format: "#,##0.00"
  }

  # --- BUSINESS/ADVANCED MEASURES ---
  measure: total_gross_spend {
    type: sum
    group_label: "Business"
    label: "Total Gross Spend"
    sql: ${spend_amount} ;;
    value_format: "#,##0.00"
  }
  measure: total_net_spend {
    type: sum
    group_label: "Business"
    label: "Total Net Spend"
    sql: ${net_transaction} ;;
    value_format: "#,##0.00"
  }
  measure: total_reversal_amount {
    type: sum
    group_label: "Business"
    label: "Total Reversal Amount"
    sql: ${reversal_amount} ;;
    value_format: "#,##0.00"
  }
  measure: spend_by_brand {
    type: sum
    group_label: "Business"
    label: "Spend by Brand"
    sql: CASE WHEN ${brand} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format: "#,##0.00"
  }
  measure: txn_count_by_brand {
    type: count
    group_label: "Business"
    label: "Txn Count by Brand"
    filters: [brand: "-null"]
    sql: ${mt_ref_nbr} ;;
  }
  measure: spend_by_txn_type {
    type: sum
    group_label: "Business"
    label: "Spend by Txn Type"
    sql: CASE WHEN ${transaction_type} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format: "#,##0.00"
  }
  measure: spend_domestic {
    type: sum
    group_label: "Business"
    label: "Domestic Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'DOM' THEN ${spend_amount} ELSE 0 END ;;
    value_format: "#,##0.00"
  }
  measure: spend_international {
    type: sum
    group_label: "Business"
    label: "International Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'FR' THEN ${spend_amount} ELSE 0 END ;;
    value_format: "#,##0.00"
  }
  measure: avg_spend_per_account {
    type: number
    group_label: "Business"
    label: "Avg Spend per Account"
    sql: CASE WHEN ${unique_accts_cnt} > 0 THEN ${total_gross_spend} / ${unique_accts_cnt} ELSE NULL END ;;
    value_format: "#,##0.00"
  }
  measure: total_transactions {
    type: count_distinct
    group_label: "Business"
    label: "Unique Transactions"
    sql: ${mt_ref_nbr} ;;
  }
}















view: spend_view2 {
  sql_table_name: `your_project.your_dataset.SPEND_VIEW_2` ;;

  # === DATE GROUPS ===
  dimension_group: mt_posting_date {
    type: time
    timeframes: [raw, date, week, month, quarter, year]
    label: "Posting Date"
    group_label: "Transaction Dates"
    sql: ${TABLE}.MT_POSTING_DATE ;;
  }

  # === BASE DIMENSIONS (ALL DATA FIELDS) ===
  dimension: acct                    { type: string;  label: "Account Number";           group_label: "Account Info";        sql: ${TABLE}.acct ;; }
  dimension: mcc_code                { type: string;  label: "MCC Code";                 group_label: "Merchant Info";       sql: CAST(${TABLE}.MCC_CODE AS STRING) ;; }
  dimension: description             { type: string;  label: "MCC Description";           group_label: "Merchant Info";       sql: ${TABLE}.Description ;; }
  dimension: merchant_details        { type: string;  label: "Merchant Details";          group_label: "Merchant Info";       sql: ${TABLE}.Merchant_Details ;; }
  dimension: org                     { type: number;  label: "Org ID";                   group_label: "Product Info";        sql: ${TABLE}.ORG ;; }
  dimension: transaction_type        { type: string;  label: "Transaction Type";          group_label: "Txn Details";         sql: ${TABLE}.TRANSACTION_TYPE ;; }
  dimension: spend_type              { type: string;  label: "Spend Type";                group_label: "Txn Details";         sql: ${TABLE}.SPEND_TYPE ;; }
  dimension: segment_name            { type: string;  label: "Segment Name";              group_label: "Merchant Info";       sql: ${TABLE}.Segment_Name ;; }
  dimension: mt_type                 { type: string;  label: "MT Type";                   group_label: "Product Info";        sql: ${TABLE}.mt_type ;; }
  dimension: spend_place             { type: string;  label: "Txn Geography";             group_label: "Txn Details";         sql: ${TABLE}.Spend_Place ;; }
  dimension: product                 { type: string;  label: "Product";                   group_label: "Product Info";        sql: ${TABLE}.Product ;; }
  dimension: spend_amount            { type: number;  label: "Spend Amount";              group_label: "Txn Amounts";         sql: ${TABLE}.Spend_Amount ;; }
  dimension: reversal_amount         { type: number;  label: "Reversal Amount";           group_label: "Txn Amounts";         sql: ${TABLE}.Reversal_Amount ;; }
  dimension: net_transaction         { type: number;  label: "Net Transaction";           group_label: "Txn Amounts";         sql: ${TABLE}.Net_Transaction ;; }
  dimension: spend_amount_cmb        { type: number;  label: "Spend Amount CMB";          group_label: "Txn Amounts";         sql: ${TABLE}.SPEND_AMOUNT_CMB ;; }
  dimension: spend_amount_rbmw       { type: number;  label: "Spend Amount RBMW";         group_label: "Txn Amounts";         sql: ${TABLE}.SPEND_AMOUNT_RBMW ;; }
  dimension: mt_ref_nbr              { type: string;  label: "Txn Reference No.";         group_label: "Txn Info";            sql: ${TABLE}.MT_REF_NBR ;; primary_key: yes }
  dimension: brand                   { type: string;  label: "Brand";                     group_label: "Merchant Info";       sql: ${TABLE}.BRAND ;; }
  dimension: final_category          { type: string;  label: "Final Category";            group_label: "Merchant Info";       sql: ${TABLE}.FINAL_CATEGORY ;; }
  dimension: category_general_desc   { type: string;  label: "Category General Desc";     group_label: "Merchant Info";       sql: ${TABLE}.CATEGORY_GENERAL_DESC ;; }
  dimension: category_desc           { type: string;  label: "Category Desc";             group_label: "Merchant Info";       sql: ${TABLE}.CATEGORY_DESC ;; }

  # === DERIVED DIMENSIONS ===
  dimension: merchant_name           { type: string;  label: "Merchant Name";             group_label: "Merchant Info";       sql: SUBSTRING(${TABLE}.Merchant_Details, 1, 24) ;; }
  dimension: merchant_city           { type: string;  label: "Merchant City";             group_label: "Merchant Info";       sql: REPLACE(SUBSTRING(${TABLE}.Merchant_Details, 24, 14), " ", "") ;; }
  dimension: merchant_country        { type: string;  label: "Merchant Country";          group_label: "Merchant Info";       sql: SUBSTRING(${TABLE}.Merchant_Details, 38, 2) ;; }
  dimension: mcc_combo               { type: string;  label: "MCC Combo";                 group_label: "Merchant Info";       sql: CONCAT(${mcc_code}, " - ", ${description}) ;; }

  # === DATE UTILITY DIMENSIONS ===
  dimension: today                   { type: date;    label: "Today's Date";              group_label: "Date Utilities";      sql: current_date() ;; }
  dimension: day                     { type: number;  label: "Current Day";               group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%e', current_date()) AS INT64) ;; }
  dimension: mon                     { type: number;  label: "Current Month";             group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%m', current_date()) AS INT64) ;; }
  dimension: yr                      { type: number;  label: "Current Year";              group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%Y', current_date()) AS INT64) ;; }
  dimension: mt_day                  { type: number;  label: "Txn Day";                   group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%e', ${mt_posting_date_date}) AS INT64) ;; }
  dimension: mt_mon                  { type: number;  label: "Txn Month";                 group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%m', ${mt_posting_date_date}) AS INT64) ;; }
  dimension: mt_yr                   { type: number;  label: "Txn Year";                  group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%Y', ${mt_posting_date_date}) AS INT64) ;; }

  # === PARAMETERIZED MEASURES (SWITCH LOGIC) ===
  parameter: metric_selector {
    label: "Metric Selector"
    group_label: "Switches"
    allowed_value: { label: "Net Spends in Cr" value: "net" }
    allowed_value: { label: "Gross Spends in Cr" value: "gross" }
  }
  measure: monthly_trend_metric {
    type: number
    label: "Monthly Trend Metric"
    group_label: "Switches"
    description: "Net or Gross Spends in Cr as per selector"
    sql:
      CASE
        WHEN {% parameter metric_selector %} = 'net' THEN ${total_net_transaction}
        WHEN {% parameter metric_selector %} = 'gross' THEN ${total_spend}
        ELSE NULL
      END
    ;;
    value_format: "#,##0.00"
  }
  parameter: wordcloud_metric_selector {
    label: "Word Cloud Metric"
    group_label: "Switches"
    allowed_value: { label: "Net Spends in Cr" value: "net" }
    allowed_value: { label: "Gross Spends in Cr" value: "gross" }
  }
  measure: wordcloud_metric {
    type: number
    label: "Word Cloud Metric"
    group_label: "Switches"
    sql:
      CASE
        WHEN {% parameter wordcloud_metric_selector %} = 'net' THEN ${total_net_transaction}
        WHEN {% parameter wordcloud_metric_selector %} = 'gross' THEN ${total_spend}
        ELSE NULL
      END
    ;;
    value_format: "#,##0.00"
  }

  # === MAIN MEASURES (ALL LOGIC) ===
  measure: total_spend                { type: sum;     group_label: "Core Spend"; label: "Gross Spends in Cr";        sql: ${spend_amount}/10000000 ;; value_format: "#,##0.00" }
  measure: total_reversal             { type: sum;     group_label: "Core Spend"; label: "Reversals in Cr";           sql: ${reversal_amount}/10000000 ;; value_format: "#,##0.00" }
  measure: total_net_transaction      { type: sum;     group_label: "Core Spend"; label: "Net Spends in Cr";          sql: ${net_transaction}/10000000 ;; value_format: "#,##0.00" }
  measure: average_spend              { type: average; group_label: "Core Spend"; label: "Avg Gross Spend/Txn";       sql: ${spend_amount} ;; value_format: "#,##0.00" }
  measure: average_net_transaction    { type: average; group_label: "Core Spend"; label: "Avg Net Spend/Txn";         sql: ${net_transaction} ;; value_format: "#,##0.00" }
  measure: transaction_count          { type: count;   group_label: "Core Spend"; label: "Txn Count" }
  measure: unique_accts_cnt           { type: count_distinct; group_label: "Core Spend"; label: "Unique Accounts";     sql: ${acct} ;; }

  # --- MTD MEASURES ---
  measure: total_net_transactions_mtd { type: sum;     group_label: "MTD"; label: "MTD Net Spends in Cr";             sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction}/10000000 ELSE NULL END ;; value_format: "#,##0.00" }
  measure: total_spends_mtd           { type: sum;     group_label: "MTD"; label: "MTD Gross Spends in Cr";           sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${spend_amount}/10000000 ELSE NULL END ;; value_format: "#,##0.00" }
  measure: total_reversal_mtd         { type: sum;     group_label: "MTD"; label: "MTD Reversals in Cr";              sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${reversal_amount}/10000000 ELSE NULL END ;; value_format: "#,##0.00" }
  measure: avg_net_transactions_mtd   { type: average; group_label: "MTD"; label: "MTD Avg Net Spend/Txn";            sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction} ELSE NULL END ;; value_format: "#,##0.00" }

  # --- BUSINESS/ADVANCED MEASURES ---
  measure: total_gross_spend          { type: sum;     group_label: "Business"; label: "Total Gross Spend";           sql: ${spend_amount} ;; value_format: "#,##0.00" }
  measure: total_net_spend            { type: sum;     group_label: "Business"; label: "Total Net Spend";             sql: ${net_transaction} ;; value_format: "#,##0.00" }
  measure: total_reversal_amount      { type: sum;     group_label: "Business"; label: "Total Reversal Amount";       sql: ${reversal_amount} ;; value_format: "#,##0.00" }
  measure: spend_by_brand             { type: sum;     group_label: "Business"; label: "Spend by Brand";              sql: CASE WHEN ${brand} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;; value_format: "#,##0.00" }
  measure: txn_count_by_brand         { type: count;   group_label: "Business"; label: "Txn Count by Brand";          filters: [brand: "-null"]; sql: ${mt_ref_nbr} ;; }
  measure: spend_by_txn_type          { type: sum;     group_label: "Business"; label: "Spend by Txn Type";           sql: CASE WHEN ${transaction_type} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;; value_format: "#,##0.00" }
  measure: spend_domestic             { type: sum;     group_label: "Business"; label: "Domestic Spend";              sql: CASE WHEN UPPER(${spend_place}) = 'DOM' THEN ${spend_amount} ELSE 0 END ;; value_format: "#,##0.00" }
  measure: spend_international        { type: sum;     group_label: "Business"; label: "International Spend";         sql: CASE WHEN UPPER(${spend_place}) = 'FR' THEN ${spend_amount} ELSE 0 END ;; value_format: "#,##0.00" }
  measure: avg_spend_per_account      { type: number;  group_label: "Business"; label: "Avg Spend per Account";       sql: CASE WHEN ${unique_accts_cnt} > 0 THEN ${total_gross_spend} / ${unique_accts_cnt} ELSE NULL END ;; value_format: "#,##0.00" }
  measure: total_transactions         { type: count_distinct; group_label: "Business"; label: "Unique Transactions";  sql: ${mt_ref_nbr} ;; }
}

















import pandas as pd
import re
from fuzzywuzzy import fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
import joblib
import os
import logging

# === CONFIGURE LOGGING ===
LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger(__name__)

# === MERCHANT DICTIONARY LOADER ===
def load_merchant_dictionary(path: str = "merchant_dictionary.csv"):
    """Load merchant dictionary from CSV (pipe-separated keywords)."""
    try:
        abs_path = os.path.abspath(path)
        df = pd.read_csv(abs_path, dtype=str, encoding="utf-8").fillna("")
        data = []
        for _, row in df.iterrows():
            entry = {
                "brand_keywords": [x.strip().lower() for x in row['brand_keywords'].split('|') if x.strip()],
                "merchant_name_keywords": [x.strip().lower() for x in row['merchant_name_keywords'].split('|') if x.strip()],
                "official_merchant_name": row.get('official_merchant_name', ''),
                "official_brand_name": row.get('official_brand_name', ''),
                "sector": row.get('sector', ''),
                "city": row.get('city', ''),
                "mcc": row.get('mcc', '')
            }
            if entry["brand_keywords"] or entry["merchant_name_keywords"]:
                data.append(entry)
        logger.info(f"Processed {len(data)} valid merchant dictionary entries.")
        return data
    except Exception as e:
        logger.error(f"Error loading merchant dictionary: {e}")
        return []

# === TEXT PREPROCESSOR & HEURISTICS ===
def preprocess(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'http\S+|www\.\S+', '', text)
    text = re.sub(r'\S*@\S*\s?', '', text)
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_entity_guess(raw_desc: str) -> str:
    if not raw_desc:
        return "Unknown Merchant"
    tokens = [w for w in raw_desc.split() if len(w) > 2 and any(c.isalpha() for c in w)]
    capitalized_phrases, curr = [], []
    for token in tokens:
        if token[0].isupper() or token.isupper():
            curr.append(token)
        elif curr:
            capitalized_phrases.append(' '.join(curr))
            curr = []
    if curr:
        capitalized_phrases.append(' '.join(curr))
    if capitalized_phrases:
        return max(capitalized_phrases, key=len)
    if len(tokens) >= 2:
        bigrams = [f'{tokens[i]} {tokens[i+1]}' for i in range(len(tokens)-1)]
        return max(bigrams, key=len) if bigrams else tokens[0]
    return tokens[0] if tokens else "Unknown Merchant"

# === DICTIONARY MATCHER ===
class DictionaryMatcher:
    def __init__(self, merchant_data, fuzzy_threshold=80):
        self.merchant_data = merchant_data
        self.fuzzy_threshold = fuzzy_threshold
        self.keyword_index = self._build_keyword_index()

    def _build_keyword_index(self):
        index = {}
        for entry in self.merchant_data:
            all_keywords = entry['brand_keywords'] + entry['merchant_name_keywords']
            for keyword in all_keywords:
                if keyword not in index:
                    index[keyword] = []
                index[keyword].append(entry)
        return index

    def find_match(self, processed_desc: str):
        if not processed_desc:
            return None
        best_match, best_score, match_type = None, 0, None
        for keyword, entries in self.keyword_index.items():
            if keyword in processed_desc:
                for entry in entries:
                    score = 99
                    if score > best_score:
                        best_score = score
                        best_match = entry
                        match_type = 'substring'
                        if best_score >= 98:
                            break
            if best_score >= 98:
                break
        if best_score < self.fuzzy_threshold:
            for entry in self.merchant_data:
                for keyword in entry['brand_keywords'] + entry['merchant_name_keywords']:
                    if not keyword:
                        continue
                    score_token = fuzz.token_set_ratio(processed_desc, keyword)
                    score_partial = fuzz.partial_ratio(processed_desc, keyword)
                    score = max(score_token, score_partial)
                    if score > best_score:
                        best_score = score
                        best_match = entry
                        match_type = 'token_set' if score == score_token else 'partial'
                    if best_score >= 98:
                        break
                if best_score >= 98:
                    break
        if best_match and best_score >= self.fuzzy_threshold:
            return {
                "brand": best_match["official_brand_name"],
                "merchant": best_match["official_merchant_name"],
                "sector": best_match["sector"],
                "confidence": "high" if best_score >= 90 else "medium"
            }
        return None

# === NLP SECTOR CLASSIFIER ===
class NLPSectorClassifier:
    def __init__(self, model_path="nlp_sector_model.joblib"):
        self.model_path = model_path
        self.pipeline = None

    def get_default_training_data(self):
        data = {
            'description': [
                "payment for groceries at local mart", "online order big general store", 
                "food delivery from quick bites", "monthly electricity bill payment",
                "cab ride with city movers", "ecom purchase fashion apparel",
                "swiggy bundl technologies", "amazon seller services online", 
                "zomato media pvt ltd", "tata cliq luxury shopping",
                "movie tickets pvr cinemas", "internet broadband connection act fibernet",
                "recharge mobile plan jio", "premium subscription netflix", 
                "dinner at urban restaurant", "flight booking indigo airlines",
                "pharmacy bill apollo pharmacy", "investment mutual fund groww",
                "utility gas bill payment adani gas", "transport metro card recharge",
                "retail clothing store westside", "education course fee udemy",
                "health checkup lal pathlabs", "donation to charity foundation",
                "upi transfer to friend", "atm withdrawal any bank",
                "interest credited savings account", "loan emi payment hdfc bank",
                "insurance premium lic india", "software purchase adobe creative",
                "payment to ABC solutions", "random tech services pvt ltd",
                "local kirana store purchase", "petrol pump fuel payment",
                "hospital medical treatment", "school fees payment",
                "gym membership renewal", "beauty salon services",
                "car repair garage", "book store purchase"
            ],
            'sector': [
                "Retail", "Retail", "Food Delivery", "Utilities", "Transport", "E-Commerce",
                "Food Delivery", "E-Commerce", "Food Delivery", "E-Commerce", "Entertainment", "Utilities",
                "Telecom", "Entertainment", "Dining", "Transport", "Healthcare", "Finance",
                "Utilities", "Transport", "Retail", "Education", "Healthcare", "Others",
                "Payments", "Banking", "Banking", "Finance", "Finance", "Software",
                "Services", "Technology", "Retail", "Fuel", "Healthcare", "Education",
                "Fitness", "Beauty", "Automotive", "Retail"
            ]
        }
        return pd.DataFrame(data)

    def train_model(self, training_data):
        training_data = training_data.copy()
        training_data['processed_desc'] = training_data['description'].apply(preprocess)
        training_data = training_data[training_data['processed_desc'].str.len() > 0]
        if len(training_data) == 0:
            raise ValueError("No valid training data after preprocessing")
        X = training_data['processed_desc']
        y = training_data['sector']
        self.pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(
                stop_words='english', ngram_range=(1, 2), max_df=0.95, min_df=2, max_features=5000)),
            ('clf', MultinomialNB(alpha=0.1))
        ])
        self.pipeline.fit(X, y)
        logger.info("NLP sector classifier trained (local)")

    def save_model(self):
        if self.pipeline and self.model_path:
            joblib.dump(self.pipeline, self.model_path)
            logger.info(f"NLP model saved to {self.model_path}")

    def load_model(self):
        try:
            if os.path.exists(self.model_path):
                self.pipeline = joblib.load(self.model_path)
                logger.info(f"NLP model loaded from {self.model_path}")
                return True
        except Exception as e:
            logger.error(f"Error loading NLP model: {e}")
        return False

    def predict_sector(self, processed_desc: str):
        if not self.pipeline or not processed_desc:
            return "Other"
        try:
            prediction = self.pipeline.predict([processed_desc])[0]
            return prediction
        except Exception as e:
            logger.error(f"NLP prediction error: {e}")
            return "Other"

# === MERCHANT EXTRACTOR (MAIN) ===
class MerchantExtractor:
    def __init__(self, merchant_data, model_path="nlp_sector_model.joblib",
                 retrain_nlp=False, new_nlp_data=None, fuzzy_threshold=80):
        self.merchant_data = merchant_data
        self.fuzzy_threshold = fuzzy_threshold
        self.dictionary_matcher = DictionaryMatcher(merchant_data, fuzzy_threshold)
        self.nlp_classifier = NLPSectorClassifier(model_path)
        self._initialize_nlp_model(retrain_nlp, new_nlp_data)

    def _initialize_nlp_model(self, retrain, training_data):
        if retrain and training_data is not None:
            logger.info("Retraining local NLP sector model...")
            self.nlp_classifier.train_model(training_data)
            self.nlp_classifier.save_model()
        elif self.nlp_classifier.load_model():
            logger.info("Loaded existing local NLP model.")
        else:
            logger.info("Training new NLP model with default sample data...")
            default_data = self.nlp_classifier.get_default_training_data()
            self.nlp_classifier.train_model(default_data)
            self.nlp_classifier.save_model()

    def extract(self, raw_description):
        if not isinstance(raw_description, str) or not raw_description.strip():
            return {
                "description": raw_description or "",
                "brand": "Invalid",
                "merchant": "Invalid",
                "sector": "Invalid",
                "confidence": "none"
            }
        processed_desc = preprocess(raw_description)
        dict_match = self.dictionary_matcher.find_match(processed_desc)
        if dict_match:
            return {
                "description": raw_description,
                "brand": dict_match["brand"],
                "merchant": dict_match["merchant"],
                "sector": dict_match["sector"],
                "confidence": dict_match["confidence"]
            }
        # NLP fallback
        predicted_sector = self.nlp_classifier.predict_sector(processed_desc)
        guessed_name = extract_entity_guess(raw_description)
        return {
            "description": raw_description,
            "brand": guessed_name,
            "merchant": guessed_name,
            "sector": predicted_sector,
            "confidence": "low"
        }

    def extract_batch(self, descriptions):
        return [self.extract(desc) for desc in descriptions]

# === RUN AS SCRIPT ===
if __name__ == '__main__':
    merchant_data = load_merchant_dictionary("merchant_dictionary.csv")
    if not merchant_data:
        logger.warning("No merchant data loaded, using minimal in-memory example")
        merchant_data = [{
            "brand_keywords": ["swiggy", "bundl"],
            "merchant_name_keywords": ["swiggy bundl technologies"],
            "official_merchant_name": "Bundl Technologies Pvt Ltd",
            "official_brand_name": "Swiggy",
            "sector": "Food Delivery",
            "city": "Bangalore",
            "mcc": "5812"
        }]
    extractor = MerchantExtractor(merchant_data, fuzzy_threshold=80)

    # Input descriptions
    descriptions = [
        "POS 987654321 SWIGGY BUNDL TECHNOLOGY BANGALORE IN",
        "ONLINE PAYMENT AMAZON PAY INDIA",
        "UPI 1234567890 KUMAR STATIONERY MART PUNE",
        "POS 1111 BIG BZR FUTURE RETAIL MUMBAI MH IN",
        "PAYMENT TO SRIRAM SWEETS AND BAKERY KOLKATA",
        "Random Cafe Shop Payment",
        "Strange merchant abcXyZ123"
    ]

    # Batch extraction and DataFrame/table output
    results = extractor.extract_batch(descriptions)
    df = pd.DataFrame(results, columns=["description", "brand", "merchant", "sector", "confidence"])
    print(df)

    # To save as CSV or Excel:
    # df.to_csv("merchant_extraction_output.csv", index=False)
    # df.to_excel("merchant_extraction_output.xlsx", index=False)







view: spend_view2 {
  sql_table_name: `your_project.your_dataset.SPEND_VIEW_2` ;;

  # ======================== DIMENSION GROUPS (DATES) =========================
  dimension_group: mt_posting_date {
    type: time
    timeframes: [raw, date, week, month, quarter, year]
    label: "Posting Date"
    group_label: "Transaction Dates"
    description: "Date the transaction was posted"
    sql: ${TABLE}.MT_POSTING_DATE ;;
  }

  # ======================== DIMENSIONS (SCHEMA FIELDS) =======================

  dimension: acct {
    type: string
    label: "Account Number"
    group_label: "Account Information"
    description: "Customer account number used for the transaction"
    sql: ${TABLE}.acct ;;
  }

  dimension: mcc_code {
    type: string
    label: "MCC Code"
    group_label: "Merchant Info"
    description: "Merchant Category Code (MCC)"
    sql: CAST(${TABLE}.MCC_CODE AS STRING) ;;
  }

  dimension: description {
    type: string
    label: "MCC Description"
    group_label: "Merchant Info"
    description: "Description of the MCC"
    sql: ${TABLE}.Description ;;
  }

  dimension: merchant_details {
    type: string
    label: "Merchant Details"
    group_label: "Merchant Info"
    description: "Raw merchant information from the transaction"
    sql: ${TABLE}.Merchant_Details ;;
  }

  dimension: org {
    type: number
    label: "Organization ID"
    group_label: "Card/Product Info"
    description: "Organization identifier"
    sql: ${TABLE}.ORG ;;
  }

  dimension: transaction_type {
    type: string
    label: "Transaction Type"
    group_label: "Transaction Details"
    description: "POS, Ecommerce, Cash, etc."
    sql: ${TABLE}.TRANSACTION_TYPE ;;
  }

  dimension: spend_type {
    type: string
    label: "Spend Type"
    group_label: "Transaction Details"
    description: "Type of spend (e.g., Debit, EMI, etc.)"
    sql: ${TABLE}.SPEND_TYPE ;;
  }

  dimension: segment_name {
    type: string
    label: "Segment Name"
    group_label: "Merchant Info"
    description: "Segment/category of the merchant"
    sql: ${TABLE}.Segment_Name ;;
  }

  dimension: mt_type {
    type: string
    label: "MT Type"
    group_label: "Card/Product Info"
    description: "Card type used (if available)"
    sql: ${TABLE}.mt_type ;;
  }

  dimension: spend_place {
    type: string
    label: "Transaction Geography"
    group_label: "Transaction Details"
    description: "DOM = Domestic, FR = International"
    sql: ${TABLE}.Spend_Place ;;
  }

  dimension: product {
    type: string
    label: "Product"
    group_label: "Card/Product Info"
    description: "Product or card type used"
    sql: ${TABLE}.Product ;;
  }

  dimension: spend_amount {
    type: number
    label: "Spend Amount"
    group_label: "Transaction Amounts"
    description: "Amount spent in the transaction"
    sql: ${TABLE}.Spend_Amount ;;
  }

  dimension: reversal_amount {
    type: number
    label: "Reversal Amount"
    group_label: "Transaction Amounts"
    description: "Amount reversed for the transaction"
    sql: ${TABLE}.Reversal_Amount ;;
  }

  dimension: net_transaction {
    type: number
    label: "Net Transaction"
    group_label: "Transaction Amounts"
    description: "Net spend after reversal"
    sql: ${TABLE}.Net_Transaction ;;
  }

  dimension: spend_amount_cmb {
    type: number
    label: "Spend Amount CMB"
    group_label: "Transaction Amounts"
    description: "Spend amount CMB"
    sql: ${TABLE}.SPEND_AMOUNT_CMB ;;
  }

  dimension: spend_amount_rbmw {
    type: number
    label: "Spend Amount RBMW"
    group_label: "Transaction Amounts"
    description: "Spend amount RBMW"
    sql: ${TABLE}.SPEND_AMOUNT_RBMW ;;
  }

  dimension: mt_ref_nbr {
    type: string
    label: "Transaction Reference Number"
    group_label: "Transaction Info"
    description: "Unique transaction reference number"
    sql: ${TABLE}.MT_REF_NBR ;;
    primary_key: yes
  }

  dimension: brand {
    type: string
    label: "Brand"
    group_label: "Merchant Info"
    description: "Brand associated with the transaction"
    sql: ${TABLE}.BRAND ;;
  }

  dimension: final_category {
    type: string
    label: "Final Category"
    group_label: "Merchant Info"
    description: "Final category of the transaction"
    sql: ${TABLE}.FINAL_CATEGORY ;;
  }

  dimension: category_general_desc {
    type: string
    label: "Category General Description"
    group_label: "Merchant Info"
    description: "General description of the merchant category"
    sql: ${TABLE}.CATEGORY_GENERAL_DESC ;;
  }

  dimension: category_desc {
    type: string
    label: "Category Description"
    group_label: "Merchant Info"
    description: "Detailed description of the merchant category"
    sql: ${TABLE}.CATEGORY_DESC ;;
  }

  # ======================== DERIVED DIMENSIONS ===============================

  dimension: merchant_name {
    type: string
    label: "Merchant Name"
    group_label: "Merchant Info"
    description: "Parsed merchant name from merchant_details"
    sql: SUBSTRING(${TABLE}.Merchant_Details, 1, 24) ;;
  }

  dimension: merchant_city {
    type: string
    label: "Merchant City"
    group_label: "Merchant Info"
    description: "Parsed merchant city from merchant_details"
    sql: REPLACE(SUBSTRING(${TABLE}.Merchant_Details, 24, 14), " ", "") ;;
  }

  dimension: merchant_country {
    type: string
    label: "Merchant Country"
    group_label: "Merchant Info"
    description: "Parsed merchant country from merchant_details"
    sql: SUBSTRING(${TABLE}.Merchant_Details, 38, 2) ;;
  }

  dimension: mcc_combo {
    type: string
    label: "MCC Combo"
    group_label: "Merchant Info"
    description: "MCC code and description combo"
    sql: CONCAT(${mcc_code}, " - ", ${description}) ;;
  }

  # ======================== DATE UTILITIES ===================================

  dimension: today {
    type: date
    label: "Today's Date"
    group_label: "Date Utilities"
    description: "Current system date"
    sql: current_date() ;;
  }

  dimension: day {
    type: number
    label: "Current Day"
    group_label: "Date Utilities"
    description: "Today's day of the month"
    sql: CAST(FORMAT_DATE('%e', current_date()) AS INT64) ;;
  }

  dimension: mon {
    type: number
    label: "Current Month"
    group_label: "Date Utilities"
    description: "Current month"
    sql: CAST(FORMAT_DATE('%m', current_date()) AS INT64) ;;
  }

  dimension: yr {
    type: number
    label: "Current Year"
    group_label: "Date Utilities"
    description: "Current year"
    sql: CAST(FORMAT_DATE('%Y', current_date()) AS INT64) ;;
  }

  dimension: mt_day {
    type: number
    label: "Transaction Day"
    group_label: "Date Utilities"
    description: "Transaction's day of month"
    sql: CAST(FORMAT_DATE('%e', ${mt_posting_date}) AS INT64) ;;
  }

  dimension: mt_mon {
    type: number
    label: "Transaction Month"
    group_label: "Date Utilities"
    description: "Transaction's month"
    sql: CAST(FORMAT_DATE('%m', ${mt_posting_date}) AS INT64) ;;
  }

  dimension: mt_yr {
    type: number
    label: "Transaction Year"
    group_label: "Date Utilities"
    description: "Transaction's year"
    sql: CAST(FORMAT_DATE('%Y', ${mt_posting_date}) AS INT64) ;;
  }

  # ======================== ORIGINAL MEASURES ================================

  measure: total_spend {
    type: sum
    group_label: "Core Spend Metrics"
    label: "Gross Spends in Cr"
    description: "Gross spends in crore"
    sql: ${spend_amount}/10000000 ;;
    value_format: "#,##0.00"
  }

  measure: total_reversal {
    type: sum
    group_label: "Core Spend Metrics"
    sql: ${reversal_amount}/10000000;;
    label: "Reversals in Cr"
    description: "Reversals in crore"
    value_format: "#,##0.00"
  }

  measure: total_net_transaction {
    type: sum
    group_label: "Core Spend Metrics"
    label: "Net Spends in Cr"
    description: "Net spends in crore"
    sql: ${net_transaction}/10000000 ;;
    value_format: "#,##0.00"
  }

  measure: average_spend {
    type: average
    group_label: "Core Spend Metrics"
    sql: ${spend_amount};;
    label: "Average Gross Spend per Transaction"
    value_format: "#,##0.00"
  }

  measure: average_net_transaction {
    type: average
    group_label: "Core Spend Metrics"
    label: "Average Net Spend per Transaction"
    sql: ${net_transaction};;
    value_format: "#,##0.00"
  }

  measure: transaction_count {
    label: "Count of Transactions"
    group_label: "Core Spend Metrics"
    type: count
  }

  measure: unique_accts_cnt {
    type: count_distinct
    group_label: "Core Spend Metrics"
    label: "Unique Account Count"
    sql: ${acct} ;;
  }

  # ======================== MTD MEASURES =====================================

  measure: total_net_transactions_mtd {
    type: sum
    group_label: "MTD Metrics"
    label: "MTD Net Spends in Cr"
    description: "Month to date net spends in crore"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }

  measure: total_spends_mtd {
    type: sum
    group_label: "MTD Metrics"
    label: "MTD Gross Spends in Cr"
    description: "Month to date gross spends in crore"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${spend_amount}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }

  measure: total_reversal_mtd {
    type: sum
    group_label: "MTD Metrics"
    label: "MTD Reversals in Cr"
    description: "Month to date reversals in crore"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${reversal_amount}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }

  measure: avg_net_transactions_mtd {
    type: average
    group_label: "MTD Metrics"
    label: "MTD Avg Net Spend per Transaction"
    description: "Month to date average net spend per transaction"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction} ELSE NULL END ;;
    value_format: "#,##0.00"
  }

  # =================== BUSINESS BREAKDOWN/NEW MEASURES =======================

  measure: total_gross_spend {
    type: sum
    group_label: "Business Breakdown"
    label: "Total Gross Spend"
    sql: ${spend_amount} ;;
    value_format_name: "decimal_2"
    description: "Sum of spend amount"
  }

  measure: total_net_spend {
    type: sum
    group_label: "Business Breakdown"
    label: "Total Net Spend"
    sql: ${net_transaction} ;;
    value_format_name: "decimal_2"
    description: "Sum of net transaction"
  }

  measure: total_reversal_amount {
    type: sum
    group_label: "Business Breakdown"
    label: "Total Reversal Amount"
    sql: ${reversal_amount} ;;
    value_format_name: "decimal_2"
  }

  measure: spend_by_brand {
    type: sum
    group_label: "Business Breakdown"
    label: "Spend by Brand"
    sql: CASE WHEN ${brand} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Total spend for each brand"
  }

  measure: txn_count_by_brand {
    type: count
    group_label: "Business Breakdown"
    label: "Txn Count by Brand"
    filters: [brand: "-null"]
    sql: ${mt_ref_nbr} ;;
  }

  measure: spend_by_txn_type {
    type: sum
    group_label: "Business Breakdown"
    label: "Spend by Transaction Type"
    sql: CASE WHEN ${transaction_type} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Total spend by transaction type"
  }

  measure: spend_domestic {
    type: sum
    group_label: "Business Breakdown"
    label: "Domestic Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'DOM' THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Domestic spends only"
  }

  measure: spend_international {
    type: sum
    group_label: "Business Breakdown"
    label: "International Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'FR' THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "International spends only"
  }

  measure: avg_spend_per_account {
    type: number
    group_label: "Business Breakdown"
    label: "Avg Spend per Account"
    sql: CASE WHEN ${unique_accts_cnt} > 0 THEN ${total_gross_spend} / ${unique_accts_cnt} ELSE NULL END ;;
    value_format_name: "decimal_2"
  }

  measure: total_transactions {
    type: count_distinct
    group_label: "Business Breakdown"
    label: "Unique Transactions"
    sql: ${mt_ref_nbr} ;;
    description: "Number of unique transactions"
  }

  # (Optional - Only if you have a Customer_ID field)
  # measure: unique_customers_cnt {
  #   type: count_distinct
  #   group_label: "Business Breakdown"
  #   label: "Unique Customer Count"
  #   sql: ${TABLE}.Customer_ID ;;
  #   description: "Number of unique customers"
  # }
}












view: spend_view_new {
  sql_table_name: `your_project.your_dataset.SPEND_VIEW_*` ;;

  # ------------------- DIMENSIONS -------------------

  dimension: mt_posting_date {
    type: date
    label: "Posting Date"
    sql: ${TABLE}.MT_POSTING_DATE ;;
  }

  dimension: mcc_code {
    type: string
    label: "MCC Code"
    sql: CAST(${TABLE}.MCC_CODE AS STRING) ;;
  }

  dimension: merchant_details {
    type: string
    label: "Merchant Details"
    sql: ${TABLE}.Merchant_Details ;;
  }

  dimension: description {
    type: string
    label: "Description"
    sql: ${TABLE}.Description ;;
  }

  dimension: org {
    type: number
    label: "ORG"
    sql: ${TABLE}.ORG ;;
  }

  dimension: transaction_type {
    type: string
    label: "Transaction Type"
    sql: ${TABLE}.TRANSACTION_TYPE ;;
    description: "Type of transaction: POS, Ecommerce, Cash, etc."
  }

  dimension: spend_type {
    type: string
    label: "Spend Type"
    sql: ${TABLE}.SPEND_TYPE ;;
    description: "Debit/Credit, EMI, etc."
  }

  dimension: segment_name {
    type: string
    label: "Segment Name"
    sql: ${TABLE}.Segment_Name ;;
  }

  dimension: acct {
    type: string
    label: "Account Number"
    sql: ${TABLE}.acct ;;
  }

  dimension: mt_type {
    type: string
    label: "MT Type"
    sql: ${TABLE}.mt_type ;;
    description: "Card type (if available)"
  }

  dimension: spend_place {
    type: string
    label: "Spend Place"
    sql: ${TABLE}.Spend_Place ;;
    description: "DOM = Domestic, FR = International"
  }

  dimension: product {
    type: string
    label: "Product"
    sql: ${TABLE}.Product ;;
    description: "Card/Account product"
  }

  dimension: spend_amount {
    type: number
    label: "Spend Amount"
    sql: ${TABLE}.Spend_Amount ;;
  }

  dimension: reversal_amount {
    type: number
    label: "Reversal Amount"
    sql: ${TABLE}.Reversal_Amount ;;
  }

  dimension: net_transaction {
    type: number
    label: "Net Transaction"
    sql: ${TABLE}.Net_Transaction ;;
  }

  dimension: spend_amount_cmb {
    type: number
    label: "Spend Amount CMB"
    sql: ${TABLE}.SPEND_AMOUNT_CMB ;;
  }

  dimension: spend_amount_rbmw {
    type: number
    label: "Spend Amount RBMW"
    sql: ${TABLE}.SPEND_AMOUNT_RBMW ;;
  }

  dimension: mt_ref_nbr {
    type: string
    label: "MT Reference Number"
    sql: ${TABLE}.MT_REF_NBR ;;
    description: "Unique transaction reference"
    primary_key: yes
  }

  dimension: brand {
    type: string
    label: "Brand"
    sql: ${TABLE}.BRAND ;;
  }

  dimension: final_category {
    type: string
    label: "Final Category"
    sql: ${TABLE}.FINAL_CATEGORY ;;
  }

  dimension: category_general_desc {
    type: string
    label: "Category General Description"
    sql: ${TABLE}.CATEGORY_GENERAL_DESC ;;
  }

  dimension: category_desc {
    type: string
    label: "Category Description"
    sql: ${TABLE}.CATEGORY_DESC ;;
  }

  # ------------------- MEASURES -------------------

  # Total spend amount (Gross)
  measure: total_gross_spend {
    type: sum
    label: "Total Gross Spend"
    sql: ${spend_amount} ;;
    value_format_name: "decimal_2"
    description: "Sum of spend amount"
  }

  # Total net spend (after reversal)
  measure: total_net_spend {
    type: sum
    label: "Total Net Spend"
    sql: ${net_transaction} ;;
    value_format_name: "decimal_2"
    description: "Sum of net transaction"
  }

  measure: total_reversal_amount {
    type: sum
    label: "Total Reversal Amount"
    sql: ${reversal_amount} ;;
    value_format_name: "decimal_2"
  }

  measure: unique_accts_cnt {
    type: count_distinct
    label: "Unique Accounts Count"
    sql: ${acct} ;;
    description: "Number of unique accounts"
  }

  measure: unique_customers_cnt {
    type: count_distinct
    label: "Unique Customer Count"
    sql: ${TABLE}.Customer_ID ;; # Replace with correct field if you have Customer_ID, otherwise remove
    description: "Number of unique customers"
  }

  # Spend by brand (filtered measure: example for dashboarding)
  measure: spend_by_brand {
    type: sum
    label: "Spend by Brand"
    sql: CASE WHEN ${brand} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Total spend for each brand"
  }

  # Count of transactions by brand
  measure: txn_count_by_brand {
    type: count
    label: "Txn Count by Brand"
    filters: [brand: "-null"]
    sql: ${mt_ref_nbr} ;;
  }

  # Spend by transaction type (POS, ECOM, CASH, etc.)
  measure: spend_by_txn_type {
    type: sum
    label: "Spend by Transaction Type"
    sql: CASE WHEN ${transaction_type} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Total spend by transaction type"
  }

  # Spend by geography (Domestic/International)
  measure: spend_domestic {
    type: sum
    label: "Domestic Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'DOM' THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Domestic spends only"
  }

  measure: spend_international {
    type: sum
    label: "International Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'FR' THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "International spends only"
  }

  # Average spend per account
  measure: avg_spend_per_account {
    type: number
    label: "Average Spend per Account"
    sql: CASE WHEN ${unique_accts_cnt} > 0 THEN ${total_gross_spend} / ${unique_accts_cnt} ELSE NULL END ;;
    value_format_name: "decimal_2"
  }

  # Transaction count
  measure: total_transactions {
    type: count_distinct
    label: "Unique Transactions"
    sql: ${mt_ref_nbr} ;;
    description: "Number of unique transactions"
  }
}





















view: spend_view2 {
  sql_table_name: `your_project.your_dataset.SPEND_VIEW_2` ;;

  # --- Dimensions ---

  dimension: mt_posting_date {
    type: date
    label: "Posting Date"
    sql: ${TABLE}.MT_POSTING_DATE ;;
  }

  dimension: mcc_code {
    type: number
    label: "MCC Code"
    sql: ${TABLE}.MCC_CODE ;;
  }

  dimension: merchant_details {
    type: string
    label: "Merchant Details"
    sql: ${TABLE}.Merchant_Details ;;
  }

  dimension: description {
    type: string
    label: "Description"
    sql: ${TABLE}.Description ;;
  }

  dimension: org {
    type: number
    label: "ORG"
    sql: ${TABLE}.ORG ;;
  }

  dimension: transaction_type {
    type: string
    label: "Transaction Type"
    sql: ${TABLE}.TRANSACTION_TYPE ;;
  }

  dimension: spend_type {
    type: string
    label: "Spend Type"
    sql: ${TABLE}.SPEND_TYPE ;;
  }

  dimension: segment_name {
    type: string
    label: "Segment Name"
    sql: ${TABLE}.Segment_Name ;;
  }

  dimension: acct {
    type: string
    label: "Account Number"
    sql: ${TABLE}.acct ;;
  }

  dimension: mt_type {
    type: string
    label: "MT Type"
    sql: ${TABLE}.mt_type ;;
  }

  dimension: spend_place {
    type: string
    label: "Spend Place"
    sql: ${TABLE}.Spend_Place ;;
  }

  dimension: product {
    type: string
    label: "Product"
    sql: ${TABLE}.Product ;;
  }

  dimension: mt_ref_nbr {
    type: string
    label: "MT Reference Number"
    sql: ${TABLE}.MT_REF_NBR ;;
    primary_key: yes
  }

  dimension: brand {
    type: string
    label: "Brand"
    sql: ${TABLE}.BRAND ;;
  }

  dimension: final_category {
    type: string
    label: "Final Category"
    sql: ${TABLE}.FINAL_CATEGORY ;;
  }

  dimension: category_general_desc {
    type: string
    label: "Category General Description"
    sql: ${TABLE}.CATEGORY_GENERAL_DESC ;;
  }

  dimension: category_desc {
    type: string
    label: "Category Description"
    sql: ${TABLE}.CATEGORY_DESC ;;
  }

  # --- Measures: Spend Analytics ---

  measure: total_gross_spend {
    type: sum
    sql: ${TABLE}.Spend_Amount ;;
    label: "Total Gross Spend"
    value_format_name: "decimal_2"
    description: "Sum of Spend_Amount (Gross Spend)"
  }

  measure: total_reversal_amount {
    type: sum
    sql: ${TABLE}.Reversal_Amount ;;
    label: "Total Reversal Amount"
    value_format_name: "decimal_2"
    description: "Sum of Reversal_Amount"
  }

  measure: net_spend {
    type: number
    sql: ${total_gross_spend} - ${total_reversal_amount} ;;
    label: "Net Spend"
    value_format_name: "decimal_2"
    description: "Gross Spend minus Reversal Amount"
  }

  measure: avg_gross_spend {
    type: average
    sql: ${TABLE}.Spend_Amount ;;
    label: "Average Gross Spend"
    value_format_name: "decimal_2"
    description: "Average gross spend per transaction"
  }

  measure: avg_net_spend {
    type: number
    sql: CASE WHEN COUNT(${mt_ref_nbr}) > 0 THEN (${net_spend}) / COUNT(${mt_ref_nbr}) ELSE NULL END ;;
    label: "Average Net Spend"
    value_format_name: "decimal_2"
    description: "Net Spend divided by count of transactions"
  }

  measure: count_transactions {
    type: count_distinct
    sql: ${TABLE}.MT_REF_NBR ;;
    label: "Transaction Count"
    description: "Distinct number of transactions"
  }

  measure: total_net_transaction {
    type: sum
    sql: ${TABLE}.Net_Transaction ;;
    label: "Net Transaction (DB column)"
    value_format_name: "decimal_2"
    description: "Sum of Net_Transaction column"
  }

  measure: total_spend_amount_cmb {
    type: sum
    sql: ${TABLE}.SPEND_AMOUNT_CMB ;;
    label: "Spend Amount CMB"
    value_format_name: "decimal_2"
    description: "Sum of SPEND_AMOUNT_CMB"
  }

  measure: total_spend_amount_rbmw {
    type: sum
    sql: ${TABLE}.SPEND_AMOUNT_RBMW ;;
    label: "Spend Amount RBMW"
    value_format_name: "decimal_2"
    description: "Sum of SPEND_AMOUNT_RBMW"
  }
}
