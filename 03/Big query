# Merge with spend summary
sector_spend = df.groupby('sector')['amount'].sum().reset_index()

# Filter out zero-spend sectors
valid_sectors = sector_spend[sector_spend['amount'] > 0]['sector']
df = df[df['sector'].isin(valid_sectors)]





import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import os

# --------------------------
# Step 1: Load Data
# --------------------------
# Replace 'your_file.csv' with your actual filename
df = pd.read_csv("your_file.csv", parse_dates=['date'])

# --------------------------
# Step 2: Filter Last 3 Months & Active Customers
# --------------------------
latest_date = df['date'].max()
three_months_ago = latest_date - pd.DateOffset(months=3)
df = df[df['date'] >= three_months_ago]

acct_txn_counts = df.groupby('ACCT')['date'].nunique()
valid_accts = acct_txn_counts[acct_txn_counts >= 5].index
df = df[df['ACCT'].isin(valid_accts)]

# --------------------------
# Step 3: Create Basket per ACCT
# --------------------------
baskets = df.groupby('ACCT')['sector'].apply(list).tolist()

# --------------------------
# Step 4: Transaction Encoding
# --------------------------
te = TransactionEncoder()
te_array = te.fit(baskets).transform(baskets)
df_encoded = pd.DataFrame(te_array, columns=te.columns_)

# --------------------------
# Step 5: Frequent Itemsets & Rules
# --------------------------
frequent_items = apriori(df_encoded, min_support=0.02, use_colnames=True)
rules = association_rules(frequent_items, metric='lift', min_threshold=1.0)

rules['antecedents'] = rules['antecedents'].apply(lambda x: sorted(list(x)))
rules['consequents'] = rules['consequents'].apply(lambda x: sorted(list(x)))
rules['antecedents_str'] = rules['antecedents'].apply(lambda x: ', '.join(x))
rules['consequents_str'] = rules['consequents'].apply(lambda x: ', '.join(x))
rules['score'] = rules['lift'] * rules['confidence']

# --------------------------
# Step 6: Keep Only 1-Sector Consequents
# --------------------------
rules = rules[rules['consequents'].apply(lambda x: len(x) == 1)]

# --------------------------
# Step 7: Remove A‚ÜîB Duplicate Pairs (Keep Strongest Only)
# --------------------------
rules['rule_key'] = rules.apply(
    lambda row: tuple(sorted([row['antecedents_str'], row['consequents_str']])),
    axis=1
)
rules = rules.sort_values(by='lift', ascending=False)
rules = rules.drop_duplicates(subset='rule_key')

# --------------------------
# Step 8: Build Tiered Rules
# --------------------------
top20_classic = rules.nlargest(20, 'lift').reset_index(drop=True)
rule1 = rules[rules['antecedents'].apply(len) == 1].nlargest(20, 'lift').reset_index(drop=True)
rule2 = rules[rules['antecedents'].apply(len) == 2].nlargest(20, 'lift').reset_index(drop=True)
rule3 = rules[rules['antecedents'].apply(len) == 3].nlargest(20, 'lift').reset_index(drop=True)

combined_rules = pd.concat([rule1, rule2, rule3], ignore_index=True)
top20_combined = combined_rules.nlargest(20, 'lift').reset_index(drop=True)

# --------------------------
# Step 9: Save Outputs
# --------------------------
output_dir = "market_basket_outputs"
os.makedirs(output_dir, exist_ok=True)

rule1.to_csv(f"{output_dir}/top20_rule1_X_to_Y.csv", index=False)
rule2.to_csv(f"{output_dir}/top20_rule2_XY_to_Z.csv", index=False)
rule3.to_csv(f"{output_dir}/top20_rule3_XYZ_to_A.csv", index=False)
top20_classic.to_csv(f"{output_dir}/top20_classic_rules.csv", index=False)
top20_combined.to_csv(f"{output_dir}/top20_combined_rules.csv", index=False)

print("‚úÖ Final outputs saved in folder: market_basket_outputs")












rules = association_rules(frequent_items, metric='lift', min_threshold=1.0)

# ‚úÖ Keep only 1-item consequents
rules = rules[rules['consequents'].apply(lambda x: len(x) == 1)]








# --------------------------
# Step 7: Export to Output Folder
# --------------------------
output_dir = "market_basket_outputs"
os.makedirs(output_dir, exist_ok=True)

rule1.to_csv(f"{output_dir}/top20_rule1_X_to_Y.csv", index=False)
rule2.to_csv(f"{output_dir}/top20_rule2_XY_to_Z.csv", index=False)
rule3.to_csv(f"{output_dir}/top20_rule3_XYZ_to_A.csv", index=False)
top20_classic.to_csv(f"{output_dir}/top20_classic_rules.csv", index=False)
top20_combined.to_csv(f"{output_dir}/top20_combined_rules.csv", index=False)

print("‚úÖ All 5 rule sets exported to folder: market_basket_outputs")





import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import os

# --------------------------
# Step 1: Load Your Data
# --------------------------
# Your CSV must contain: Customer_ID, Transaction_Date, Sector_Level
df = pd.read_csv("your_file.csv", parse_dates=['Transaction_Date'])

# --------------------------
# Step 2: Apply Filters
# --------------------------
# Filter: Last 3 months only
latest_date = df['Transaction_Date'].max()
three_months_ago = latest_date - pd.DateOffset(months=3)
df = df[df['Transaction_Date'] >= three_months_ago]

# Filter: Customers with ‚â• 5 transactions
cust_counts = df.groupby('Customer_ID')['Transaction_Date'].nunique()
valid_customers = cust_counts[cust_counts >= 5].index
df = df[df['Customer_ID'].isin(valid_customers)]

# --------------------------
# Step 3: Create Sector Basket per Customer
# --------------------------
baskets = df.groupby('Customer_ID')['Sector_Level'].apply(list).tolist()

# --------------------------
# Step 4: Transaction Encoding
# --------------------------
te = TransactionEncoder()
te_data = te.fit(baskets).transform(baskets)
df_encoded = pd.DataFrame(te_data, columns=te.columns_)

# --------------------------
# Step 5: Frequent Itemsets & Rules
# --------------------------
frequent_items = apriori(df_encoded, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_items, metric='lift', min_threshold=1.0)

# Clean up and prep string columns
rules['antecedents'] = rules['antecedents'].apply(lambda x: sorted(list(x)))
rules['consequents'] = rules['consequents'].apply(lambda x: sorted(list(x)))
rules['antecedents_str'] = rules['antecedents'].apply(lambda x: ', '.join(x))
rules['consequents_str'] = rules['consequents'].apply(lambda x: ', '.join(x))
rules['score'] = rules['lift'] * rules['confidence']

# --------------------------
# Step 6: Rule Segmentations
# --------------------------

# 1. Classic MBA: Top 20 by lift
top20_classic = rules.nlargest(20, 'lift').reset_index(drop=True)

# 2. Rule 1 (X ‚ûù Y): Antecedents with 1 item
rule1 = rules[rules['antecedents'].apply(len) == 1].nlargest(20, 'lift').reset_index(drop=True)

# 3. Rule 2 (X & Y ‚ûù Z): Antecedents with 2 items
rule2 = rules[rules['antecedents'].apply(len) == 2].nlargest(20, 'lift').reset_index(drop=True)

# 4. Rule 3 (X, Y, Z ‚ûù A): Antecedents with 3 items
rule3 = rules[rules['antecedents'].apply(len) == 3].nlargest(20, 'lift').reset_index(drop=True)

# 5. Combined Best 20 from Rule1, Rule2, Rule3
combined_rules = pd.concat([rule1, rule2, rule3], ignore_index=True)
top20_combined = combined_rules.nlargest(20, 'lift').reset_index(drop=True)

# --------------------------
# Step 7: Export to CSV
# --------------------------
rule1.to_csv("top20_rule1_X_to_Y.csv", index=False)
rule2.to_csv("top20_rule2_XY_to_Z.csv", index=False)
rule3.to_csv("top20_rule3_XYZ_to_A.csv", index=False)
top20_classic.to_csv("top20_classic_rules.csv", index=False)
top20_combined.to_csv("top20_combined_rules.csv", index=False)

print("‚úÖ All 5 rule sets exported successfully:")
print("- top20_rule1_X_to_Y.csv")
print("- top20_rule2_XY_to_Z.csv")
print("- top20_rule3_XYZ_to_A.csv")
print("- top20_classic_rules.csv")
print("- top20_combined_rules.csv")












import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
from pyvis.network import Network
import os

# ---------------------------
# Step 1: Load Your Data
# ---------------------------
# The dataset must contain: Customer_ID, Transaction_Date, Sector_Level
df = pd.read_csv("your_file.csv", parse_dates=['Transaction_Date'])

# ---------------------------
# Step 2: Filter Data
# ---------------------------

# Keep only last 3 months of data
latest_date = df['Transaction_Date'].max()
cutoff_date = latest_date - pd.DateOffset(months=3)
df = df[df['Transaction_Date'] >= cutoff_date]

# Keep only customers with ‚â• 5 transactions
cust_txn_counts = df.groupby('Customer_ID')['Transaction_Date'].nunique()
valid_customers = cust_txn_counts[cust_txn_counts >= 5].index
df = df[df['Customer_ID'].isin(valid_customers)]

# ---------------------------
# Step 3: Create Basket per Customer
# ---------------------------
basket_data = df.groupby('Customer_ID')['Sector_Level'].apply(list).tolist()

# ---------------------------
# Step 4: One-Hot Encode Transactions
# ---------------------------
te = TransactionEncoder()
te_data = te.fit(basket_data).transform(basket_data)
df_encoded = pd.DataFrame(te_data, columns=te.columns_)

# ---------------------------
# Step 5: Generate Frequent Itemsets and Rules
# ---------------------------
frequent_items = apriori(df_encoded, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_items, metric='lift', min_threshold=1.0)

# Prepare rule strings for filtering and visualization
rules['antecedents'] = rules['antecedents'].apply(lambda x: sorted(list(x)))
rules['consequents'] = rules['consequents'].apply(lambda x: sorted(list(x)))
rules['antecedents_str'] = rules['antecedents'].apply(lambda x: ', '.join(x))
rules['consequents_str'] = rules['consequents'].apply(lambda x: ', '.join(x))
rules['score'] = rules['lift'] * rules['confidence']

# ---------------------------
# Step 6: Extract 3 Rule Levels
# ---------------------------

# Rule 1: If X ‚ûù Y
rule1 = rules[rules['antecedents'].apply(len) == 1].nlargest(20, 'lift').reset_index(drop=True)

# Rule 2: If X & Y ‚ûù Z
rule2 = rules[rules['antecedents'].apply(len) == 2].nlargest(20, 'lift').reset_index(drop=True)

# Rule 3: If X, Y & Z ‚ûù A
rule3 = rules[rules['antecedents'].apply(len) == 3].nlargest(20, 'lift').reset_index(drop=True)

# ---------------------------
# Step 7: Export Rules to CSV
# ---------------------------
rule1.to_csv("top20_rule1_X_to_Y.csv", index=False)
rule2.to_csv("top20_rule2_XY_to_Z.csv", index=False)
rule3.to_csv("top20_rule3_XYZ_to_A.csv", index=False)

# ---------------------------
# Step 8: PyVis Network Visualization
# ---------------------------
def create_pyvis_graph(rule_df, output_file):
    g = Network(height="600px", width="100%", notebook=False, directed=True)
    for _, row in rule_df.iterrows():
        ant = row['antecedents_str']
        con = row['consequents_str']
        g.add_node(ant, label=ant)
        g.add_node(con, label=con)
        g.add_edge(ant, con, title=f"Confidence: {row['confidence']:.2f} | Lift: {row['lift']:.2f}")
    g.show(output_file)

# Create and save interactive network graphs
create_pyvis_graph(rule1, "rule1_X_to_Y.html")
create_pyvis_graph(rule2, "rule2_XY_to_Z.html")
create_pyvis_graph(rule3, "rule3_XYZ_to_A.html")

print("‚úÖ Market Basket Analysis completed successfully.")















import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
from pyvis.network import Network
import os

# -----------------------
# Step 1: Load Data
# -----------------------
# Sample input: Transaction_ID, Item
df = pd.read_csv("transactions.csv")  # Replace with your actual file

# Convert transactions into list of item lists
basket_list = df.groupby("Transaction_ID")['Item'].apply(list).tolist()

# -----------------------
# Step 2: Encode Transaction Data
# -----------------------
te = TransactionEncoder()
te_array = te.fit(basket_list).transform(basket_list)
df_encoded = pd.DataFrame(te_array, columns=te.columns_)

# -----------------------
# Step 3: Frequent Itemsets & Rules
# -----------------------
frequent_items = apriori(df_encoded, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_items, metric="lift", min_threshold=1.0)

# Clean and prepare rule columns
rules['antecedents'] = rules['antecedents'].apply(lambda x: sorted(list(x)))
rules['consequents'] = rules['consequents'].apply(lambda x: sorted(list(x)))
rules['antecedents_str'] = rules['antecedents'].apply(lambda x: ', '.join(x))
rules['consequents_str'] = rules['consequents'].apply(lambda x: ', '.join(x))
rules['score'] = rules['lift'] * rules['confidence']

# -----------------------
# Step 4: Extract Top 20 Rules per Level
# -----------------------
# Rule 1: If X ‚ûî Y
rule1 = rules[rules['antecedents'].apply(len) == 1].nlargest(20, 'lift').reset_index(drop=True)

# Rule 2: If X & Y ‚ûî Z
rule2 = rules[rules['antecedents'].apply(len) == 2].nlargest(20, 'lift').reset_index(drop=True)

# Rule 3: If X & Y & Z ‚ûî A
rule3 = rules[rules['antecedents'].apply(len) == 3].nlargest(20, 'lift').reset_index(drop=True)

# -----------------------
# Step 5: Export Top Rules
# -----------------------
rule1.to_csv("top20_rule1_X_to_Y.csv", index=False)
rule2.to_csv("top20_rule2_XY_to_Z.csv", index=False)
rule3.to_csv("top20_rule3_XYZ_to_A.csv", index=False)

# -----------------------
# Step 6: Generate PyVis Graphs
# -----------------------
def create_pyvis_graph(rule_df, output_html):
    g = Network(height="600px", width="100%", notebook=False, directed=True)
    for _, row in rule_df.iterrows():
        ant = row['antecedents_str']
        con = row['consequents_str']
        g.add_node(ant, label=ant)
        g.add_node(con, label=con)
        g.add_edge(ant, con, title=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}")
    g.show(output_html)

create_pyvis_graph(rule1, "rule1_X_to_Y.html")
create_pyvis_graph(rule2, "rule2_XY_to_Z.html")
create_pyvis_graph(rule3, "rule3_XYZ_to_A.html")

print("‚úÖ Done! Top 20 rules per level exported and visualized.")















# 8. Enhanced Sankey Diagram Function
# ------------------------------------
def plot_sankey(rules_df, title):
    if rules_df.empty:
        print(f"No rules to display for {title}.")
        return

    # Prepare nodes: sorted for clean order
    antecedent_nodes = sorted(set().union(*rules_df['antecedents']))
    consequent_nodes = sorted(set().union(*rules_df['consequents']))
    nodes = antecedent_nodes + [n for n in consequent_nodes if n not in antecedent_nodes]

    node_map = {k: v for v, k in enumerate(nodes)}
    sources, targets, values, customdata = [], [], [], []

    for _, row in rules_df.iterrows():
        for antecedent in row['antecedents']:
            for consequent in row['consequents']:
                sources.append(node_map[antecedent])
                targets.append(node_map[consequent])
                values.append(row['confidence'])
                customdata.append(f"Lift: {row['lift']:.2f}, Support: {row['support']:.2f}")

    # Assign colors: blue for antecedents, orange for consequents
    colors = ['#1f77b4' if n in antecedent_nodes else '#ff7f0e' for n in nodes]

    fig = go.Figure(data=[go.Sankey(
        node=dict(
            pad=20,
            thickness=25,
            line=dict(color="black", width=0.5),
            label=nodes,
            color=colors,
        ),
        link=dict(
            source=sources,
            target=targets,
            value=values,
            hovertemplate='Conf: %{value:.2f}<br>%{customdata}<extra></extra>',
            customdata=customdata,
            color="rgba(150,150,150,0.5)"
        ))])

    fig.update_layout(
        title_text=title,
        font_size=12,
        height=700,
        width=1200,
        plot_bgcolor='white',
        paper_bgcolor='white'
    )
    fig.show()

# ------------------------------------
# 9. Filter and Display Each Rule Level
# ------------------------------------

# A. If X ‚ûî Y
rules_XY = rules[(rules['antecedents'].apply(len) == 1) &
                 (rules['consequents'].apply(len) == 1)]
print("‚úÖ If X ‚ûî Y Rules")
print(rules_XY[['antecedents','consequents','support','confidence','lift']])
plot_sankey(rules_XY, "If X ‚ûî Y Sankey Diagram")

# B. If X and Y ‚ûî Z
rules_XY_Z = rules[(rules['antecedents'].apply(len) == 2) &
                   (rules['consequents'].apply(len) == 1)]
print("\n‚úÖ If X and Y ‚ûî Z Rules")
print(rules_XY_Z[['antecedents','consequents','support','confidence','lift']])
plot_sankey(rules_XY_Z, "If X and Y ‚ûî Z Sankey Diagram")

# C. If X and Y and Z ‚ûî A
rules_XYZ_A = rules[(rules['antecedents'].apply(len) == 3) &
                    (rules['consequents'].apply(len) == 1)]
print("\n‚úÖ If X and Y and Z ‚ûî A Rules")
print(rules_XYZ_A[['antecedents','consequents','support','confidence','lift']])
plot_sankey(rules_XYZ_A, "If X and Y and Z ‚ûî A Sankey Diagram")








# ------------------------------------
# 1. Import Required Libraries
# ------------------------------------
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import networkx as nx
import plotly.graph_objects as go

# ------------------------------------
# 2. User-defined Thresholds
# ------------------------------------
min_lift = float(input("Enter minimum lift threshold (e.g. 1.2): "))
min_confidence = float(input("Enter minimum confidence threshold (e.g. 0.6): "))

# ------------------------------------
# 3. Load Data
# ------------------------------------
df = pd.read_csv('your_data.csv')  # Replace with your file path

# ------------------------------------
# 4. Data Preprocessing
# ------------------------------------
df['MT_EFF_DATE'] = pd.to_datetime(df['MT_EFF_DATE'])

# Filter last 3 months
max_date = df['MT_EFF_DATE'].max()
start_date = max_date - pd.DateOffset(months=3)
filtered_df = df[df['MT_EFF_DATE'] >= start_date].copy()

# Filter customers with ‚â•5 transactions
cust_txn_counts = filtered_df.groupby('ACCT').size()
eligible_customers = cust_txn_counts[cust_txn_counts >= 5].index
filtered_df = filtered_df[filtered_df['ACCT'].isin(eligible_customers)].copy()

# Create basket as customer level
filtered_df['basket_id'] = filtered_df['ACCT'].astype(str)

# Group sectors per customer and deduplicate
basket_sector = filtered_df.groupby('basket_id')['sector'].apply(lambda x: list(set(x))).tolist()

# ------------------------------------
# 5. One-Hot Encoding
# ------------------------------------
te = TransactionEncoder()
te_ary = te.fit(basket_sector).transform(basket_sector)
basket_df = pd.DataFrame(te_ary, columns=te.columns_)

# ------------------------------------
# 6. Apriori Frequent Itemsets
# ------------------------------------
frequent_itemsets = apriori(basket_df, min_support=0.01, use_colnames=True, max_len=4)

# ------------------------------------
# 7. Generate Association Rules
# ------------------------------------
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# Filter by user-defined lift and confidence
rules = rules[(rules['lift'] >= min_lift) & (rules['confidence'] >= min_confidence)]

# ------------------------------------
# 8. Filter Rule Levels
# ------------------------------------

# A. If X ‚ûî Y
rules_XY = rules[(rules['antecedents'].apply(len) == 1) &
                 (rules['consequents'].apply(len) == 1)]

# B. If X and Y ‚ûî Z
rules_XY_Z = rules[(rules['antecedents'].apply(len) == 2) &
                   (rules['consequents'].apply(len) == 1)]

# C. If X and Y and Z ‚ûî A
rules_XYZ_A = rules[(rules['antecedents'].apply(len) == 3) &
                    (rules['consequents'].apply(len) == 1)]

# ------------------------------------
# 9. Display Outputs
# ------------------------------------
print("‚úÖ If X ‚ûî Y Rules")
print(rules_XY[['antecedents','consequents','support','confidence','lift']])

print("\n‚úÖ If X and Y ‚ûî Z Rules")
print(rules_XY_Z[['antecedents','consequents','support','confidence','lift']])

print("\n‚úÖ If X and Y and Z ‚ûî A Rules")
print(rules_XYZ_A[['antecedents','consequents','support','confidence','lift']])

# ------------------------------------
# 10. Interactive Plotly Network Graph Function
# ------------------------------------
def plotly_network_graph(rules_df, title):
    G = nx.DiGraph()

    # Build NetworkX graph
    for _, row in rules_df.iterrows():
        for antecedent in row['antecedents']:
            for consequent in row['consequents']:
                G.add_edge(antecedent, consequent,
                           weight=row['lift'],
                           label=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}")

    pos = nx.spring_layout(G)

    edge_x = []
    edge_y = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x += [x0, x1, None]
        edge_y += [y0, y1, None]

    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=2, color='#888'),
        hoverinfo='none',
        mode='lines')

    node_x = []
    node_y = []
    node_text = []
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_text.append(node)

    node_trace = go.Scatter(
        x=node_x, y=node_y,
        mode='markers+text',
        hoverinfo='text',
        marker=dict(
            showscale=False,
            color='skyblue',
            size=20,
            line_width=2),
        text=node_text,
        textposition="bottom center"
    )

    fig = go.Figure(data=[edge_trace, node_trace],
                    layout=go.Layout(
                        title=title,
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=20,l=5,r=5,t=40),
                        xaxis=dict(showgrid=False, zeroline=False),
                        yaxis=dict(showgrid=False, zeroline=False))
                   )

    fig.show()

# ------------------------------------
# 11. Generate Interactive Graphs
# ------------------------------------

# If X ‚ûî Y
plotly_network_graph(rules_XY, "If X ‚ûî Y Rules Network Graph (Plotly)")

# If X and Y ‚ûî Z
plotly_network_graph(rules_XY_Z, "If X and Y ‚ûî Z Rules Network Graph (Plotly)")

# If X and Y and Z ‚ûî A
plotly_network_graph(rules_XYZ_A, "If X and Y and Z ‚ûî A Rules Network Graph (Plotly)")





















# -------------------------------
# 1. Import Required Libraries
# -------------------------------
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
from pyvis.network import Network

# -------------------------------
# 2. Load Data
# -------------------------------
df = pd.read_csv('your_data.csv')  # Replace with your actual file path

# -------------------------------
# 3. Data Preprocessing
# -------------------------------
# Convert 'MT_EFF_DATE' to datetime
df['MT_EFF_DATE'] = pd.to_datetime(df['MT_EFF_DATE'])

# Filter data for last 3 months
max_date = df['MT_EFF_DATE'].max()
start_date = max_date - pd.DateOffset(months=3)
filtered_df = df[df['MT_EFF_DATE'] >= start_date].copy()

# Filter customers with at least 5 transactions
cust_txn_counts = filtered_df.groupby('ACCT').size()
eligible_customers = cust_txn_counts[cust_txn_counts >= 5].index
filtered_df = filtered_df[filtered_df['ACCT'].isin(eligible_customers)].copy()

# Define basket as customer-level
filtered_df['basket_id'] = filtered_df['ACCT'].astype(str)

# Group sectors per customer and deduplicate
basket_sector = filtered_df.groupby('basket_id')['sector'].apply(lambda x: list(set(x))).tolist()

# -------------------------------
# 4. One-Hot Encoding
# -------------------------------
te = TransactionEncoder()
te_ary = te.fit(basket_sector).transform(basket_sector)
basket_df = pd.DataFrame(te_ary, columns=te.columns_)

# -------------------------------
# 5. Apriori Frequent Itemsets
# -------------------------------
frequent_itemsets = apriori(basket_df, min_support=0.01, use_colnames=True, max_len=4)

# -------------------------------
# 6. Generate Association Rules
# -------------------------------
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# -------------------------------
# 7. Filter Different Rule Levels
# -------------------------------

# A. If X ‚ûî Y
rules_XY = rules[(rules['antecedents'].apply(len) == 1) &
                 (rules['consequents'].apply(len) == 1)]

# B. If X and Y ‚ûî Z
rules_XY_Z = rules[(rules['antecedents'].apply(len) == 2) &
                   (rules['consequents'].apply(len) == 1)]

# C. If X and Y and Z ‚ûî A
rules_XYZ_A = rules[(rules['antecedents'].apply(len) == 3) &
                    (rules['consequents'].apply(len) == 1)]

# -------------------------------
# 8. Display Rule Outputs
# -------------------------------
print("‚úÖ If X ‚ûî Y Rules")
print(rules_XY[['antecedents','consequents','support','confidence','lift']])

print("\n‚úÖ If X and Y ‚ûî Z Rules")
print(rules_XY_Z[['antecedents','consequents','support','confidence','lift']])

print("\n‚úÖ If X and Y and Z ‚ûî A Rules")
print(rules_XYZ_A[['antecedents','consequents','support','confidence','lift']])

# -------------------------------
# 9. Interactive Graph Function Using PyVis
# -------------------------------
def interactive_rules_graph(rules_df, filename, title):
    net = Network(height='600px', width='100%', notebook=True, directed=True)
    net.force_atlas_2based()

    # Add nodes and edges
    for _, row in rules_df.iterrows():
        for antecedent in row['antecedents']:
            net.add_node(antecedent, label=antecedent)
        for consequent in row['consequents']:
            net.add_node(consequent, label=consequent)
        for antecedent in row['antecedents']:
            for consequent in row['consequents']:
                net.add_edge(antecedent, consequent,
                             title=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}",
                             value=row['confidence'])

    net.show_buttons()
    net.show(filename)
    print(f"‚úÖ {title} graph saved as {filename}")

# -------------------------------
# 10. Generate Interactive Graphs for Each Rule Level
# -------------------------------

# If X ‚ûî Y
interactive_rules_graph(rules_XY, 'rules_XY_graph.html', "If X ‚ûî Y")

# If X and Y ‚ûî Z
interactive_rules_graph(rules_XY_Z, 'rules_XY_Z_graph.html', "If X and Y ‚ûî Z")

# If X and Y and Z ‚ûî A
interactive_rules_graph(rules_XYZ_A, 'rules_XYZ_A_graph.html', "If X and Y and Z ‚ûî A")






















# After date filtering

# Calculate transaction count per customer
cust_txn_counts = filtered_df.groupby('cusid').size()

# Filter customers with at least 5 transactions
eligible_customers = cust_txn_counts[cust_txn_counts >= 5].index

# Filter dataset
filtered_df = filtered_df[filtered_df['cusid'].isin(eligible_customers)].copy()

print(f"Using {len(eligible_customers)} customers with >=5 transactions for analysis")











# 1. Import Libraries
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import networkx as nx
import matplotlib.pyplot as plt

# 2. Load Data
df = pd.read_csv('your_data.csv')  # Replace with your actual file path

# 3. Convert 'date' column to datetime (good practice for consistency)
df['date'] = pd.to_datetime(df['date'])

# 4. Filter Data for Desired Time Period (Example: Last 3 months from max date)
max_date = df['date'].max()
start_date = max_date - pd.DateOffset(months=3)

filtered_df = df[df['date'] >= start_date].copy()

print(f"Using data from {start_date.date()} to {max_date.date()}")

# ‚úîÔ∏è Alternatively, for custom period selection:
# start_date = pd.to_datetime('2025-04-01')
# end_date = pd.to_datetime('2025-06-30')
# filtered_df = df[(df['date'] >= start_date) & (df['date'] <= end_date)].copy()

# 5. Define Basket as Customer Level for Overall Affinity
filtered_df['basket_id'] = filtered_df['cusid'].astype(str)

# 6. Group into unique sector lists per customer (deduplicated)
basket_sector = filtered_df.groupby('basket_id')['sector'].apply(lambda x: list(set(x))).tolist()

# 7. One-Hot Encoding
te = TransactionEncoder()
te_ary = te.fit(basket_sector).transform(basket_sector)
basket_df = pd.DataFrame(te_ary, columns=te.columns_)

# 8. Apply Apriori Algorithm
frequent_itemsets = apriori(basket_df, min_support=0.005, use_colnames=True)

# 9. Generate Association Rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# 10. Filter Strong Rules (example: Lift > 1.2, Confidence > 0.3)
strong_rules = rules[(rules['lift'] > 1.2) & (rules['confidence'] > 0.3)]

# Display Strong Rules
print("Strong Association Rules:")
print(strong_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# 11. Visualization using NetworkX
G = nx.DiGraph()

# Add edges with lift as edge attribute
for _, row in strong_rules.iterrows():
    for antecedent in row['antecedents']:
        for consequent in row['consequents']:
            G.add_edge(antecedent, consequent, weight=row['lift'], label=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}")

# Draw Network Graph
plt.figure(figsize=(12,8))
pos = nx.spring_layout(G, k=0.5)

nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='skyblue')
nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20)
nx.draw_networkx_labels(G, pos, font_size=10, font_family="sans-serif")

edge_labels = nx.get_edge_attributes(G, 'label')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9)

plt.title("Overall Customer Purchase Affinity - Strong Rules Network Graph")
plt.axis('off')
plt.show()
























# 1. Import Libraries
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import networkx as nx
import matplotlib.pyplot as plt

# 2. Load Data
df = pd.read_csv('your_data.csv')  # Replace with your file path

# 3. Convert 'date' column to datetime
df['date'] = pd.to_datetime(df['date'])

# 4. Filter Data for Desired Time Period
# Example: Filter last 3 months from max date in dataset

# Calculate max date
max_date = df['date'].max()

# Calculate start date for last 3 months
start_date = max_date - pd.DateOffset(months=3)

# Apply filter
filtered_df = df[df['date'] >= start_date].copy()

print(f"Using data from {start_date.date()} to {max_date.date()}")

# ‚úîÔ∏è Alternatively, for custom period selection:
# start_date = pd.to_datetime('2025-04-01')
# end_date = pd.to_datetime('2025-06-30')
# filtered_df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]

# 5. Create Basket ID
filtered_df['basket_id'] = filtered_df['cusid'].astype(str) + "_" + filtered_df['date'].astype(str)

# 6. Group into transaction lists
basket_sector = filtered_df.groupby('basket_id')['sector'].apply(list).tolist()

# 7. One-Hot Encoding
te = TransactionEncoder()
te_ary = te.fit(basket_sector).transform(basket_sector)
basket_df = pd.DataFrame(te_ary, columns=te.columns_)

# 8. Apply Apriori
frequent_itemsets = apriori(basket_df, min_support=0.01, use_colnames=True)

# 9. Generate Association Rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# 10. Filter Strong Rules: Lift > 1.2, Confidence > 0.6
strong_rules = rules[(rules['lift'] > 1.2) & (rules['confidence'] > 0.6)]

# Display Strong Rules
print("Strong Association Rules:")
print(strong_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# 11. Visualization using NetworkX
G = nx.DiGraph()

# Add edges with lift as edge attribute
for _, row in strong_rules.iterrows():
    for antecedent in row['antecedents']:
        for consequent in row['consequents']:
            G.add_edge(antecedent, consequent, weight=row['lift'], label=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}")

# Draw Network Graph
plt.figure(figsize=(12,8))
pos = nx.spring_layout(G, k=0.5)

nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='skyblue')
nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20)
nx.draw_networkx_labels(G, pos, font_size=10, font_family="sans-serif")

edge_labels = nx.get_edge_attributes(G, 'label')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9)

plt.title("Market Basket Analysis Strong Rules Network Graph")
plt.axis('off')
plt.show()









# 1. Import Libraries
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import networkx as nx
import matplotlib.pyplot as plt

# 2. Load Data
df = pd.read_csv('your_data.csv')  # Replace with your actual file

# 3. Create Basket ID
df['basket_id'] = df['cusid'].astype(str) + "_" + df['date'].astype(str)

# 4. Group into transaction lists
basket_sector = df.groupby('basket_id')['sector'].apply(list).tolist()

# 5. One-Hot Encoding
te = TransactionEncoder()
te_ary = te.fit(basket_sector).transform(basket_sector)
basket_df = pd.DataFrame(te_ary, columns=te.columns_)

# 6. Apply Apriori
frequent_itemsets = apriori(basket_df, min_support=0.01, use_colnames=True)

# 7. Generate Association Rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# 8. Filter Strong Rules: Lift > 1.2, Confidence > 0.6
strong_rules = rules[(rules['lift'] > 1.2) & (rules['confidence'] > 0.6)]

# Display Strong Rules
print("Strong Association Rules:")
print(strong_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# 9. Visualization using NetworkX
G = nx.DiGraph()

# Add edges with lift as edge attribute
for _, row in strong_rules.iterrows():
    for antecedent in row['antecedents']:
        for consequent in row['consequents']:
            G.add_edge(antecedent, consequent, weight=row['lift'], label=f"Conf: {row['confidence']:.2f}, Lift: {row['lift']:.2f}")

# Draw Network Graph
plt.figure(figsize=(12,8))
pos = nx.spring_layout(G, k=0.5)  # k controls distance between nodes

# Draw nodes and edges
nx.draw_networkx_nodes(G, pos, node_size=1500, node_color='skyblue')
nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20)
nx.draw_networkx_labels(G, pos, font_size=10, font_family="sans-serif")

# Draw edge labels
edge_labels = nx.get_edge_attributes(G, 'label')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9)

plt.title("Market Basket Analysis Strong Rules Network Graph")
plt.axis('off')
plt.show()




















Sub WaterfallPivotWithUpdatedConditions()
    Dim ws As Worksheet
    Set ws = ThisWorkbook.Sheets("Sheet1") ' Change if needed

    Dim pvt As PivotTable
    Set pvt = ws.PivotTables(1)

    Dim pf As PivotField
    Dim i As Long
    Dim filterVal As String

    Application.ScreenUpdating = False ' For faster execution

    For i = 1 To 62
        Set pf = pvt.PageFields(i)

        ' Set filter value based on conditions
        Select Case i
            Case 38
                filterVal = "Not Part of"
            Case 51 To 54
                filterVal = "ELIGIBLE"
            Case 60 To 61
                filterVal = "1"
            Case 62
                filterVal = "OTHR"
            Case Else
                filterVal = "0"
        End Select

        ' Apply the filter; skip silently if value not found
        On Error Resume Next
        pf.CurrentPage = filterVal
        On Error GoTo 0

        ' Refresh pivot after applying each filter
        pvt.RefreshTable

        ' Copy sum cust and acct from A65 and B65 to E & F in row i
        ws.Cells(i, "E").Value = ws.Range("A65").Value
        ws.Cells(i, "F").Value = ws.Range("B65").Value

        ' Log applied filter value in column D
        ws.Cells(i, "D").Value = filterVal
    Next i

    Application.ScreenUpdating = True

    MsgBox "Waterfall extraction complete with updated conditions.", vbInformation
End Sub















Sub WaterfallPivotWithExceptions()
    Dim ws As Worksheet
    Set ws = ThisWorkbook.Sheets("Sheet1") ' Change if needed

    Dim pvt As PivotTable
    Set pvt = ws.PivotTables(1)

    Dim pf As PivotField
    Dim i As Long
    Dim filterVal As String

    For i = 1 To 61
        Set pf = pvt.PageFields(i)

        ' Set filter value based on conditions
        Select Case i
            Case 38
                filterVal = "Not Part of"
            Case 51 To 54
                filterVal = "ELIGIBLE"
            Case Else
                filterVal = "0"
        End Select

        ' Try applying the filter (skip if value not found)
        On Error Resume Next
        pf.CurrentPage = filterVal
        On Error GoTo 0

        ' Refresh pivot after each change
        pvt.RefreshTable

        ' Copy sum cust and acct to columns E and F
        ws.Cells(i, "E").Value = ws.Range("A64").Value
        ws.Cells(i, "F").Value = ws.Range("B64").Value

        ' Optional: log applied filter value in column D
        ws.Cells(i, "D").Value = filterVal
    Next i

    MsgBox "Waterfall extraction complete.", vbInformation
End Sub









Sub WaterfallPivotFilterCopy()
    Dim ws As Worksheet
    Set ws = ThisWorkbook.Sheets("Sheet1") ' Update if needed

    Dim pvt As PivotTable
    Set pvt = ws.PivotTables(1) ' Assuming one pivot table

    Dim i As Long
    Dim pf As PivotField

    ' Loop through filters in A1 to A61
    For i = 1 To 61
        Set pf = pvt.PageFields(i) ' Gets the ith Page filter

        ' Apply filter = "0"
        On Error Resume Next
        pf.CurrentPage = "0"
        On Error GoTo 0

        ' Refresh pivot after each filter update
        pvt.RefreshTable

        ' Copy A64 (cust) and B64 (acct) to E & F of current row
        ws.Cells(i, "E").Value = ws.Range("A64").Value
        ws.Cells(i, "F").Value = ws.Range("B64").Value
    Next i

    MsgBox "Waterfall copy completed for all filters set to 0", vbInformation
End Sub











@echo off
echo [%DATE% %TIME%] === STEP 1: Running Python Web Scraping ===
"C:\Python311\python.exe" "C:\Scripts\python_script_1.py"
IF %ERRORLEVEL% NEQ 0 (
    echo Python Script 1 failed. Exiting.
    exit /b %ERRORLEVEL%
)

echo [%DATE% %TIME%] === STEP 2: Running SAS Processing ===
"C:\Program Files\SASHome\SASFoundation\9.4\sas.exe" -sysin "C:\Scripts\sas_script.sas" -log "C:\Logs\sas_script_%DATE:/=-%.log"
IF %ERRORLEVEL% NEQ 0 (
    echo SAS Script failed. Exiting.
    exit /b %ERRORLEVEL%
)

echo [%DATE% %TIME%] === STEP 3: Running SharePoint Upload Python Script ===
"C:\Python311\python.exe" "C:\Scripts\python_script_2.py"
IF %ERRORLEVEL% NEQ 0 (
    echo Python Script 2 failed. Exiting.
    exit /b %ERRORLEVEL%
)

echo [%DATE% %TIME%] === ALL STEPS COMPLETED SUCCESSFULLY ===








def generate_hot_encoding(df, sectors):
    interaction_matrix = df.pivot_table(index='ACT', columns='Sector', values='TRAN_AMT', aggfunc='sum')
    interaction_matrix = interaction_matrix.notnull().astype(int)
    for sector in sectors:
        if sector not in interaction_matrix.columns:
            interaction_matrix[sector] = 0
    return interaction_matrix[sorted(sectors)]












def generate_hot_encoding(df, sectors):
    df = df[['ACT', 'Sector']].dropna()
    df['Sector'] = df['Sector'].astype(str).str.strip()

    hot_encoded = pd.get_dummies(df, columns=['Sector'], prefix='', prefix_sep='_') \
                     .groupby('ACT').max()

    # Align with master sector list
    for sector in sectors:
        if sector not in hot_encoded.columns:
            hot_encoded[sector] = 0

    # Ensure column order
    return hot_encoded[sorted(sectors)]





import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

# ----------------------
# Load and Preprocess
# ----------------------
def load_and_preprocess_data(filepath):
    df = pd.read_csv(filepath)
    df['MT_EFF_DATE'] = pd.to_datetime(df['MT_EFF_DATE'], errors='coerce')
    df.dropna(subset=['MT_EFF_DATE'], inplace=True)
    return df

# ----------------------
# Interaction Matrix
# ----------------------
def generate_interaction_matrix(df, sectors, metric='amount'):
    if metric == 'amount':
        matrix = df.groupby(['ACT', 'Sector'])['TRAN_AMT'].sum().unstack(fill_value=0)
    elif metric == 'frequency':
        matrix = df.groupby(['ACT', 'Sector']).size().unstack(fill_value=0)
    else:
        raise ValueError("Metric must be 'amount' or 'frequency'")
    
    for sector in sectors:
        if sector not in matrix.columns:
            matrix[sector] = 0
    return matrix[sorted(sectors)].astype(np.float32)

# ----------------------
# Normalize Matrix
# ----------------------
def scale_matrix(matrix):
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(matrix)
    return pd.DataFrame(scaled, index=matrix.index, columns=matrix.columns)

# ----------------------
# One-hot Encoding
# ----------------------
def generate_hot_encoding(df, sectors):
    hot_encoded = pd.get_dummies(df[['ACT', 'Sector']], columns=['Sector'], prefix='', prefix_sep='_') \
                    .groupby('ACT').max()
    for sector in sectors:
        if sector not in hot_encoded.columns:
            hot_encoded[sector] = 0
    return hot_encoded[sorted(sectors)]

# ----------------------
# Combine Cosine Similarities
# ----------------------
def combine_similarity(amount_matrix, frequency_matrix, weight_amount=0.6):
    sim_amt = cosine_similarity(amount_matrix.T)
    sim_freq = cosine_similarity(frequency_matrix.T)
    return weight_amount * sim_amt + (1 - weight_amount) * sim_freq

# ----------------------
# Rank Recommendations
# ----------------------
def get_recommendation_ranks(test_matrix, sector_similarity):
    scores = np.dot(test_matrix, sector_similarity)
    ranks = (-scores).argsort(axis=1).argsort(axis=1) + 1
    return pd.DataFrame(ranks, index=test_matrix.index, columns=test_matrix.columns)

# ----------------------
# Rank Actual Transaction
# ----------------------
def generate_transaction_ranks(df, sectors):
    matrix = generate_interaction_matrix(df, sectors, metric='amount')
    return matrix.rank(axis=1, method='max', ascending=False).astype(int)

# ----------------------
# Main Execution
# ----------------------
def run_dual_matrix_recommender(filepath, split_date_str, output_prefix):
    # Load & prep
    df = load_and_preprocess_data(filepath)
    split_date = pd.to_datetime(split_date_str)
    sectors = sorted(df['Sector'].dropna().unique())

    # Split train/test
    train_df = df[df['MT_EFF_DATE'] < split_date]
    test_df = df[df['MT_EFF_DATE'] >= split_date]

    # Matrices
    train_amt_matrix = generate_interaction_matrix(train_df, sectors, metric='amount')
    train_freq_matrix = generate_interaction_matrix(train_df, sectors, metric='frequency')
    test_amt_matrix = generate_interaction_matrix(test_df, sectors, metric='amount')

    # Normalize
    train_amt_scaled = scale_matrix(train_amt_matrix)
    train_freq_scaled = scale_matrix(train_freq_matrix)
    test_amt_scaled = scale_matrix(test_amt_matrix)

    # Combined similarity
    sector_similarity = combine_similarity(train_amt_scaled, train_freq_scaled, weight_amount=0.6)

    # Hot encodings
    train_hot_encoding = generate_hot_encoding(train_df, sectors)
    test_hot_encoding = generate_hot_encoding(test_df, sectors)

    # Ranks
    test_recommendations = get_recommendation_ranks(test_amt_scaled, sector_similarity)
    test_transaction_ranks = generate_transaction_ranks(test_df, sectors)

    # Export
    test_recommendations.to_csv(f'{output_prefix}_recommendations.csv')

    with pd.ExcelWriter(f'{output_prefix}_full_output.xlsx', engine='openpyxl') as writer:
        train_hot_encoding.to_excel(writer, sheet_name='Train Hot Encoding')
        test_hot_encoding.to_excel(writer, sheet_name='Test Hot Encoding')
        test_recommendations.to_excel(writer, sheet_name='Recommendations')
        test_transaction_ranks.to_excel(writer, sheet_name='Actual Transaction Ranks')

    print(f"‚úÖ Recommendation engine completed using both amount & frequency (weight 60/40).")
    print(f"üìÅ Files saved to: {output_prefix}_recommendations.csv and {output_prefix}_full_output.xlsx")

# Example run
run_dual_matrix_recommender(
    filepath='Z:/WORK/Persona Data/CSAT_LV_SPEND1.csv',
    split_date_str='2024-12-31',
    output_prefix='Z:/WORK/Persona Data/FinalRecommender_June18'
)



















/* Step 1: Setup options and define remote controller */
options nomprint nosymbolgen NETENCRYPTALGORITHM=TRIPLEDES noconnectmetaconnection;

%let control = aspsas2-cnt-eng.hk.hsbc 7551;
options remote=control;

/* Step 2: Safely terminate and re-establish a clean remote connection */
signoff control;
signon control user="your_user_id" password="your_password";   /* Replace with secure credentials or metadata binding */

/* Step 3: RSUBMIT block to handle binary file transfer */
rsubmit;

/* Define source ZIP file on HK SAS server (do NOT use encoding with binary files) */
filename rfile "/sasdata/hsbc/dil/INM/IMCE/external_data_transfer/GupShup/landing/inbound/hsbc_premier_23rdto26thApril2025";

/* Define destination ZIP file on INM SAS server */
filename lfile "/appvol/mix_nas/inm/INMDNA/IMCE/sandbox/Shikhar/WA_auto/hsbc_premier_23rdto26thApril2025.zip";

/* Perform secure binary download */
proc download infile=rfile outfile=lfile binary;
run;

endrsubmit;

/* Step 4: Close the session */
signoff control;










/* Step 1: General Options */
options nomprint nosymbolgen NETENCRYPTALGORITHM=TRIPLEDES noconnectmetaconnection;

/* Step 2: Define remote server and port */
%let control = aspsas2-cnt-eng.hk.hsbc 7551;
options remote=control;

/* Step 3: Safely sign off any existing session and sign on with credentials */
signoff control;
signon control user="your_user_id" password="your_password";  /* üîí Use secure method in production */

/* Step 4: Begin RSUBMIT to HK Grid */
rsubmit;

/* Define source file path on HK SAS server */
filename rfile "/sasdata/hsbc/dil/INM/IMCE/external_data_transfer/GupShup/landing/inbound/hsbc_premier_23rdto26thApril2025";

/* Define destination path on INM SAS server */
filename lfile "/appvol/mix_nas/inm/INMDNA/IMCE/sandbox/Shikhar/WA_auto/hsbc_premier_23rdto26thApril2025.zip";

/* Download the file from remote to local in binary mode */
proc download infile=rfile outfile=lfile binary;
run;

endrsubmit;

/* Step 5: Close the remote connection */
signoff control;







%macro transfer_file(remote_path=, local_path=, user=, password=);
    options nomprint nosymbolgen NETENCRYPTALGORITHM=TRIPLEDES;

    %let control=aspsas2-cnt-eng.hk.hsbc 7551;
    options remote=control;

    signoff control;
    signon control user="&user" password="&password";

    rsubmit;
        filename rfile "&remote_path";
        filename lfile "&local_path";
        proc download infile=rfile outfile=lfile binary;
        run;
    endrsubmit;

    signoff control;
%mend;

%transfer_file(
    remote_path=/sasdata/hsbc/dil/INM/IMCE/external_data_transfer/GupShup/landing/inbound/hsbc_premier_23rdto26thApril2025,
    local_path=/appvol/mix_nas/inm/INMDNA/IMCE/sandbox/Shikhar/WA_auto/hsbc_premier_23rdto26thApril2025.zip,
    user=your_user,
    password=your_password
);















Sub MultiFileLookupAllDirections()
    Dim wsConfig As Worksheet, wsMaster As Worksheet
    Dim lookupKey As String, variablesToExtract() As String
    Dim fDialog As FileDialog, folderPath As String
    Dim fileName As String, wbSource As Workbook, wsSource As Worksheet
    Dim headerDict As Object, resultDict As Object, tempDict As Object
    Dim keyVal As Variant, varName As Variant
    Dim masterKeyColLetter As String, masterKeyCol As Long
    Dim lastRow As Long, i As Long, j As Long, outputColStart As Long
    Dim srcLastRow As Long, srcLastCol As Long
    Dim rowArr As Variant, headers As Variant

    Application.ScreenUpdating = False
    Application.DisplayAlerts = False
    Application.EnableEvents = False

    ' Setup sheets
    Set wsConfig = ThisWorkbook.Sheets("Config")
    Set wsMaster = ThisWorkbook.Sheets("MasterList")

    ' Read config
    lookupKey = Trim(wsConfig.Range("B1").Value)
    variablesToExtract = Split(wsConfig.Range("B2").Value, ",")
    For i = 0 To UBound(variablesToExtract)
        variablesToExtract(i) = Trim(variablesToExtract(i))
    Next i

    ' Prompt for folder
    Set fDialog = Application.FileDialog(msoFileDialogFolderPicker)
    With fDialog
        .Title = "Select Folder Containing Excel Files"
        If .Show <> -1 Then
            MsgBox "No folder selected. Exiting.", vbExclamation
            Exit Sub
        End If
        folderPath = .SelectedItems(1) & "\"
    End With

    ' Prompt for master column
    masterKeyColLetter = InputBox("Enter the column letter (A-Z) in 'MasterList' that contains the lookup key:", "Select Lookup Column", "A")
    If masterKeyColLetter = "" Then Exit Sub
    masterKeyCol = Range(masterKeyColLetter & "1").Column

    ' Collect lookup values
    Dim lookupDict As Object
    Set lookupDict = CreateObject("Scripting.Dictionary")
    lastRow = wsMaster.Cells(wsMaster.Rows.Count, masterKeyCol).End(xlUp).Row
    For i = 2 To lastRow
        keyVal = Trim(wsMaster.Cells(i, masterKeyCol).Value)
        If keyVal <> "" Then
            lookupDict(CStr(keyVal)) = i ' store row number
        End If
    Next i

    ' Prepare result dictionary
    Set resultDict = CreateObject("Scripting.Dictionary")

    ' Loop files
    fileName = Dir(folderPath & "*.xls*")
    Do While fileName <> ""
        Set wbSource = Workbooks.Open(folderPath & fileName, False, True)

        For Each wsSource In wbSource.Sheets
            srcLastRow = wsSource.Cells(wsSource.Rows.Count, 1).End(xlUp).Row
            srcLastCol = wsSource.Cells(1, wsSource.Columns.Count).End(xlToLeft).Column
            headers = wsSource.Range(wsSource.Cells(1, 1), wsSource.Cells(1, srcLastCol)).Value

            ' Build header dictionary
            Set headerDict = CreateObject("Scripting.Dictionary")
            For j = 1 To srcLastCol
                If Trim(headers(1, j)) <> "" Then
                    headerDict(Trim(headers(1, j))) = j
                End If
            Next j

            ' Continue only if lookupKey column exists
            If headerDict.exists(lookupKey) Then
                Dim keyCol As Long: keyCol = headerDict(lookupKey)

                For i = 2 To srcLastRow
                    rowArr = wsSource.Range(wsSource.Cells(i, 1), wsSource.Cells(i, srcLastCol)).Value
                    keyVal = rowArr(1, keyCol)
                    If lookupDict.exists(CStr(keyVal)) Then
                        If Not resultDict.exists(CStr(keyVal)) Then
                            Set tempDict = CreateObject("Scripting.Dictionary")
                            For Each varName In variablesToExtract
                                If headerDict.exists(varName) Then
                                    tempDict(varName) = rowArr(1, headerDict(varName))
                                End If
                            Next varName
                            resultDict(CStr(keyVal)) = tempDict
                        End If
                    End If
                Next i
            End If
        Next wsSource

        wbSource.Close SaveChanges:=False
        fileName = Dir
    Loop

    ' Output to MasterList
    outputColStart = masterKeyCol + 1
    For j = 0 To UBound(variablesToExtract)
        wsMaster.Cells(1, outputColStart + j).Value = variablesToExtract(j)
    Next j

    For Each keyVal In resultDict.Keys
        i = lookupDict(keyVal)
        Set tempDict = resultDict(keyVal)
        For j = 0 To UBound(variablesToExtract)
            varName = variablesToExtract(j)
            If tempDict.exists(varName) Then
                wsMaster.Cells(i, outputColStart + j).Value = tempDict(varName)
            End If
        Next j
    Next keyVal

    MsgBox "Lookup complete. Data written to 'MasterList'.", vbInformation

    Application.ScreenUpdating = True
    Application.DisplayAlerts = True
    Application.EnableEvents = True
End Sub





















proc univariate data=your_dataset noprint;
    var propensity_score;
    output out=percentile_cutoffs
        pctlpts = 10 20 30 40 50 60 70 80 90
        pctlpre = P_;
run;

data final_with_deciles;
    if _N_ = 1 then set percentile_cutoffs; /* Load percentiles once */
    set your_dataset;

    if propensity_score <= P_10 then decile = 1;
    else if propensity_score <= P_20 then decile = 2;
    else if propensity_score <= P_30 then decile = 3;
    else if propensity_score <= P_40 then decile = 4;
    else if propensity_score <= P_50 then decile = 5;
    else if propensity_score <= P_60 then decile = 6;
    else if propensity_score <= P_70 then decile = 7;
    else if propensity_score <= P_80 then decile = 8;
    else if propensity_score <= P_90 then decile = 9;
    else decile = 10;
run;

















proc rank data=your_dataset out=ranked_dataset groups=10;
    var propensity_score;
    ranks decile;
run;

data ranked_dataset;
    set ranked_dataset;
    decile = decile + 1; /* To make deciles 1 to 10 instead of 0 to 9 */
run;












/* Step 1: Filter Card Only customers from Dec 2023 */
proc sql;
    create table dec23_card_only as
    select cusid, final_cus_seg as seg_dec23
    from dec23_data
    where final_cus_seg = "Card Only";
quit;

/* Step 2: Filter active customers from Dec 2024 */
proc sql;
    create table dec24_active as
    select cusid, final_cus_seg as seg_dec24
    from dec24_data
    where active_dec24 = 1;
quit;

/* Step 3: Join and build transition labels */
proc sql;
    create table card_only_transitions as
    select 
        a.cusid,
        a.seg_dec23,
        coalesce(b.seg_dec24, "Dropped") as seg_dec24,
        cats(a.seg_dec23, " ‚Üí ", coalesce(b.seg_dec24, "Dropped")) as segment_transition
    from dec23_card_only a
    left join dec24_active b
    on a.cusid = b.cusid;
quit;

/* Step 4: Summary counts by transition */
proc freq data=card_only_transitions;
    tables segment_transition / nocum nopercent;
run;








proc sql noprint;
    select max(tran_date) into :last_date from trans_data;
quit;

/* Step 2: Compute activity summary using last_date as anchor */
proc sql;
    create table cust_activity_summary as
    select 
        cust_id,
        count(distinct intnx('month', tran_date, 0, 'b')) as active_months,

        case 
            when max(intnx('month', tran_date, 0, 'b')) >= intnx('month', &last_date, -3, 'b') 
                then "3M Active"
            when max(intnx('month', tran_date, 0, 'b')) >= intnx('month', &last_date, -6, 'b') 
                then "6M Active"
            when max(intnx('month', tran_date, 0, 'b')) >= intnx('month', &last_date, -9, 'b') 
                then "9M Active"
            else "Inactive"
        end as recent_activity_label
    from trans_data
    group by cust_id;
quit;












import py7zr
import os

def zip_csv_with_py7zr(csv_file_path, zip_file_path, password):
    if not os.path.exists(csv_file_path):
        raise FileNotFoundError(f"{csv_file_path} does not exist.")

    with py7zr.SevenZipFile(zip_file_path, 'w', password=password) as archive:
        archive.write(csv_file_path, arcname=os.path.basename(csv_file_path))

    print(f"‚úÖ AES-encrypted 7z file created: {os.path.abspath(zip_file_path)}")

# === USAGE ===
csv_file = 'sample.csv'
zip_file = 'secure_sample.7z'
password = 'StrongAES123'

zip_csv_with_py7zr(csv_file, zip_file, password)










import zipfile
import os

def zip_with_basic_password(csv_file_path, zip_file_path, password):
    if not os.path.exists(csv_file_path):
        raise FileNotFoundError(f"{csv_file_path} does not exist.")

    with zipfile.ZipFile(zip_file_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:
        # Write file and apply password
        zf.setpassword(password.encode())
        zf.write(csv_file_path, arcname=os.path.basename(csv_file_path))

    print(f"‚úÖ Basic password-protected ZIP created: {zip_file_path}")

# === USAGE ===
csv_file = 'sample.csv'
zip_file = 'sample_protected.zip'
password = 'Basic123'

zip_with_basic_password(csv_file, zip_file, password)










/*********************************************************************
*  SAS Automated Workflow with Dual-Key Deduplication                *
*  - Direct SFTP folder access                                       *
*  - Master log tracks both mobile_number and cusid                  *
**********************************************************************/

/* --- PARAMETERS --- */
%let today = %sysfunc(today(), yymmddn8.);
%let yyyymmdd = %sysfunc(putn(&today, yymmddn8.));
%let local_zip = /sftp/inbox/daily_&yyyymmdd..zip;
%let extract_dir = /sas/data/extract/&yyyymmdd.;
%let extract_file = &extract_dir./inputfile.xlsx;
%let output_dir = /sas/data/output/&yyyymmdd.;
%let output_xlsx = &output_dir./matched_customers.xlsx;
%let output_zip = &output_dir./matched_customers.zip;
%let pwd = YourPassword123;
%let eligible_base = /sas/data/base/eligible_base.sas7bdat;
%let log_file = /sas/data/logs/mobile_master_log.sas7bdat;
%let process_log = /sas/data/logs/master_process_log.sas7bdat;
%let email_to = receiver@email.com;

/* --- 1. Create output/extract dirs if not exist --- */
options noxwait;
x "mkdir -p &extract_dir";
x "mkdir -p &output_dir";

/* --- 2. Unzip file --- */
x "unzip -o &local_zip -d &extract_dir";

/* --- 3. Import Excel --- */
proc import datafile="&extract_file"
  out=raw_data dbms=xlsx replace;
  getnames=yes;
run;

/* --- 4. Filter: OTP=YES and T&C=YES --- */
data filtered;
  set raw_data;
  where upcase(otp)='YES' and upcase(terms_and_conditions)='YES';
run;

/* --- 5. Ensure master log exists --- */
%if %sysfunc(exist(mobile_master_log))=0 %then %do;
  data mobile_master_log;
    length mobile_number $20 cusid $20;
    stop;
  run;
%end;

/* --- 6. Join to eligible base for cusid --- */
proc sql;
  create table matched_base as
  select a.*, b.cusid
  from filtered a
  inner join eligible_base b
    on a.mobile_number=b.mobile_number and a.dob=b.dob;
quit;

/* --- 7. Remove already-processed (dedupe by mobile or cusid) --- */
proc sql;
  create table new_customers as
  select *
  from matched_base
  where not exists (
    select 1 from mobile_master_log
    where matched_base.mobile_number = mobile_master_log.mobile_number
       or matched_base.cusid = mobile_master_log.cusid
  );
quit;

/* --- 8. Append both keys to the master log --- */
data to_append;
  set new_customers(keep=mobile_number cusid);
run;

proc append base=mobile_master_log data=to_append force;
run;

/* --- 9. Export final output --- */
proc export data=new_customers
  outfile="&output_xlsx"
  dbms=xlsx replace;
run;

/* --- 10. Password-protect and Zip --- */
x "zip -j -P &pwd &output_zip &output_xlsx";

/* --- 11. Collect Stats for Summary --- */
proc sql noprint;
  select count(*) into :rec_infile from raw_data;
  select count(*) into :filtered_infile from filtered;
  select count(*) into :matched_count from matched_base;
  select count(*) into :final_count from new_customers;
quit;

%let dedup_count = %eval(&matched_count - &final_count);

/* --- 12. Email With Attachment --- */
filename mymail email
  to=("&email_to")
  subject="Daily Eligible Customer File: &yyyymmdd"
  attach=("&output_zip");

data _null_;
  file mymail;
  put "Summary for &yyyymmdd:";
  put "Total customers in file: &rec_infile";
  put "OTP=YES & T&C=YES: &filtered_infile";
  put "After mobile & DOB match: &matched_count";
  put "Duplicates removed (by mobile or cusid): &dedup_count";
  put "Final unique eligible: &final_count";
run;

/* --- 13. Log the Process --- */
data log_today;
  length file_name $100 status $10;
  format date date9.;
  date = today();
  file_name = "daily_&yyyymmdd..zip";
  total_received = &rec_infile;
  filtered_valid = &filtered_infile;
  matched_base = &matched_count;
  duplicates = &dedup_count;
  final_unique = &final_count;
  status = "SUCCESS";
run;

%if %sysfunc(exist(master_process_log))=0 %then %do;
  data master_process_log;
    length file_name $100 status $10;
    format date date9.;
    stop;
  run;
%end;

proc append base=master_process_log data=log_today force;
run;






















view: spend_view2 {
  sql_table_name: `your_project.your_dataset.SPEND_VIEW_2`

  # === DATE GROUP ===
  dimension_group: mt_posting_date {
    type: time
    timeframes: [raw, date, week, month, quarter, year]
    label: "Posting Date"
    group_label: "Transaction Dates"
    sql: ${TABLE}.MT_POSTING_DATE ;;
  }

  # === BASE DIMENSIONS ===
  dimension: acct {
    type: string
    label: "Account Number"
    group_label: "Account Info"
    sql: ${TABLE}.acct ;;
  }
  dimension: mcc_code {
    type: string
    label: "MCC Code"
    group_label: "Merchant Info"
    sql: CAST(${TABLE}.MCC_CODE AS STRING) ;;
  }
  dimension: description {
    type: string
    label: "MCC Description"
    group_label: "Merchant Info"
    sql: ${TABLE}.Description ;;
  }
  dimension: merchant_details {
    type: string
    label: "Merchant Details"
    group_label: "Merchant Info"
    sql: ${TABLE}.Merchant_Details ;;
  }
  dimension: org {
    type: number
    label: "Org ID"
    group_label: "Product Info"
    sql: ${TABLE}.ORG ;;
  }
  dimension: transaction_type {
    type: string
    label: "Transaction Type"
    group_label: "Txn Details"
    sql: ${TABLE}.TRANSACTION_TYPE ;;
  }
  dimension: spend_type {
    type: string
    label: "Spend Type"
    group_label: "Txn Details"
    sql: ${TABLE}.SPEND_TYPE ;;
  }
  dimension: segment_name {
    type: string
    label: "Segment Name"
    group_label: "Merchant Info"
    sql: ${TABLE}.Segment_Name ;;
  }
  dimension: mt_type {
    type: string
    label: "MT Type"
    group_label: "Product Info"
    sql: ${TABLE}.mt_type ;;
  }
  dimension: spend_place {
    type: string
    label: "Txn Geography"
    group_label: "Txn Details"
    sql: ${TABLE}.Spend_Place ;;
  }
  dimension: product {
    type: string
    label: "Product"
    group_label: "Product Info"
    sql: ${TABLE}.Product ;;
  }
  dimension: spend_amount {
    type: number
    label: "Spend Amount"
    group_label: "Txn Amounts"
    sql: ${TABLE}.Spend_Amount ;;
  }
  dimension: reversal_amount {
    type: number
    label: "Reversal Amount"
    group_label: "Txn Amounts"
    sql: ${TABLE}.Reversal_Amount ;;
  }
  dimension: net_transaction {
    type: number
    label: "Net Transaction"
    group_label: "Txn Amounts"
    sql: ${TABLE}.Net_Transaction ;;
  }
  dimension: spend_amount_cmb {
    type: number
    label: "Spend Amount CMB"
    group_label: "Txn Amounts"
    sql: ${TABLE}.SPEND_AMOUNT_CMB ;;
  }
  dimension: spend_amount_rbmw {
    type: number
    label: "Spend Amount RBMW"
    group_label: "Txn Amounts"
    sql: ${TABLE}.SPEND_AMOUNT_RBMW ;;
  }
  dimension: mt_ref_nbr {
    type: string
    label: "Txn Reference No."
    group_label: "Txn Info"
    primary_key: yes
    sql: ${TABLE}.MT_REF_NBR ;;
  }
  dimension: brand {
    type: string
    label: "Brand"
    group_label: "Merchant Info"
    sql: ${TABLE}.BRAND ;;
  }
  dimension: final_category {
    type: string
    label: "Final Category"
    group_label: "Merchant Info"
    sql: ${TABLE}.FINAL_CATEGORY ;;
  }
  dimension: category_general_desc {
    type: string
    label: "Category General Desc"
    group_label: "Merchant Info"
    sql: ${TABLE}.CATEGORY_GENERAL_DESC ;;
  }
  dimension: category_desc {
    type: string
    label: "Category Desc"
    group_label: "Merchant Info"
    sql: ${TABLE}.CATEGORY_DESC ;;
  }

  # === DERIVED DIMENSIONS ===
  dimension: merchant_name {
    type: string
    label: "Merchant Name"
    group_label: "Merchant Info"
    sql: SUBSTRING(${TABLE}.Merchant_Details, 1, 24) ;;
  }
  dimension: merchant_city {
    type: string
    label: "Merchant City"
    group_label: "Merchant Info"
    sql: REPLACE(SUBSTRING(${TABLE}.Merchant_Details, 24, 14), " ", "") ;;
  }
  dimension: merchant_country {
    type: string
    label: "Merchant Country"
    group_label: "Merchant Info"
    sql: SUBSTRING(${TABLE}.Merchant_Details, 38, 2) ;;
  }
  dimension: mcc_combo {
    type: string
    label: "MCC Combo"
    group_label: "Merchant Info"
    sql: CONCAT(${mcc_code}, " - ", ${description}) ;;
  }

  # === DATE UTILITIES ===
  dimension: today {
    type: date
    label: "Today's Date"
    group_label: "Date Utilities"
    sql: current_date() ;;
  }
  dimension: day {
    type: number
    label: "Current Day"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%e', current_date()) AS INT64) ;;
  }
  dimension: mon {
    type: number
    label: "Current Month"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%m', current_date()) AS INT64) ;;
  }
  dimension: yr {
    type: number
    label: "Current Year"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%Y', current_date()) AS INT64) ;;
  }
  dimension: mt_day {
    type: number
    label: "Txn Day"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%e', ${mt_posting_date_date}) AS INT64) ;;
  }
  dimension: mt_mon {
    type: number
    label: "Txn Month"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%m', ${mt_posting_date_date}) AS INT64) ;;
  }
  dimension: mt_yr {
    type: number
    label: "Txn Year"
    group_label: "Date Utilities"
    sql: CAST(FORMAT_DATE('%Y', ${mt_posting_date_date}) AS INT64) ;;
  }

  # === PARAMETERS & DYNAMIC MEASURES ===
  parameter: metric_selector {
    label: "Metric Selector"
    group_label: "Switches"
    allowed_value: { label: "Net Spends in Cr" value: "net" }
    allowed_value: { label: "Gross Spends in Cr" value: "gross" }
  }
  measure: monthly_trend_metric {
    type: number
    label: "Monthly Trend Metric"
    group_label: "Switches"
    description: "Net or Gross Spends in Cr as per selector"
    sql: 
      CASE
        WHEN {% parameter metric_selector %} = 'net' THEN ${total_net_transaction}
        WHEN {% parameter metric_selector %} = 'gross' THEN ${total_spend}
        ELSE NULL
      END ;;
  }
  parameter: wordcloud_metric_selector {
    label: "Word Cloud Metric"
    group_label: "Switches"
    allowed_value: { label: "Net Spends in Cr" value: "net" }
    allowed_value: { label: "Gross Spends in Cr" value: "gross" }
  }
  measure: wordcloud_metric {
    type: number
    label: "Word Cloud Metric"
    group_label: "Switches"
    sql: 
      CASE
        WHEN {% parameter wordcloud_metric_selector %} = 'net' THEN ${total_net_transaction}
        WHEN {% parameter wordcloud_metric_selector %} = 'gross' THEN ${total_spend}
        ELSE NULL
      END ;;
  }

  # === CORE MEASURES ===
  measure: total_spend {
    type: sum
    group_label: "Core Spend"
    label: "Gross Spends in Cr"
    sql: ${spend_amount}/10000000 ;;
    value_format: "#,##0.00"
  }
  measure: total_reversal {
    type: sum
    group_label: "Core Spend"
    label: "Reversals in Cr"
    sql: ${reversal_amount}/10000000 ;;
    value_format: "#,##0.00"
  }
  measure: total_net_transaction {
    type: sum
    group_label: "Core Spend"
    label: "Net Spends in Cr"
    sql: ${net_transaction}/10000000 ;;
    value_format: "#,##0.00"
  }
  measure: average_spend {
    type: average
    group_label: "Core Spend"
    label: "Avg Gross Spend/Txn"
    sql: ${spend_amount} ;;
    value_format: "#,##0.00"
  }
  measure: average_net_transaction {
    type: average
    group_label: "Core Spend"
    label: "Avg Net Spend/Txn"
    sql: ${net_transaction} ;;
    value_format: "#,##0.00"
  }
  measure: transaction_count {
    type: count
    group_label: "Core Spend"
    label: "Txn Count"
  }
  measure: unique_accts_cnt {
    type: count_distinct
    group_label: "Core Spend"
    label: "Unique Accounts"
    sql: ${acct} ;;
  }

  # --- MTD MEASURES ---
  measure: total_net_transactions_mtd {
    type: sum
    group_label: "MTD"
    label: "MTD Net Spends in Cr"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }
  measure: total_spends_mtd {
    type: sum
    group_label: "MTD"
    label: "MTD Gross Spends in Cr"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${spend_amount}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }
  measure: total_reversal_mtd {
    type: sum
    group_label: "MTD"
    label: "MTD Reversals in Cr"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${reversal_amount}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }
  measure: avg_net_transactions_mtd {
    type: average
    group_label: "MTD"
    label: "MTD Avg Net Spend/Txn"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction} ELSE NULL END ;;
    value_format: "#,##0.00"
  }

  # --- BUSINESS/ADVANCED MEASURES ---
  measure: total_gross_spend {
    type: sum
    group_label: "Business"
    label: "Total Gross Spend"
    sql: ${spend_amount} ;;
    value_format: "#,##0.00"
  }
  measure: total_net_spend {
    type: sum
    group_label: "Business"
    label: "Total Net Spend"
    sql: ${net_transaction} ;;
    value_format: "#,##0.00"
  }
  measure: total_reversal_amount {
    type: sum
    group_label: "Business"
    label: "Total Reversal Amount"
    sql: ${reversal_amount} ;;
    value_format: "#,##0.00"
  }
  measure: spend_by_brand {
    type: sum
    group_label: "Business"
    label: "Spend by Brand"
    sql: CASE WHEN ${brand} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format: "#,##0.00"
  }
  measure: txn_count_by_brand {
    type: count
    group_label: "Business"
    label: "Txn Count by Brand"
    filters: [brand: "-null"]
    sql: ${mt_ref_nbr} ;;
  }
  measure: spend_by_txn_type {
    type: sum
    group_label: "Business"
    label: "Spend by Txn Type"
    sql: CASE WHEN ${transaction_type} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format: "#,##0.00"
  }
  measure: spend_domestic {
    type: sum
    group_label: "Business"
    label: "Domestic Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'DOM' THEN ${spend_amount} ELSE 0 END ;;
    value_format: "#,##0.00"
  }
  measure: spend_international {
    type: sum
    group_label: "Business"
    label: "International Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'FR' THEN ${spend_amount} ELSE 0 END ;;
    value_format: "#,##0.00"
  }
  measure: avg_spend_per_account {
    type: number
    group_label: "Business"
    label: "Avg Spend per Account"
    sql: CASE WHEN ${unique_accts_cnt} > 0 THEN ${total_gross_spend} / ${unique_accts_cnt} ELSE NULL END ;;
    value_format: "#,##0.00"
  }
  measure: total_transactions {
    type: count_distinct
    group_label: "Business"
    label: "Unique Transactions"
    sql: ${mt_ref_nbr} ;;
  }
}















view: spend_view2 {
  sql_table_name: `your_project.your_dataset.SPEND_VIEW_2` ;;

  # === DATE GROUPS ===
  dimension_group: mt_posting_date {
    type: time
    timeframes: [raw, date, week, month, quarter, year]
    label: "Posting Date"
    group_label: "Transaction Dates"
    sql: ${TABLE}.MT_POSTING_DATE ;;
  }

  # === BASE DIMENSIONS (ALL DATA FIELDS) ===
  dimension: acct                    { type: string;  label: "Account Number";           group_label: "Account Info";        sql: ${TABLE}.acct ;; }
  dimension: mcc_code                { type: string;  label: "MCC Code";                 group_label: "Merchant Info";       sql: CAST(${TABLE}.MCC_CODE AS STRING) ;; }
  dimension: description             { type: string;  label: "MCC Description";           group_label: "Merchant Info";       sql: ${TABLE}.Description ;; }
  dimension: merchant_details        { type: string;  label: "Merchant Details";          group_label: "Merchant Info";       sql: ${TABLE}.Merchant_Details ;; }
  dimension: org                     { type: number;  label: "Org ID";                   group_label: "Product Info";        sql: ${TABLE}.ORG ;; }
  dimension: transaction_type        { type: string;  label: "Transaction Type";          group_label: "Txn Details";         sql: ${TABLE}.TRANSACTION_TYPE ;; }
  dimension: spend_type              { type: string;  label: "Spend Type";                group_label: "Txn Details";         sql: ${TABLE}.SPEND_TYPE ;; }
  dimension: segment_name            { type: string;  label: "Segment Name";              group_label: "Merchant Info";       sql: ${TABLE}.Segment_Name ;; }
  dimension: mt_type                 { type: string;  label: "MT Type";                   group_label: "Product Info";        sql: ${TABLE}.mt_type ;; }
  dimension: spend_place             { type: string;  label: "Txn Geography";             group_label: "Txn Details";         sql: ${TABLE}.Spend_Place ;; }
  dimension: product                 { type: string;  label: "Product";                   group_label: "Product Info";        sql: ${TABLE}.Product ;; }
  dimension: spend_amount            { type: number;  label: "Spend Amount";              group_label: "Txn Amounts";         sql: ${TABLE}.Spend_Amount ;; }
  dimension: reversal_amount         { type: number;  label: "Reversal Amount";           group_label: "Txn Amounts";         sql: ${TABLE}.Reversal_Amount ;; }
  dimension: net_transaction         { type: number;  label: "Net Transaction";           group_label: "Txn Amounts";         sql: ${TABLE}.Net_Transaction ;; }
  dimension: spend_amount_cmb        { type: number;  label: "Spend Amount CMB";          group_label: "Txn Amounts";         sql: ${TABLE}.SPEND_AMOUNT_CMB ;; }
  dimension: spend_amount_rbmw       { type: number;  label: "Spend Amount RBMW";         group_label: "Txn Amounts";         sql: ${TABLE}.SPEND_AMOUNT_RBMW ;; }
  dimension: mt_ref_nbr              { type: string;  label: "Txn Reference No.";         group_label: "Txn Info";            sql: ${TABLE}.MT_REF_NBR ;; primary_key: yes }
  dimension: brand                   { type: string;  label: "Brand";                     group_label: "Merchant Info";       sql: ${TABLE}.BRAND ;; }
  dimension: final_category          { type: string;  label: "Final Category";            group_label: "Merchant Info";       sql: ${TABLE}.FINAL_CATEGORY ;; }
  dimension: category_general_desc   { type: string;  label: "Category General Desc";     group_label: "Merchant Info";       sql: ${TABLE}.CATEGORY_GENERAL_DESC ;; }
  dimension: category_desc           { type: string;  label: "Category Desc";             group_label: "Merchant Info";       sql: ${TABLE}.CATEGORY_DESC ;; }

  # === DERIVED DIMENSIONS ===
  dimension: merchant_name           { type: string;  label: "Merchant Name";             group_label: "Merchant Info";       sql: SUBSTRING(${TABLE}.Merchant_Details, 1, 24) ;; }
  dimension: merchant_city           { type: string;  label: "Merchant City";             group_label: "Merchant Info";       sql: REPLACE(SUBSTRING(${TABLE}.Merchant_Details, 24, 14), " ", "") ;; }
  dimension: merchant_country        { type: string;  label: "Merchant Country";          group_label: "Merchant Info";       sql: SUBSTRING(${TABLE}.Merchant_Details, 38, 2) ;; }
  dimension: mcc_combo               { type: string;  label: "MCC Combo";                 group_label: "Merchant Info";       sql: CONCAT(${mcc_code}, " - ", ${description}) ;; }

  # === DATE UTILITY DIMENSIONS ===
  dimension: today                   { type: date;    label: "Today's Date";              group_label: "Date Utilities";      sql: current_date() ;; }
  dimension: day                     { type: number;  label: "Current Day";               group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%e', current_date()) AS INT64) ;; }
  dimension: mon                     { type: number;  label: "Current Month";             group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%m', current_date()) AS INT64) ;; }
  dimension: yr                      { type: number;  label: "Current Year";              group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%Y', current_date()) AS INT64) ;; }
  dimension: mt_day                  { type: number;  label: "Txn Day";                   group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%e', ${mt_posting_date_date}) AS INT64) ;; }
  dimension: mt_mon                  { type: number;  label: "Txn Month";                 group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%m', ${mt_posting_date_date}) AS INT64) ;; }
  dimension: mt_yr                   { type: number;  label: "Txn Year";                  group_label: "Date Utilities";      sql: CAST(FORMAT_DATE('%Y', ${mt_posting_date_date}) AS INT64) ;; }

  # === PARAMETERIZED MEASURES (SWITCH LOGIC) ===
  parameter: metric_selector {
    label: "Metric Selector"
    group_label: "Switches"
    allowed_value: { label: "Net Spends in Cr" value: "net" }
    allowed_value: { label: "Gross Spends in Cr" value: "gross" }
  }
  measure: monthly_trend_metric {
    type: number
    label: "Monthly Trend Metric"
    group_label: "Switches"
    description: "Net or Gross Spends in Cr as per selector"
    sql:
      CASE
        WHEN {% parameter metric_selector %} = 'net' THEN ${total_net_transaction}
        WHEN {% parameter metric_selector %} = 'gross' THEN ${total_spend}
        ELSE NULL
      END
    ;;
    value_format: "#,##0.00"
  }
  parameter: wordcloud_metric_selector {
    label: "Word Cloud Metric"
    group_label: "Switches"
    allowed_value: { label: "Net Spends in Cr" value: "net" }
    allowed_value: { label: "Gross Spends in Cr" value: "gross" }
  }
  measure: wordcloud_metric {
    type: number
    label: "Word Cloud Metric"
    group_label: "Switches"
    sql:
      CASE
        WHEN {% parameter wordcloud_metric_selector %} = 'net' THEN ${total_net_transaction}
        WHEN {% parameter wordcloud_metric_selector %} = 'gross' THEN ${total_spend}
        ELSE NULL
      END
    ;;
    value_format: "#,##0.00"
  }

  # === MAIN MEASURES (ALL LOGIC) ===
  measure: total_spend                { type: sum;     group_label: "Core Spend"; label: "Gross Spends in Cr";        sql: ${spend_amount}/10000000 ;; value_format: "#,##0.00" }
  measure: total_reversal             { type: sum;     group_label: "Core Spend"; label: "Reversals in Cr";           sql: ${reversal_amount}/10000000 ;; value_format: "#,##0.00" }
  measure: total_net_transaction      { type: sum;     group_label: "Core Spend"; label: "Net Spends in Cr";          sql: ${net_transaction}/10000000 ;; value_format: "#,##0.00" }
  measure: average_spend              { type: average; group_label: "Core Spend"; label: "Avg Gross Spend/Txn";       sql: ${spend_amount} ;; value_format: "#,##0.00" }
  measure: average_net_transaction    { type: average; group_label: "Core Spend"; label: "Avg Net Spend/Txn";         sql: ${net_transaction} ;; value_format: "#,##0.00" }
  measure: transaction_count          { type: count;   group_label: "Core Spend"; label: "Txn Count" }
  measure: unique_accts_cnt           { type: count_distinct; group_label: "Core Spend"; label: "Unique Accounts";     sql: ${acct} ;; }

  # --- MTD MEASURES ---
  measure: total_net_transactions_mtd { type: sum;     group_label: "MTD"; label: "MTD Net Spends in Cr";             sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction}/10000000 ELSE NULL END ;; value_format: "#,##0.00" }
  measure: total_spends_mtd           { type: sum;     group_label: "MTD"; label: "MTD Gross Spends in Cr";           sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${spend_amount}/10000000 ELSE NULL END ;; value_format: "#,##0.00" }
  measure: total_reversal_mtd         { type: sum;     group_label: "MTD"; label: "MTD Reversals in Cr";              sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${reversal_amount}/10000000 ELSE NULL END ;; value_format: "#,##0.00" }
  measure: avg_net_transactions_mtd   { type: average; group_label: "MTD"; label: "MTD Avg Net Spend/Txn";            sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction} ELSE NULL END ;; value_format: "#,##0.00" }

  # --- BUSINESS/ADVANCED MEASURES ---
  measure: total_gross_spend          { type: sum;     group_label: "Business"; label: "Total Gross Spend";           sql: ${spend_amount} ;; value_format: "#,##0.00" }
  measure: total_net_spend            { type: sum;     group_label: "Business"; label: "Total Net Spend";             sql: ${net_transaction} ;; value_format: "#,##0.00" }
  measure: total_reversal_amount      { type: sum;     group_label: "Business"; label: "Total Reversal Amount";       sql: ${reversal_amount} ;; value_format: "#,##0.00" }
  measure: spend_by_brand             { type: sum;     group_label: "Business"; label: "Spend by Brand";              sql: CASE WHEN ${brand} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;; value_format: "#,##0.00" }
  measure: txn_count_by_brand         { type: count;   group_label: "Business"; label: "Txn Count by Brand";          filters: [brand: "-null"]; sql: ${mt_ref_nbr} ;; }
  measure: spend_by_txn_type          { type: sum;     group_label: "Business"; label: "Spend by Txn Type";           sql: CASE WHEN ${transaction_type} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;; value_format: "#,##0.00" }
  measure: spend_domestic             { type: sum;     group_label: "Business"; label: "Domestic Spend";              sql: CASE WHEN UPPER(${spend_place}) = 'DOM' THEN ${spend_amount} ELSE 0 END ;; value_format: "#,##0.00" }
  measure: spend_international        { type: sum;     group_label: "Business"; label: "International Spend";         sql: CASE WHEN UPPER(${spend_place}) = 'FR' THEN ${spend_amount} ELSE 0 END ;; value_format: "#,##0.00" }
  measure: avg_spend_per_account      { type: number;  group_label: "Business"; label: "Avg Spend per Account";       sql: CASE WHEN ${unique_accts_cnt} > 0 THEN ${total_gross_spend} / ${unique_accts_cnt} ELSE NULL END ;; value_format: "#,##0.00" }
  measure: total_transactions         { type: count_distinct; group_label: "Business"; label: "Unique Transactions";  sql: ${mt_ref_nbr} ;; }
}

















import pandas as pd
import re
from fuzzywuzzy import fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
import joblib
import os
import logging

# === CONFIGURE LOGGING ===
LOG_LEVEL = os.environ.get("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger(__name__)

# === MERCHANT DICTIONARY LOADER ===
def load_merchant_dictionary(path: str = "merchant_dictionary.csv"):
    """Load merchant dictionary from CSV (pipe-separated keywords)."""
    try:
        abs_path = os.path.abspath(path)
        df = pd.read_csv(abs_path, dtype=str, encoding="utf-8").fillna("")
        data = []
        for _, row in df.iterrows():
            entry = {
                "brand_keywords": [x.strip().lower() for x in row['brand_keywords'].split('|') if x.strip()],
                "merchant_name_keywords": [x.strip().lower() for x in row['merchant_name_keywords'].split('|') if x.strip()],
                "official_merchant_name": row.get('official_merchant_name', ''),
                "official_brand_name": row.get('official_brand_name', ''),
                "sector": row.get('sector', ''),
                "city": row.get('city', ''),
                "mcc": row.get('mcc', '')
            }
            if entry["brand_keywords"] or entry["merchant_name_keywords"]:
                data.append(entry)
        logger.info(f"Processed {len(data)} valid merchant dictionary entries.")
        return data
    except Exception as e:
        logger.error(f"Error loading merchant dictionary: {e}")
        return []

# === TEXT PREPROCESSOR & HEURISTICS ===
def preprocess(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'http\S+|www\.\S+', '', text)
    text = re.sub(r'\S*@\S*\s?', '', text)
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_entity_guess(raw_desc: str) -> str:
    if not raw_desc:
        return "Unknown Merchant"
    tokens = [w for w in raw_desc.split() if len(w) > 2 and any(c.isalpha() for c in w)]
    capitalized_phrases, curr = [], []
    for token in tokens:
        if token[0].isupper() or token.isupper():
            curr.append(token)
        elif curr:
            capitalized_phrases.append(' '.join(curr))
            curr = []
    if curr:
        capitalized_phrases.append(' '.join(curr))
    if capitalized_phrases:
        return max(capitalized_phrases, key=len)
    if len(tokens) >= 2:
        bigrams = [f'{tokens[i]} {tokens[i+1]}' for i in range(len(tokens)-1)]
        return max(bigrams, key=len) if bigrams else tokens[0]
    return tokens[0] if tokens else "Unknown Merchant"

# === DICTIONARY MATCHER ===
class DictionaryMatcher:
    def __init__(self, merchant_data, fuzzy_threshold=80):
        self.merchant_data = merchant_data
        self.fuzzy_threshold = fuzzy_threshold
        self.keyword_index = self._build_keyword_index()

    def _build_keyword_index(self):
        index = {}
        for entry in self.merchant_data:
            all_keywords = entry['brand_keywords'] + entry['merchant_name_keywords']
            for keyword in all_keywords:
                if keyword not in index:
                    index[keyword] = []
                index[keyword].append(entry)
        return index

    def find_match(self, processed_desc: str):
        if not processed_desc:
            return None
        best_match, best_score, match_type = None, 0, None
        for keyword, entries in self.keyword_index.items():
            if keyword in processed_desc:
                for entry in entries:
                    score = 99
                    if score > best_score:
                        best_score = score
                        best_match = entry
                        match_type = 'substring'
                        if best_score >= 98:
                            break
            if best_score >= 98:
                break
        if best_score < self.fuzzy_threshold:
            for entry in self.merchant_data:
                for keyword in entry['brand_keywords'] + entry['merchant_name_keywords']:
                    if not keyword:
                        continue
                    score_token = fuzz.token_set_ratio(processed_desc, keyword)
                    score_partial = fuzz.partial_ratio(processed_desc, keyword)
                    score = max(score_token, score_partial)
                    if score > best_score:
                        best_score = score
                        best_match = entry
                        match_type = 'token_set' if score == score_token else 'partial'
                    if best_score >= 98:
                        break
                if best_score >= 98:
                    break
        if best_match and best_score >= self.fuzzy_threshold:
            return {
                "brand": best_match["official_brand_name"],
                "merchant": best_match["official_merchant_name"],
                "sector": best_match["sector"],
                "confidence": "high" if best_score >= 90 else "medium"
            }
        return None

# === NLP SECTOR CLASSIFIER ===
class NLPSectorClassifier:
    def __init__(self, model_path="nlp_sector_model.joblib"):
        self.model_path = model_path
        self.pipeline = None

    def get_default_training_data(self):
        data = {
            'description': [
                "payment for groceries at local mart", "online order big general store", 
                "food delivery from quick bites", "monthly electricity bill payment",
                "cab ride with city movers", "ecom purchase fashion apparel",
                "swiggy bundl technologies", "amazon seller services online", 
                "zomato media pvt ltd", "tata cliq luxury shopping",
                "movie tickets pvr cinemas", "internet broadband connection act fibernet",
                "recharge mobile plan jio", "premium subscription netflix", 
                "dinner at urban restaurant", "flight booking indigo airlines",
                "pharmacy bill apollo pharmacy", "investment mutual fund groww",
                "utility gas bill payment adani gas", "transport metro card recharge",
                "retail clothing store westside", "education course fee udemy",
                "health checkup lal pathlabs", "donation to charity foundation",
                "upi transfer to friend", "atm withdrawal any bank",
                "interest credited savings account", "loan emi payment hdfc bank",
                "insurance premium lic india", "software purchase adobe creative",
                "payment to ABC solutions", "random tech services pvt ltd",
                "local kirana store purchase", "petrol pump fuel payment",
                "hospital medical treatment", "school fees payment",
                "gym membership renewal", "beauty salon services",
                "car repair garage", "book store purchase"
            ],
            'sector': [
                "Retail", "Retail", "Food Delivery", "Utilities", "Transport", "E-Commerce",
                "Food Delivery", "E-Commerce", "Food Delivery", "E-Commerce", "Entertainment", "Utilities",
                "Telecom", "Entertainment", "Dining", "Transport", "Healthcare", "Finance",
                "Utilities", "Transport", "Retail", "Education", "Healthcare", "Others",
                "Payments", "Banking", "Banking", "Finance", "Finance", "Software",
                "Services", "Technology", "Retail", "Fuel", "Healthcare", "Education",
                "Fitness", "Beauty", "Automotive", "Retail"
            ]
        }
        return pd.DataFrame(data)

    def train_model(self, training_data):
        training_data = training_data.copy()
        training_data['processed_desc'] = training_data['description'].apply(preprocess)
        training_data = training_data[training_data['processed_desc'].str.len() > 0]
        if len(training_data) == 0:
            raise ValueError("No valid training data after preprocessing")
        X = training_data['processed_desc']
        y = training_data['sector']
        self.pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(
                stop_words='english', ngram_range=(1, 2), max_df=0.95, min_df=2, max_features=5000)),
            ('clf', MultinomialNB(alpha=0.1))
        ])
        self.pipeline.fit(X, y)
        logger.info("NLP sector classifier trained (local)")

    def save_model(self):
        if self.pipeline and self.model_path:
            joblib.dump(self.pipeline, self.model_path)
            logger.info(f"NLP model saved to {self.model_path}")

    def load_model(self):
        try:
            if os.path.exists(self.model_path):
                self.pipeline = joblib.load(self.model_path)
                logger.info(f"NLP model loaded from {self.model_path}")
                return True
        except Exception as e:
            logger.error(f"Error loading NLP model: {e}")
        return False

    def predict_sector(self, processed_desc: str):
        if not self.pipeline or not processed_desc:
            return "Other"
        try:
            prediction = self.pipeline.predict([processed_desc])[0]
            return prediction
        except Exception as e:
            logger.error(f"NLP prediction error: {e}")
            return "Other"

# === MERCHANT EXTRACTOR (MAIN) ===
class MerchantExtractor:
    def __init__(self, merchant_data, model_path="nlp_sector_model.joblib",
                 retrain_nlp=False, new_nlp_data=None, fuzzy_threshold=80):
        self.merchant_data = merchant_data
        self.fuzzy_threshold = fuzzy_threshold
        self.dictionary_matcher = DictionaryMatcher(merchant_data, fuzzy_threshold)
        self.nlp_classifier = NLPSectorClassifier(model_path)
        self._initialize_nlp_model(retrain_nlp, new_nlp_data)

    def _initialize_nlp_model(self, retrain, training_data):
        if retrain and training_data is not None:
            logger.info("Retraining local NLP sector model...")
            self.nlp_classifier.train_model(training_data)
            self.nlp_classifier.save_model()
        elif self.nlp_classifier.load_model():
            logger.info("Loaded existing local NLP model.")
        else:
            logger.info("Training new NLP model with default sample data...")
            default_data = self.nlp_classifier.get_default_training_data()
            self.nlp_classifier.train_model(default_data)
            self.nlp_classifier.save_model()

    def extract(self, raw_description):
        if not isinstance(raw_description, str) or not raw_description.strip():
            return {
                "description": raw_description or "",
                "brand": "Invalid",
                "merchant": "Invalid",
                "sector": "Invalid",
                "confidence": "none"
            }
        processed_desc = preprocess(raw_description)
        dict_match = self.dictionary_matcher.find_match(processed_desc)
        if dict_match:
            return {
                "description": raw_description,
                "brand": dict_match["brand"],
                "merchant": dict_match["merchant"],
                "sector": dict_match["sector"],
                "confidence": dict_match["confidence"]
            }
        # NLP fallback
        predicted_sector = self.nlp_classifier.predict_sector(processed_desc)
        guessed_name = extract_entity_guess(raw_description)
        return {
            "description": raw_description,
            "brand": guessed_name,
            "merchant": guessed_name,
            "sector": predicted_sector,
            "confidence": "low"
        }

    def extract_batch(self, descriptions):
        return [self.extract(desc) for desc in descriptions]

# === RUN AS SCRIPT ===
if __name__ == '__main__':
    merchant_data = load_merchant_dictionary("merchant_dictionary.csv")
    if not merchant_data:
        logger.warning("No merchant data loaded, using minimal in-memory example")
        merchant_data = [{
            "brand_keywords": ["swiggy", "bundl"],
            "merchant_name_keywords": ["swiggy bundl technologies"],
            "official_merchant_name": "Bundl Technologies Pvt Ltd",
            "official_brand_name": "Swiggy",
            "sector": "Food Delivery",
            "city": "Bangalore",
            "mcc": "5812"
        }]
    extractor = MerchantExtractor(merchant_data, fuzzy_threshold=80)

    # Input descriptions
    descriptions = [
        "POS 987654321 SWIGGY BUNDL TECHNOLOGY BANGALORE IN",
        "ONLINE PAYMENT AMAZON PAY INDIA",
        "UPI 1234567890 KUMAR STATIONERY MART PUNE",
        "POS 1111 BIG BZR FUTURE RETAIL MUMBAI MH IN",
        "PAYMENT TO SRIRAM SWEETS AND BAKERY KOLKATA",
        "Random Cafe Shop Payment",
        "Strange merchant abcXyZ123"
    ]

    # Batch extraction and DataFrame/table output
    results = extractor.extract_batch(descriptions)
    df = pd.DataFrame(results, columns=["description", "brand", "merchant", "sector", "confidence"])
    print(df)

    # To save as CSV or Excel:
    # df.to_csv("merchant_extraction_output.csv", index=False)
    # df.to_excel("merchant_extraction_output.xlsx", index=False)







view: spend_view2 {
  sql_table_name: `your_project.your_dataset.SPEND_VIEW_2` ;;

  # ======================== DIMENSION GROUPS (DATES) =========================
  dimension_group: mt_posting_date {
    type: time
    timeframes: [raw, date, week, month, quarter, year]
    label: "Posting Date"
    group_label: "Transaction Dates"
    description: "Date the transaction was posted"
    sql: ${TABLE}.MT_POSTING_DATE ;;
  }

  # ======================== DIMENSIONS (SCHEMA FIELDS) =======================

  dimension: acct {
    type: string
    label: "Account Number"
    group_label: "Account Information"
    description: "Customer account number used for the transaction"
    sql: ${TABLE}.acct ;;
  }

  dimension: mcc_code {
    type: string
    label: "MCC Code"
    group_label: "Merchant Info"
    description: "Merchant Category Code (MCC)"
    sql: CAST(${TABLE}.MCC_CODE AS STRING) ;;
  }

  dimension: description {
    type: string
    label: "MCC Description"
    group_label: "Merchant Info"
    description: "Description of the MCC"
    sql: ${TABLE}.Description ;;
  }

  dimension: merchant_details {
    type: string
    label: "Merchant Details"
    group_label: "Merchant Info"
    description: "Raw merchant information from the transaction"
    sql: ${TABLE}.Merchant_Details ;;
  }

  dimension: org {
    type: number
    label: "Organization ID"
    group_label: "Card/Product Info"
    description: "Organization identifier"
    sql: ${TABLE}.ORG ;;
  }

  dimension: transaction_type {
    type: string
    label: "Transaction Type"
    group_label: "Transaction Details"
    description: "POS, Ecommerce, Cash, etc."
    sql: ${TABLE}.TRANSACTION_TYPE ;;
  }

  dimension: spend_type {
    type: string
    label: "Spend Type"
    group_label: "Transaction Details"
    description: "Type of spend (e.g., Debit, EMI, etc.)"
    sql: ${TABLE}.SPEND_TYPE ;;
  }

  dimension: segment_name {
    type: string
    label: "Segment Name"
    group_label: "Merchant Info"
    description: "Segment/category of the merchant"
    sql: ${TABLE}.Segment_Name ;;
  }

  dimension: mt_type {
    type: string
    label: "MT Type"
    group_label: "Card/Product Info"
    description: "Card type used (if available)"
    sql: ${TABLE}.mt_type ;;
  }

  dimension: spend_place {
    type: string
    label: "Transaction Geography"
    group_label: "Transaction Details"
    description: "DOM = Domestic, FR = International"
    sql: ${TABLE}.Spend_Place ;;
  }

  dimension: product {
    type: string
    label: "Product"
    group_label: "Card/Product Info"
    description: "Product or card type used"
    sql: ${TABLE}.Product ;;
  }

  dimension: spend_amount {
    type: number
    label: "Spend Amount"
    group_label: "Transaction Amounts"
    description: "Amount spent in the transaction"
    sql: ${TABLE}.Spend_Amount ;;
  }

  dimension: reversal_amount {
    type: number
    label: "Reversal Amount"
    group_label: "Transaction Amounts"
    description: "Amount reversed for the transaction"
    sql: ${TABLE}.Reversal_Amount ;;
  }

  dimension: net_transaction {
    type: number
    label: "Net Transaction"
    group_label: "Transaction Amounts"
    description: "Net spend after reversal"
    sql: ${TABLE}.Net_Transaction ;;
  }

  dimension: spend_amount_cmb {
    type: number
    label: "Spend Amount CMB"
    group_label: "Transaction Amounts"
    description: "Spend amount CMB"
    sql: ${TABLE}.SPEND_AMOUNT_CMB ;;
  }

  dimension: spend_amount_rbmw {
    type: number
    label: "Spend Amount RBMW"
    group_label: "Transaction Amounts"
    description: "Spend amount RBMW"
    sql: ${TABLE}.SPEND_AMOUNT_RBMW ;;
  }

  dimension: mt_ref_nbr {
    type: string
    label: "Transaction Reference Number"
    group_label: "Transaction Info"
    description: "Unique transaction reference number"
    sql: ${TABLE}.MT_REF_NBR ;;
    primary_key: yes
  }

  dimension: brand {
    type: string
    label: "Brand"
    group_label: "Merchant Info"
    description: "Brand associated with the transaction"
    sql: ${TABLE}.BRAND ;;
  }

  dimension: final_category {
    type: string
    label: "Final Category"
    group_label: "Merchant Info"
    description: "Final category of the transaction"
    sql: ${TABLE}.FINAL_CATEGORY ;;
  }

  dimension: category_general_desc {
    type: string
    label: "Category General Description"
    group_label: "Merchant Info"
    description: "General description of the merchant category"
    sql: ${TABLE}.CATEGORY_GENERAL_DESC ;;
  }

  dimension: category_desc {
    type: string
    label: "Category Description"
    group_label: "Merchant Info"
    description: "Detailed description of the merchant category"
    sql: ${TABLE}.CATEGORY_DESC ;;
  }

  # ======================== DERIVED DIMENSIONS ===============================

  dimension: merchant_name {
    type: string
    label: "Merchant Name"
    group_label: "Merchant Info"
    description: "Parsed merchant name from merchant_details"
    sql: SUBSTRING(${TABLE}.Merchant_Details, 1, 24) ;;
  }

  dimension: merchant_city {
    type: string
    label: "Merchant City"
    group_label: "Merchant Info"
    description: "Parsed merchant city from merchant_details"
    sql: REPLACE(SUBSTRING(${TABLE}.Merchant_Details, 24, 14), " ", "") ;;
  }

  dimension: merchant_country {
    type: string
    label: "Merchant Country"
    group_label: "Merchant Info"
    description: "Parsed merchant country from merchant_details"
    sql: SUBSTRING(${TABLE}.Merchant_Details, 38, 2) ;;
  }

  dimension: mcc_combo {
    type: string
    label: "MCC Combo"
    group_label: "Merchant Info"
    description: "MCC code and description combo"
    sql: CONCAT(${mcc_code}, " - ", ${description}) ;;
  }

  # ======================== DATE UTILITIES ===================================

  dimension: today {
    type: date
    label: "Today's Date"
    group_label: "Date Utilities"
    description: "Current system date"
    sql: current_date() ;;
  }

  dimension: day {
    type: number
    label: "Current Day"
    group_label: "Date Utilities"
    description: "Today's day of the month"
    sql: CAST(FORMAT_DATE('%e', current_date()) AS INT64) ;;
  }

  dimension: mon {
    type: number
    label: "Current Month"
    group_label: "Date Utilities"
    description: "Current month"
    sql: CAST(FORMAT_DATE('%m', current_date()) AS INT64) ;;
  }

  dimension: yr {
    type: number
    label: "Current Year"
    group_label: "Date Utilities"
    description: "Current year"
    sql: CAST(FORMAT_DATE('%Y', current_date()) AS INT64) ;;
  }

  dimension: mt_day {
    type: number
    label: "Transaction Day"
    group_label: "Date Utilities"
    description: "Transaction's day of month"
    sql: CAST(FORMAT_DATE('%e', ${mt_posting_date}) AS INT64) ;;
  }

  dimension: mt_mon {
    type: number
    label: "Transaction Month"
    group_label: "Date Utilities"
    description: "Transaction's month"
    sql: CAST(FORMAT_DATE('%m', ${mt_posting_date}) AS INT64) ;;
  }

  dimension: mt_yr {
    type: number
    label: "Transaction Year"
    group_label: "Date Utilities"
    description: "Transaction's year"
    sql: CAST(FORMAT_DATE('%Y', ${mt_posting_date}) AS INT64) ;;
  }

  # ======================== ORIGINAL MEASURES ================================

  measure: total_spend {
    type: sum
    group_label: "Core Spend Metrics"
    label: "Gross Spends in Cr"
    description: "Gross spends in crore"
    sql: ${spend_amount}/10000000 ;;
    value_format: "#,##0.00"
  }

  measure: total_reversal {
    type: sum
    group_label: "Core Spend Metrics"
    sql: ${reversal_amount}/10000000;;
    label: "Reversals in Cr"
    description: "Reversals in crore"
    value_format: "#,##0.00"
  }

  measure: total_net_transaction {
    type: sum
    group_label: "Core Spend Metrics"
    label: "Net Spends in Cr"
    description: "Net spends in crore"
    sql: ${net_transaction}/10000000 ;;
    value_format: "#,##0.00"
  }

  measure: average_spend {
    type: average
    group_label: "Core Spend Metrics"
    sql: ${spend_amount};;
    label: "Average Gross Spend per Transaction"
    value_format: "#,##0.00"
  }

  measure: average_net_transaction {
    type: average
    group_label: "Core Spend Metrics"
    label: "Average Net Spend per Transaction"
    sql: ${net_transaction};;
    value_format: "#,##0.00"
  }

  measure: transaction_count {
    label: "Count of Transactions"
    group_label: "Core Spend Metrics"
    type: count
  }

  measure: unique_accts_cnt {
    type: count_distinct
    group_label: "Core Spend Metrics"
    label: "Unique Account Count"
    sql: ${acct} ;;
  }

  # ======================== MTD MEASURES =====================================

  measure: total_net_transactions_mtd {
    type: sum
    group_label: "MTD Metrics"
    label: "MTD Net Spends in Cr"
    description: "Month to date net spends in crore"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }

  measure: total_spends_mtd {
    type: sum
    group_label: "MTD Metrics"
    label: "MTD Gross Spends in Cr"
    description: "Month to date gross spends in crore"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${spend_amount}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }

  measure: total_reversal_mtd {
    type: sum
    group_label: "MTD Metrics"
    label: "MTD Reversals in Cr"
    description: "Month to date reversals in crore"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${reversal_amount}/10000000 ELSE NULL END ;;
    value_format: "#,##0.00"
  }

  measure: avg_net_transactions_mtd {
    type: average
    group_label: "MTD Metrics"
    label: "MTD Avg Net Spend per Transaction"
    description: "Month to date average net spend per transaction"
    sql: CASE WHEN ${mt_day} <= ${day}-1 THEN ${net_transaction} ELSE NULL END ;;
    value_format: "#,##0.00"
  }

  # =================== BUSINESS BREAKDOWN/NEW MEASURES =======================

  measure: total_gross_spend {
    type: sum
    group_label: "Business Breakdown"
    label: "Total Gross Spend"
    sql: ${spend_amount} ;;
    value_format_name: "decimal_2"
    description: "Sum of spend amount"
  }

  measure: total_net_spend {
    type: sum
    group_label: "Business Breakdown"
    label: "Total Net Spend"
    sql: ${net_transaction} ;;
    value_format_name: "decimal_2"
    description: "Sum of net transaction"
  }

  measure: total_reversal_amount {
    type: sum
    group_label: "Business Breakdown"
    label: "Total Reversal Amount"
    sql: ${reversal_amount} ;;
    value_format_name: "decimal_2"
  }

  measure: spend_by_brand {
    type: sum
    group_label: "Business Breakdown"
    label: "Spend by Brand"
    sql: CASE WHEN ${brand} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Total spend for each brand"
  }

  measure: txn_count_by_brand {
    type: count
    group_label: "Business Breakdown"
    label: "Txn Count by Brand"
    filters: [brand: "-null"]
    sql: ${mt_ref_nbr} ;;
  }

  measure: spend_by_txn_type {
    type: sum
    group_label: "Business Breakdown"
    label: "Spend by Transaction Type"
    sql: CASE WHEN ${transaction_type} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Total spend by transaction type"
  }

  measure: spend_domestic {
    type: sum
    group_label: "Business Breakdown"
    label: "Domestic Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'DOM' THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Domestic spends only"
  }

  measure: spend_international {
    type: sum
    group_label: "Business Breakdown"
    label: "International Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'FR' THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "International spends only"
  }

  measure: avg_spend_per_account {
    type: number
    group_label: "Business Breakdown"
    label: "Avg Spend per Account"
    sql: CASE WHEN ${unique_accts_cnt} > 0 THEN ${total_gross_spend} / ${unique_accts_cnt} ELSE NULL END ;;
    value_format_name: "decimal_2"
  }

  measure: total_transactions {
    type: count_distinct
    group_label: "Business Breakdown"
    label: "Unique Transactions"
    sql: ${mt_ref_nbr} ;;
    description: "Number of unique transactions"
  }

  # (Optional - Only if you have a Customer_ID field)
  # measure: unique_customers_cnt {
  #   type: count_distinct
  #   group_label: "Business Breakdown"
  #   label: "Unique Customer Count"
  #   sql: ${TABLE}.Customer_ID ;;
  #   description: "Number of unique customers"
  # }
}












view: spend_view_new {
  sql_table_name: `your_project.your_dataset.SPEND_VIEW_*` ;;

  # ------------------- DIMENSIONS -------------------

  dimension: mt_posting_date {
    type: date
    label: "Posting Date"
    sql: ${TABLE}.MT_POSTING_DATE ;;
  }

  dimension: mcc_code {
    type: string
    label: "MCC Code"
    sql: CAST(${TABLE}.MCC_CODE AS STRING) ;;
  }

  dimension: merchant_details {
    type: string
    label: "Merchant Details"
    sql: ${TABLE}.Merchant_Details ;;
  }

  dimension: description {
    type: string
    label: "Description"
    sql: ${TABLE}.Description ;;
  }

  dimension: org {
    type: number
    label: "ORG"
    sql: ${TABLE}.ORG ;;
  }

  dimension: transaction_type {
    type: string
    label: "Transaction Type"
    sql: ${TABLE}.TRANSACTION_TYPE ;;
    description: "Type of transaction: POS, Ecommerce, Cash, etc."
  }

  dimension: spend_type {
    type: string
    label: "Spend Type"
    sql: ${TABLE}.SPEND_TYPE ;;
    description: "Debit/Credit, EMI, etc."
  }

  dimension: segment_name {
    type: string
    label: "Segment Name"
    sql: ${TABLE}.Segment_Name ;;
  }

  dimension: acct {
    type: string
    label: "Account Number"
    sql: ${TABLE}.acct ;;
  }

  dimension: mt_type {
    type: string
    label: "MT Type"
    sql: ${TABLE}.mt_type ;;
    description: "Card type (if available)"
  }

  dimension: spend_place {
    type: string
    label: "Spend Place"
    sql: ${TABLE}.Spend_Place ;;
    description: "DOM = Domestic, FR = International"
  }

  dimension: product {
    type: string
    label: "Product"
    sql: ${TABLE}.Product ;;
    description: "Card/Account product"
  }

  dimension: spend_amount {
    type: number
    label: "Spend Amount"
    sql: ${TABLE}.Spend_Amount ;;
  }

  dimension: reversal_amount {
    type: number
    label: "Reversal Amount"
    sql: ${TABLE}.Reversal_Amount ;;
  }

  dimension: net_transaction {
    type: number
    label: "Net Transaction"
    sql: ${TABLE}.Net_Transaction ;;
  }

  dimension: spend_amount_cmb {
    type: number
    label: "Spend Amount CMB"
    sql: ${TABLE}.SPEND_AMOUNT_CMB ;;
  }

  dimension: spend_amount_rbmw {
    type: number
    label: "Spend Amount RBMW"
    sql: ${TABLE}.SPEND_AMOUNT_RBMW ;;
  }

  dimension: mt_ref_nbr {
    type: string
    label: "MT Reference Number"
    sql: ${TABLE}.MT_REF_NBR ;;
    description: "Unique transaction reference"
    primary_key: yes
  }

  dimension: brand {
    type: string
    label: "Brand"
    sql: ${TABLE}.BRAND ;;
  }

  dimension: final_category {
    type: string
    label: "Final Category"
    sql: ${TABLE}.FINAL_CATEGORY ;;
  }

  dimension: category_general_desc {
    type: string
    label: "Category General Description"
    sql: ${TABLE}.CATEGORY_GENERAL_DESC ;;
  }

  dimension: category_desc {
    type: string
    label: "Category Description"
    sql: ${TABLE}.CATEGORY_DESC ;;
  }

  # ------------------- MEASURES -------------------

  # Total spend amount (Gross)
  measure: total_gross_spend {
    type: sum
    label: "Total Gross Spend"
    sql: ${spend_amount} ;;
    value_format_name: "decimal_2"
    description: "Sum of spend amount"
  }

  # Total net spend (after reversal)
  measure: total_net_spend {
    type: sum
    label: "Total Net Spend"
    sql: ${net_transaction} ;;
    value_format_name: "decimal_2"
    description: "Sum of net transaction"
  }

  measure: total_reversal_amount {
    type: sum
    label: "Total Reversal Amount"
    sql: ${reversal_amount} ;;
    value_format_name: "decimal_2"
  }

  measure: unique_accts_cnt {
    type: count_distinct
    label: "Unique Accounts Count"
    sql: ${acct} ;;
    description: "Number of unique accounts"
  }

  measure: unique_customers_cnt {
    type: count_distinct
    label: "Unique Customer Count"
    sql: ${TABLE}.Customer_ID ;; # Replace with correct field if you have Customer_ID, otherwise remove
    description: "Number of unique customers"
  }

  # Spend by brand (filtered measure: example for dashboarding)
  measure: spend_by_brand {
    type: sum
    label: "Spend by Brand"
    sql: CASE WHEN ${brand} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Total spend for each brand"
  }

  # Count of transactions by brand
  measure: txn_count_by_brand {
    type: count
    label: "Txn Count by Brand"
    filters: [brand: "-null"]
    sql: ${mt_ref_nbr} ;;
  }

  # Spend by transaction type (POS, ECOM, CASH, etc.)
  measure: spend_by_txn_type {
    type: sum
    label: "Spend by Transaction Type"
    sql: CASE WHEN ${transaction_type} IS NOT NULL THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Total spend by transaction type"
  }

  # Spend by geography (Domestic/International)
  measure: spend_domestic {
    type: sum
    label: "Domestic Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'DOM' THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "Domestic spends only"
  }

  measure: spend_international {
    type: sum
    label: "International Spend"
    sql: CASE WHEN UPPER(${spend_place}) = 'FR' THEN ${spend_amount} ELSE 0 END ;;
    value_format_name: "decimal_2"
    description: "International spends only"
  }

  # Average spend per account
  measure: avg_spend_per_account {
    type: number
    label: "Average Spend per Account"
    sql: CASE WHEN ${unique_accts_cnt} > 0 THEN ${total_gross_spend} / ${unique_accts_cnt} ELSE NULL END ;;
    value_format_name: "decimal_2"
  }

  # Transaction count
  measure: total_transactions {
    type: count_distinct
    label: "Unique Transactions"
    sql: ${mt_ref_nbr} ;;
    description: "Number of unique transactions"
  }
}





















view: spend_view2 {
  sql_table_name: `your_project.your_dataset.SPEND_VIEW_2` ;;

  # --- Dimensions ---

  dimension: mt_posting_date {
    type: date
    label: "Posting Date"
    sql: ${TABLE}.MT_POSTING_DATE ;;
  }

  dimension: mcc_code {
    type: number
    label: "MCC Code"
    sql: ${TABLE}.MCC_CODE ;;
  }

  dimension: merchant_details {
    type: string
    label: "Merchant Details"
    sql: ${TABLE}.Merchant_Details ;;
  }

  dimension: description {
    type: string
    label: "Description"
    sql: ${TABLE}.Description ;;
  }

  dimension: org {
    type: number
    label: "ORG"
    sql: ${TABLE}.ORG ;;
  }

  dimension: transaction_type {
    type: string
    label: "Transaction Type"
    sql: ${TABLE}.TRANSACTION_TYPE ;;
  }

  dimension: spend_type {
    type: string
    label: "Spend Type"
    sql: ${TABLE}.SPEND_TYPE ;;
  }

  dimension: segment_name {
    type: string
    label: "Segment Name"
    sql: ${TABLE}.Segment_Name ;;
  }

  dimension: acct {
    type: string
    label: "Account Number"
    sql: ${TABLE}.acct ;;
  }

  dimension: mt_type {
    type: string
    label: "MT Type"
    sql: ${TABLE}.mt_type ;;
  }

  dimension: spend_place {
    type: string
    label: "Spend Place"
    sql: ${TABLE}.Spend_Place ;;
  }

  dimension: product {
    type: string
    label: "Product"
    sql: ${TABLE}.Product ;;
  }

  dimension: mt_ref_nbr {
    type: string
    label: "MT Reference Number"
    sql: ${TABLE}.MT_REF_NBR ;;
    primary_key: yes
  }

  dimension: brand {
    type: string
    label: "Brand"
    sql: ${TABLE}.BRAND ;;
  }

  dimension: final_category {
    type: string
    label: "Final Category"
    sql: ${TABLE}.FINAL_CATEGORY ;;
  }

  dimension: category_general_desc {
    type: string
    label: "Category General Description"
    sql: ${TABLE}.CATEGORY_GENERAL_DESC ;;
  }

  dimension: category_desc {
    type: string
    label: "Category Description"
    sql: ${TABLE}.CATEGORY_DESC ;;
  }

  # --- Measures: Spend Analytics ---

  measure: total_gross_spend {
    type: sum
    sql: ${TABLE}.Spend_Amount ;;
    label: "Total Gross Spend"
    value_format_name: "decimal_2"
    description: "Sum of Spend_Amount (Gross Spend)"
  }

  measure: total_reversal_amount {
    type: sum
    sql: ${TABLE}.Reversal_Amount ;;
    label: "Total Reversal Amount"
    value_format_name: "decimal_2"
    description: "Sum of Reversal_Amount"
  }

  measure: net_spend {
    type: number
    sql: ${total_gross_spend} - ${total_reversal_amount} ;;
    label: "Net Spend"
    value_format_name: "decimal_2"
    description: "Gross Spend minus Reversal Amount"
  }

  measure: avg_gross_spend {
    type: average
    sql: ${TABLE}.Spend_Amount ;;
    label: "Average Gross Spend"
    value_format_name: "decimal_2"
    description: "Average gross spend per transaction"
  }

  measure: avg_net_spend {
    type: number
    sql: CASE WHEN COUNT(${mt_ref_nbr}) > 0 THEN (${net_spend}) / COUNT(${mt_ref_nbr}) ELSE NULL END ;;
    label: "Average Net Spend"
    value_format_name: "decimal_2"
    description: "Net Spend divided by count of transactions"
  }

  measure: count_transactions {
    type: count_distinct
    sql: ${TABLE}.MT_REF_NBR ;;
    label: "Transaction Count"
    description: "Distinct number of transactions"
  }

  measure: total_net_transaction {
    type: sum
    sql: ${TABLE}.Net_Transaction ;;
    label: "Net Transaction (DB column)"
    value_format_name: "decimal_2"
    description: "Sum of Net_Transaction column"
  }

  measure: total_spend_amount_cmb {
    type: sum
    sql: ${TABLE}.SPEND_AMOUNT_CMB ;;
    label: "Spend Amount CMB"
    value_format_name: "decimal_2"
    description: "Sum of SPEND_AMOUNT_CMB"
  }

  measure: total_spend_amount_rbmw {
    type: sum
    sql: ${TABLE}.SPEND_AMOUNT_RBMW ;;
    label: "Spend Amount RBMW"
    value_format_name: "decimal_2"
    description: "Sum of SPEND_AMOUNT_RBMW"
  }
}
