import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Read the data from the CSV file
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create a pivot table to get transaction frequencies per sector
sector_frequency = df.pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

# Create pivot tables for weekday and weekend transaction frequencies per sector
sector_frequency_weekday = df[df['Day_Type'] == 'Weekday'].pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

sector_frequency_weekend = df[df['Day_Type'] == 'Weekend'].pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

# Function for Cosine Similarity
def calculate_cosine_similarity(data):
    similarity_matrix = cosine_similarity(data)
    return similarity_matrix

# Function to find the dominant sector based on cosine similarity
def find_dominant_sector(data, customer_index):
    similarities = calculate_cosine_similarity(data.values)
    similarities_series = pd.Series(similarities[customer_index], index=data.index)
    most_similar_customer = similarities_series.idxmax()
    dominant_sector = data.loc[[most_similar_customer]].idxmax(axis=1).values[0]
    return dominant_sector

# Function to find the sector with the highest transaction frequency
def find_sector_with_highest_transactions(data, customer_index):
    return data.iloc[customer_index].idxmax()

# Define a function to identify the dominant persona labels for a customer
def identify_persona_and_transactions(customer_id):
    customer_id = int(customer_id) if str(customer_id).isdigit() else customer_id
    customer_index = list(sector_frequency.index).index(customer_id)

    # Overall dominant sector based on cosine similarity
    dominant_sector_overall = find_dominant_sector(sector_frequency, customer_index)
    # Sector with the highest transaction frequency overall
    highest_transactions_overall = find_sector_with_highest_transactions(sector_frequency, customer_index)

    # Weekday dominant sector based on cosine similarity
    dominant_sector_weekday = find_dominant_sector(sector_frequency_weekday, customer_index)
    # Weekday sector with the highest transaction frequency
    highest_transactions_weekday = find_sector_with_highest_transactions(sector_frequency_weekday, customer_index)

    # Weekend dominant sector based on cosine similarity
    dominant_sector_weekend = find_dominant_sector(sector_frequency_weekend, customer_index)
    # Weekend sector with the highest transaction frequency
    highest_transactions_weekend = find_sector_with_highest_transactions(sector_frequency_weekend, customer_index)

    return (dominant_sector_overall, highest_transactions_overall, 
            dominant_sector_weekday, highest_transactions_weekday, 
            dominant_sector_weekend, highest_transactions_weekend)

# Example usage
customer_id = "12345"  # Ensure this matches an ACCT value in your CSV
results = identify_persona_and_transactions(customer_id)
print(f"Customer {customer_id} Overall Dominant Sector (Cosine Similarity): {results[0]}, Highest Transactions: {results[1]}")
print(f"Customer {customer_id} Weekday Dominant Sector (Cosine Similarity): {results[2]}, Highest Transactions: {results[3]}")
print(f"Customer {customer_id} Weekend Dominant Sector (Cosine Similarity): {results[4]}, Highest Transactions: {results[5]}")









import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Step 1: Load and pivot the transaction data
df = pd.read_csv('path/to/your/transactions.csv')  # Make sure to adjust the path to your CSV file
df_pivoted = df.pivot_table(index='CustomerID', columns='Sector', values='Amount', aggfunc='sum', fill_value=0)

# Step 2: Compute the cosine similarity matrix
similarity_matrix = cosine_similarity(df_pivoted)
np.fill_diagonal(similarity_matrix, 0)  # Ignore self-similarity by setting diagonal to 0

# Convert the similarity matrix to a DataFrame for easier manipulation
similarity_df = pd.DataFrame(similarity_matrix, index=df_pivoted.index, columns=df_pivoted.index)

# Step 3: Define a function to infer persona based on the most similar customers
def infer_persona(customer_id, top_n=3):
    # Get top N similar customers for the target customer
    top_neighbors = similarity_df.loc[customer_id].sort_values(ascending=False).head(top_n).index
    
    # Aggregate the spending of these neighbors to infer dominant sector
    neighbors_spending = df_pivoted.loc[top_neighbors].sum().idxmax()
    
    # Mapping from sector to persona (customize as needed)
    sector_to_persona = {
        'Entertainment': 'Entertainment Enthusiast',
        'Shopping': 'Shopping Addict',
        'Travel': 'Travel Buff',
        'Restaurant': 'Restaurant Regular',
        'Groceries': 'Grocery Guru'
    }
    
    return sector_to_persona.get(neighbors_spending, 'Other')

# Example usage
# Replace 'some_customer_id' with an actual CustomerID from your dataset
some_customer_id = df_pivoted.index[0]  # Example: Use the first CustomerID in the pivoted DataFrame
persona = infer_persona(some_customer_id)
print(f"Customer {some_customer_id} inferred persona: {persona}")












import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Read the data from the CSV file
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create a pivot table to get transaction frequencies per sector
sector_frequency = df.pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

# Create pivot tables for weekday and weekend transaction frequencies per sector
sector_frequency_weekday = df[df['Day_Type'] == 'Weekday'].pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

sector_frequency_weekend = df[df['Day_Type'] == 'Weekend'].pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

# Function for Cosine Similarity
def calculate_cosine_similarity(data):
    similarity_matrix = cosine_similarity(data)
    return similarity_matrix

# Function to find the dominant sector
def find_dominant_sector(data, customer_index):
    similarities = calculate_cosine_similarity(data.values)
    similarities_series = pd.Series(similarities[customer_index], index=data.index)
    most_similar_customer = similarities_series.idxmax()
    dominant_sector = data.loc[[most_similar_customer]].idxmax(axis=1).values[0]
    return dominant_sector

# Define a function to identify the dominant persona labels for a customer
def identify_persona(customer_id):
    customer_id = int(customer_id) if str(customer_id).isdigit() else customer_id
    customer_index = list(sector_frequency.index).index(customer_id)

    # Overall dominant sector
    dominant_sector_overall = find_dominant_sector(sector_frequency, customer_index)

    # Weekday dominant sector
    dominant_sector_weekday = find_dominant_sector(sector_frequency_weekday, customer_index)

    # Weekend dominant sector
    dominant_sector_weekend = find_dominant_sector(sector_frequency_weekend, customer_index)

    return dominant_sector_overall, dominant_sector_weekday, dominant_sector_weekend

# Example usage
customer_id = "12345"  # Ensure this matches an ACCT value in your CSV
overall, weekday, weekend = identify_persona(customer_id)
print(f"Customer {customer_id} Overall Dominant Sector: {overall}")
print(f"Customer {customer_id} Weekday Dominant Sector: {weekday}")
print(f"Customer {customer_id} Weekend Dominant Sector: {weekend}")














import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Read the data from the CSV file
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create a pivot table to get transaction frequencies per sector and day type
sector_frequency_by_day = df.pivot_table(
    index="ACCT",
    columns=["Sector", "Day_Type"],
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

# Function for Cosine Similarity
def calculate_cosine_similarity(data):
    similarity_matrix = cosine_similarity(data)
    return similarity_matrix

# Define a function to identify the dominant persona label for a customer considering day type
def identify_persona(customer_id, day_type, similarity_method="cosine"):
    # Ensure customer_id is correctly formatted for indexing
    customer_id = int(customer_id) if str(customer_id).isdigit() else customer_id
    
    if similarity_method == 'cosine':
        # Filter the pivot table for the specific day_type ('Weekday' or 'Weekend')
        filtered_data = sector_frequency_by_day.xs(day_type, level='Day_Type', axis=1)
        similarity_matrix = calculate_cosine_similarity(filtered_data.values)
    else:
        raise ValueError("Invalid similarity method")
    
    # Convert filtered_data.index (ACCT) to a list and find the index of the customer_id
    customer_index = list(filtered_data.index).index(customer_id)
    
    # Get similarity scores for the specified customer_id
    similarities = similarity_matrix[customer_index]
    
    # Convert similarities to a Series to utilize idxmax()
    similarities_series = pd.Series(similarities, index=filtered_data.index)
    
    # Find the most similar customer
    most_similar_customer = similarities_series.idxmax()
    
    # Determine the dominant sector for the most similar customer
    dominant_sector = filtered_data.loc[[most_similar_customer]].idxmax(axis=1).values[0]
    
    return dominant_sector

# Example usage
customer_id = "12345"  # Ensure this matches an ACCT value in your CSV
day_type = "Weekend"  # Choose 'Weekday' or 'Weekend'
persona_label = identify_persona(customer_id, day_type)
print(f"Customer {customer_id} Persona (Cosine Similarity) for {day_type}: {persona_label}")import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Load your data
data = pd.read_csv("transaction_data.csv")

# Feature engineering
data['date'] = pd.to_datetime(data['date'])
data['month'] = data['date'].dt.month

# Aggregate by customer AND sector
customer_data = data.groupby(['Acct', 'Sector']).agg(tran_amt_sum=('tran_amt', 'sum'))

# Pivot to get sectors as columns
customer_data = customer_data.unstack().fillna(0)
customer_data.columns = customer_data.columns.droplevel(0) 

# Reset the index to make 'Acct' a regular column
customer_data = customer_data.reset_index() 

# Select features (now the sector columns)
features = customer_data[['Entertainment', 'Travel', 'Shopping', 'Groceries', 'Restaurant']]
features['age_num'] = customer_data['age_num']  # Include 'age_num'

# Outlier Detection (Choose one technique)
scaler = StandardScaler()

# Technique 1: Z-Score Thresholding
z_scores = np.abs(scaler.fit_transform(features))
customer_data = customer_data[(z_scores < 3).all(axis=1)] 
features = customer_data[['Entertainment', 'Travel', 'Shopping', 'Groceries', 'Restaurant', 'age_num']]

# Technique 2: IQR-based
# Q1 = features.quantile(0.25)
# Q3 = features.quantile(0.75)
# IQR = Q3 - Q1
# customer_data = customer_data[~((features < (Q1 - 1.5 * IQR)) | (features > (Q3 + 1.5 * IQR))).any(axis=1)]
# ... (Re-select features if using IQR technique)


# Data Scaling (after outlier handling)
scaled_features = scaler.fit_transform(features) 

# Clustering and Validation
cluster_range = range(2, 8)
results = []

for num_clusters in cluster_range:
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(scaled_features)

    silhouette_avg = silhouette_score(scaled_features, cluster_labels)  
    results.append((num_clusters, silhouette_avg))

best_result = max(results, key=lambda x: x[1])
num_clusters = best_result[0]

# Perform Clustering with the best number of clusters
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
clusters = kmeans.fit_predict(scaled_features)

# Add cluster labels back to the customer data
customer_data['Cluster'] = clusters
print(customer_data)
