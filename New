import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Load data
data = pd.read_csv("customer_spending.csv")
features = data[['Entertainment', 'Travel', 'Shopping', 'Groceries', 'Restaurant']]

# Outlier Detection (Choose one technique)
scaler = StandardScaler()

# Technique 1: Z-Score Thresholding
z_scores = np.abs(scaler.fit_transform(features))
data = data[(z_scores < 3).all(axis=1)]  # Remove points with Z-score > 3 in any feature

# Technique 2: IQR-based
Q1 = features.quantile(0.25)
Q3 = features.quantile(0.75)
IQR = Q3 - Q1
data = data[~((features < (Q1 - 1.5 * IQR)) | (features > (Q3 + 1.5 * IQR))).any(axis=1)]

# Data Scaling (after outlier handling)
scaled_features = scaler.fit_transform(data) 

# Clustering and Validation
cluster_range = range(2, 8)  # Test clusters from  2 to 7
results = []

for num_clusters in cluster_range:
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(scaled_features)

    # Calculate silhouette score for validation
    silhouette_avg = silhouette_score(scaled_features, cluster_labels)  
    results.append((num_clusters, silhouette_avg))

# Select the best number of clusters based on silhouette score
best_result = max(results, key=lambda x: x[1])
num_clusters = best_result[0]

# Perform Clustering with the best number of clusters
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
clusters = kmeans.fit_predict(scaled_features)
data['Cluster'] = clusters

print(data)















import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import svds

# Read the data from the CSV file
df = pd.read_csv("IF_MAIN6.csv")

# Create a pivot table to get transaction frequencies per sector
sector_frequency = df.pivot_table(
    index = "ACCT",
    columns = "Sector",
    values = "TRAN_AMT",  
    aggfunc = 'count', 
    fill_value = 0       
)

# Function for Cosine Similarity
def calculate_cosine_similarity(data):
    similarity_matrix = cosine_similarity(data)
    return similarity_matrix

# Function for Jaccard similarity 
def calculate_jaccard_similarity(data):
    data_matrix = csr_matrix(data)  # Convert to a sparse matrix for efficiency
    similarity_matrix = 1 - svds(data_matrix, k=10)[1]  # Efficient for large datasets
    return similarity_matrix

# Define a function to identify the dominant persona label for a customer
def identify_persona(customer_id, similarity_method="cosine"):
    if similarity_method == 'cosine':
        similarity_matrix = calculate_cosine_similarity(sector_frequency)
    elif similarity_method == 'jaccard':
        similarity_matrix = calculate_jaccard_similarity(sector_frequency)
    else:
        raise ValueError("Invalid similarity method")

    similarities = similarity_matrix[sector_frequency.index == customer_id].iloc[0]
    most_similar_customer = sector_frequency.index[similarities.idxmax()]
    dominant_sector = sector_frequency.loc[most_similar_customer].idxmax() 
    return dominant_sector

# Example usage (choose your preferred similarity method)
customer_id = "12345" 
persona_label = identify_persona(customer_id, similarity_method="cosine")
print(f"Customer {customer_id} Persona (Cosine Similarity): {persona_label}") 

persona_label = identify_persona(customer_id, similarity_method="jaccard")
print(f"Customer {customer_id} Persona (Jaccard Similarity): {persona_label}") 




import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import svds
import numpy as np

# Read the data from the CSV file
df = pd.read_csv("IF_MAIN6.csv")

# Create a pivot table to get transaction frequencies per sector
sector_frequency = df.pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

# Function for Cosine Similarity
def calculate_cosine_similarity(data):
    similarity_matrix = cosine_similarity(data)
    return similarity_matrix

# Function for Jaccard similarity 
def calculate_jaccard_similarity(data):
    data_matrix = csr_matrix(data)  # Convert to a sparse matrix for efficiency
    similarity_matrix = 1 - svds(data_matrix, k=10)[1]  # Efficient for large datasets
    return similarity_matrix

# Define a function to identify the dominant persona label for a customer
def identify_persona(customer_id, similarity_method="cosine"):
    # Ensure customer_id is an integer for indexing if it's a numeric type
    customer_id = int(customer_id) if customer_id.isdigit() else customer_id
    
    if similarity_method == 'cosine':
        similarity_matrix = calculate_cosine_similarity(sector_frequency.values)
    elif similarity_method == 'jaccard':
        similarity_matrix = calculate_jaccard_similarity(sector_frequency.values)
    else:
        raise ValueError("Invalid similarity method")
    
    # Convert sector_frequency.index (ACCT) to a list and find the index of the customer_id
    customer_index = list(sector_frequency.index).index(customer_id)
    
    # Get similarity scores for the specified customer_id
    similarities = similarity_matrix[customer_index]
    
    # Convert similarities to a Series to utilize idxmax()
    similarities_series = pd.Series(similarities, index=sector_frequency.index)
    
    # Find the most similar customer
    most_similar_customer = similarities_series.idxmax()
    
    # Determine the dominant sector for the most similar customer
    dominant_sector = sector_frequency.loc[[most_similar_customer]].idxmax(axis=1).values[0]
    
    return dominant_sector

# Example usage (choose your preferred similarity method)
customer_id = "12345"  # Ensure this matches an ACCT value in your CSV
persona_label = identify_persona(customer_id, similarity_method="cosine")
print(f"Customer {customer_id} Persona (Cosine Similarity): {persona_label}") 

persona_label = identify_persona(customer_id, similarity_method="jaccard")
print(f"Customer {customer_id} Persona (Jaccard Similarity): {persona_label}")






import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import SpectralClustering
from sklearn.preprocessing import StandardScaler

# Load your data
df = pd.read_csv("IF_MAIN6.csv")

# Create a pivot table to summarize transaction frequencies per sector for each account
sector_frequency = df.pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

# Normalize the data
# It's a good practice to normalize the data when using cosine similarity
scaler = StandardScaler()
sector_frequency_scaled = scaler.fit_transform(sector_frequency)

# Compute the cosine similarity matrix
cosine_sim_matrix = cosine_similarity(sector_frequency_scaled)

# Apply Spectral Clustering using the precomputed cosine similarity matrix
# Adjust 'n_clusters' based on your specific needs or insights from the data
n_clusters = 5
spectral_clust = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=0)
clusters = spectral_clust.fit_predict(cosine_sim_matrix)

# Add the cluster labels to the original data
sector_frequency['Cluster'] = clusters

# Explore the resulting clusters
# This is a basic example of how to examine the clusters
for i in range(n_clusters):
    print(f"\nCluster {i} characteristics:")
    cluster_members = sector_frequency[sector_frequency['Cluster'] == i]
    print(cluster_members.mean())

# If needed, save the results to a new CSV file
# sector_frequency.to_csv("clustered_data.csv")

# Example: Identifying the cluster for a specific customer
customer_id = '12345'  # Replace with an actual ACCT number from your dataset
if customer_id in sector_frequency.index:
    customer_cluster = sector_frequency.loc[customer_id, 'Cluster']
    print(f"\nCustomer {customer_id} is in Cluster: {customer_cluster}")
else:
    print("\nCustomer ID not found in dataset.")




import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import SpectralClustering
from sklearn.preprocessing import StandardScaler

# Load your data
df = pd.read_csv("IF_MAIN6.csv")

# Initial data cleaning to ensure no 'inf' or 'NaN' values that could disrupt the process
df.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace inf with NaN
df.fillna(0, inplace=True)  # Fill NaN values with 0

# Create a pivot table to summarize transaction frequencies per sector for each account
sector_frequency = df.pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

# Check and handle columns with no variance, which can cause issues during normalization
variance = sector_frequency.var()
columns_with_no_variance = variance[variance == 0].index
if not columns_with_no_variance.empty:
    sector_frequency.drop(columns=columns_with_no_variance, axis=1, inplace=True)

# Normalize the data
scaler = StandardScaler()
sector_frequency_scaled = scaler.fit_transform(sector_frequency)

# Compute the cosine similarity matrix
cosine_sim_matrix = cosine_similarity(sector_frequency_scaled)

# Apply Spectral Clustering using the precomputed cosine similarity matrix
n_clusters = 5  # This is an example; adjust based on your data analysis
spectral_clust = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=0)
clusters = spectral_clust.fit_predict(cosine_sim_matrix)

# Add the cluster labels to the original DataFrame
sector_frequency['Cluster'] = clusters

# Example: Identifying the cluster for a specific customer
customer_id = '12345'  # Replace with an actual ACCT number from your dataset
if customer_id in sector_frequency.index:
    customer_cluster = sector_frequency.loc[customer_id, 'Cluster']
    print(f"\nCustomer {customer_id} is in Cluster: {customer_cluster}")
else:
    print("\nCustomer ID not found in dataset.")

# Optionally, explore the clusters
# Here, you can print out the mean transaction frequency per sector for each cluster or any other metric of interest
for i in range(n_clusters):
    print(f"\nCluster {i} characteristics:")
    cluster_members = sector_frequency[sector_frequency['Cluster'] == i]
    print(cluster_members.mean().sort_values(ascending=False).head())  # Top sectors for each cluster
