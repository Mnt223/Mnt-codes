import pandas as pd
import spacy
from datasets import Dataset, DatasetDict
from transformers import RobertaTokenizerFast, RobertaForTokenClassification, Trainer, TrainingArguments

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

def preprocess_with_ner(address):
    # Use spaCy for NER to identify potential merchant names
    doc = nlp(address.lower())
    tokens = [token.text for token in doc]
    labels = [0] * len(tokens)  # Initialize labels
    
    for ent in doc.ents:
        # Expand entity types that could be relevant to merchant names
        if ent.label_ in ["ORG", "GPE", "PRODUCT", "EVENT"]:
            for i in range(ent.start, ent.end):
                labels[i] = 1  # Mark as merchant name
    
    return {'tokens': tokens, 'ner_labels': labels}

# Load your dataset
df = pd.read_csv('your_dataset.csv', usecols=['address'])

# Apply preprocessing to each address
preprocessed_data = [preprocess_with_ner(address) for address in df['address']]
tokens = [item['tokens'] for item in preprocessed_data]
labels = [item['ner_labels'] for item in preprocessed_data]

# Prepare dataset for token classification
data = DatasetDict({
    'train': Dataset.from_dict({'tokens': tokens, 'ner_labels': labels})
})

# Initialize RoBERTa tokenizer
tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')

# Tokenize and align labels
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, padding='max_length', is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples['ner_labels']):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = [-100 if id is None else label[id] for id in word_ids]
        labels.append(label_ids)
    tokenized_inputs['labels'] = labels
    return tokenized_inputs

tokenized_data = data.map(tokenize_and_align_labels, batched=True)

# Define RoBERTa model for token classification
model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=2)

# Training arguments
training_args = TrainingArguments(
    output_dir='./roberta_merchant_name_model',
    evaluation_strategy='epoch',
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize and train the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data['train'],
)

trainer.train()

# Save the trained model and tokenizer
model.save_pretrained('./roberta_merchant_name_model')
tokenizer.save_pretrained('./roberta_merchant_name_model')







import pandas as pd
import spacy
from datasets import Dataset
from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments

# Load spaCy English model for NER
nlp = spacy.load("en_core_web_sm")

def preprocess_with_ner(address):
    doc = nlp(address.lower())
    tokens = [token.text for token in doc]
    labels = [0] * len(tokens)  # Initialize all tokens as non-merchant
    
    # Use NER to improve merchant name identification
    for ent in doc.ents:
        # Include more entity types that could be relevant to merchant names
        if ent.label_ in ["ORG", "GPE", "PRODUCT", "EVENT"]:
            for i in range(ent.start, ent.end):
                labels[i] = 1  # Mark as merchant name

    return tokens, labels

# Load your dataset
df = pd.read_csv('your_dataset.csv', usecols=['address'])

# Apply the preprocessing
processed_data = df['address'].apply(preprocess_with_ner)
df['tokens'], df['labels'] = zip(*processed_data)

# Convert processed data to a Hugging Face Dataset and tokenize
data = Dataset.from_pandas(pd.DataFrame({
    'tokens': df['tokens'].tolist(),
    'labels': df['labels'].tolist()
}))

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, padding='max_length', is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples['labels']):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = [-100 if id is None else label[id] for id in word_ids]
        labels.append(label_ids)
    tokenized_inputs['labels'] = labels
    return tokenized_inputs

tokenized_data = data.map(tokenize_and_align_labels, batched=True)

# Define the model for token classification
model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Training arguments
training_args = TrainingArguments(
    output_dir='./merchant_name_model_final',
    evaluation_strategy='epoch',
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data,
)

# Train the model
trainer.train()

# Save the trained model and tokenizer
model.save_pretrained('./merchant_name_model_final')
tokenizer.save_pretrained('./merchant_name_model_final')














import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Read the data from the CSV file
df = pd.read_csv("IF_MAIN6.csv")

# Assuming 'Age' is a column in your dataframe and 'Occupation' is a categorical feature
# First, one-hot encode 'Occupation' and standardize 'Age'
column_transformer = ColumnTransformer(transformers=[
    ('numerical', StandardScaler(), ['Age']),
    ('categorical', OneHotEncoder(), ['Occupation'])
], remainder='passthrough')

# Apply the transformations and create a new DataFrame
transformed_features = column_transformer.fit_transform(df[['Age', 'Occupation']])
transformed_features_df = pd.DataFrame(transformed_features)

# Assuming the rest of your dataframe ('df') consists of the transaction data needed for the pivot table
# Concatenate the transformed features with the pivot table data
# You might need to adjust this part depending on the structure of your original dataframe
sector_frequency = df.pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

# Combine the sector frequency data with the transformed features
combined_features = pd.concat([transformed_features_df, sector_frequency.reset_index(drop=True)], axis=1)

# Continue with your cosine similarity calculations using 'combined_features'
def calculate_cosine_similarity(data):
    similarity_matrix = cosine_similarity(data)
    return similarity_matrix

# The rest of your code for identifying personas would remain largely the same,
# but ensure to use the combined features for similarity calculations










import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler

# Load and preprocess the data
df = pd.read_csv("IF_MAIN6.csv")
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])

# Create pivot tables for transaction metrics
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_sum = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)
sector_avg = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='mean', fill_value=0)

# Normalize the matrices
scaler = MinMaxScaler()
frequency_scaled, sum_scaled, avg_scaled = [scaler.fit_transform(matrix) for matrix in [sector_frequency, sector_sum, sector_avg]]

# Calculate the composite score
composite_scaled = (frequency_scaled + sum_scaled + avg_scaled) / 3

# Compute cosine similarity for each matrix and the composite
similarity_matrices = [cosine_similarity(matrix) for matrix in [frequency_scaled, sum_scaled, avg_scaled, composite_scaled]]

# Save the cosine similarity matrices to CSV files
similarity_labels = ['Frequency', 'Sum', 'Average', 'Composite']
for similarity_matrix, label in zip(similarity_matrices, similarity_labels):
    similarity_df = pd.DataFrame(similarity_matrix, index=sector_frequency.index, columns=sector_frequency.index)
    similarity_df.to_csv(f"Cosine_Similarity_{label}.csv")

# Function to suggest dominant sectors
def suggest_dominant_sectors(customer_index, similarity_matrix, top_n=5):
    sorted_indices = similarity_matrix[customer_index].argsort()[::-1][1:top_n+1]
    similar_customer_ids = sector_frequency.index[sorted_indices].tolist()
    similar_sectors = sector_frequency.loc[similar_customer_ids].sum().rank(ascending=False).sort_values().index[:3].tolist()
    return similar_sectors

# Initialize an empty DataFrame to store the results
results_df = pd.DataFrame()

# Iterate over all customers to suggest dominant sectors based on the composite score
for customer_id in sector_frequency.index:
    customer_index = sector_frequency.index.get_loc(customer_id)
    composite_sectors = suggest_dominant_sectors(customer_index, similarity_matrices[-1])  # Using composite similarity
    temp_df = pd.DataFrame({
        'ACCT': [customer_id], 
        'Top_Composite_Sectors': [', '.join(composite_sectors)]
    })
    # Use pandas.concat instead of DataFrame.append
    results_df = pd.concat([results_df, temp_df], ignore_index=True)

# Save the results to a CSV file
results_df.to_csv("All_Customers_Dominant_Sectors_Composite.csv", index=False)

print("Dominant sectors based on composite scores for all customers have been saved to All_Customers_Dominant_Sectors_Composite.csv")
print("Cosine similarity matrices saved as CSV files.")















import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler

# Assuming all previous steps are the same and focusing on generating output for all customers

# Load and preprocess the data
df = pd.read_csv("IF_MAIN6.csv")
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])

# Create pivot tables for transaction metrics
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_sum = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)
sector_avg = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='mean', fill_value=0)

# Normalize, compute cosine similarity, and combine as before
scaler = MinMaxScaler()
frequency_scaled, sum_scaled, avg_scaled = [scaler.fit_transform(matrix) for matrix in [sector_frequency, sector_sum, sector_avg]]
similarity_matrices = [cosine_similarity(matrix) for matrix in [frequency_scaled, sum_scaled, avg_scaled]]
combined_similarity = sum(similarity_matrices) / len(similarity_matrices)

# Define the function to suggest dominant sectors based on similar customers
def suggest_dominant_sectors(combined_similarity_matrix, customer_index, top_n=5):
    sorted_indices = combined_similarity_matrix[customer_index].argsort()[::-1][1:top_n+1]  # Exclude self
    similar_customer_ids = sector_frequency.index[sorted_indices].tolist()
    # Aggregate sector data for similar customers
    similar_sector_frequency = sector_frequency.loc[similar_customer_ids].sum().rank(ascending=False).sort_values().index[:3].tolist()
    similar_sector_sum = sector_sum.loc[similar_customer_ids].sum().rank(ascending=False).sort_values().index[:3].tolist()
    similar_sector_avg = sector_avg.loc[similar_customer_ids].sum().rank(ascending=False).sort_values().index[:3].tolist()
    return similar_sector_frequency, similar_sector_sum, similar_sector_avg

# Prepare a DataFrame to store the results
results_df = pd.DataFrame(columns=['ACCT', 'Top_Frequency_Sectors', 'Top_Sum_Sectors', 'Top_Average_Sectors'])

# Iterate over all customers
for customer_id in sector_frequency.index:
    customer_index = sector_frequency.index.get_loc(customer_id)
    freq_sectors, sum_sectors, avg_sectors = suggest_dominant_sectors(combined_similarity, customer_index)
    results_df = results_df.append({
        'ACCT': customer_id, 
        'Top_Frequency_Sectors': ', '.join(freq_sectors),
        'Top_Sum_Sectors': ', '.join(sum_sectors),
        'Top_Average_Sectors': ', '.join(avg_sectors)
    }, ignore_index=True)

# Save the results to a CSV file
results_df.to_csv("All_Customers_Dominant_Sectors.csv", index=False)

print("Dominant sectors for all customers have been saved to All_Customers_Dominant_Sectors.csv")










import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load the data
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create pivot tables for transaction frequencies and total amounts per sector
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

# Create separate pivot tables for weekday and weekend
sector_frequency_weekday = df[df['Day_Type'] == 'Weekday'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts_weekday = df[df['Day_Type'] == 'Weekday'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

sector_frequency_weekend = df[df['Day_Type'] == 'Weekend'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts_weekend = df[df['Day_Type'] == 'Weekend'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

# Normalize the frequency and amounts to ensure they are on the same scale
scaler = MinMaxScaler()
# Overall
sector_frequency_scaled = pd.DataFrame(scaler.fit_transform(sector_frequency), index=sector_frequency.index, columns=sector_frequency.columns)
sector_amounts_scaled = pd.DataFrame(scaler.fit_transform(sector_amounts), index=sector_amounts.index, columns=sector_amounts.columns)

# Weekday
sector_frequency_weekday_scaled = pd.DataFrame(scaler.fit_transform(sector_frequency_weekday), index=sector_frequency_weekday.index, columns=sector_frequency_weekday.columns)
sector_amounts_weekday_scaled = pd.DataFrame(scaler.fit_transform(sector_amounts_weekday), index=sector_amounts_weekday.index, columns=sector_amounts_weekday.columns)

# Weekend
sector_frequency_weekend_scaled = pd.DataFrame(scaler.fit_transform(sector_frequency_weekend), index=sector_frequency_weekend.index, columns=sector_frequency_weekend.columns)
sector_amounts_weekend_scaled = pd.DataFrame(scaler.fit_transform(sector_amounts_weekend), index=sector_amounts_weekend.index, columns=sector_amounts_weekend.columns)

# Define a function to rank sectors within a DataFrame row
def rank_sectors(row):
    return row.rank(ascending=False, method='min')

# Apply the ranking function to each DataFrame
ranked_frequency = sector_frequency_scaled.apply(rank_sectors, axis=1)
ranked_amounts = sector_amounts_scaled.apply(rank_sectors, axis=1)
ranked_composite = ((sector_frequency_scaled + sector_amounts_scaled) / 2).apply(rank_sectors, axis=1)

ranked_frequency_weekday = sector_frequency_weekday_scaled.apply(rank_sectors, axis=1)
ranked_amounts_weekday = sector_amounts_weekday_scaled.apply(rank_sectors, axis=1)

ranked_frequency_weekend = sector_frequency_weekend_scaled.apply(rank_sectors, axis=1)
ranked_amounts_weekend = sector_amounts_weekend_scaled.apply(rank_sectors, axis=1)

# Combine the rankings into a single DataFrame for easier export
combined_rankings = pd.concat([
    ranked_frequency.add_suffix('_FreqRank'), 
    ranked_amounts.add_suffix('_AmtRank'), 
    ranked_composite.add_suffix('_CompRank'),
    ranked_frequency_weekday.add_suffix('_FreqRank_Weekday'), 
    ranked_amounts_weekday.add_suffix('_AmtRank_Weekday'),
    ranked_frequency_weekend.add_suffix('_FreqRank_Weekend'), 
    ranked_amounts_weekend.add_suffix('_AmtRank_Weekend'),
], axis=1)

# Save the combined rankings to a CSV file
combined_rankings.to_csv("Customer_Sector_Rankings_Inclusive.csv")

print("Rankings saved to Customer_Sector_Rankings_Inclusive.csv")










import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load the data
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create pivot tables for transaction frequencies and total amounts per sector
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

# Normalize the frequency and amounts to ensure they are on the same scale
scaler = MinMaxScaler()
sector_frequency_scaled = pd.DataFrame(scaler.fit_transform(sector_frequency), index=sector_frequency.index, columns=sector_frequency.columns)
sector_amounts_scaled = pd.DataFrame(scaler.fit_transform(sector_amounts), index=sector_amounts.index, columns=sector_amounts.columns)

# Calculate a composite score by averaging the scaled frequency and amount scores
sector_composite = (sector_frequency_scaled + sector_amounts_scaled) / 2

# Define a function to rank sectors within a DataFrame row
def rank_sectors(row):
    return row.rank(ascending=False, method='min')

# Apply the ranking function to each DataFrame
ranked_frequency = sector_frequency_scaled.apply(rank_sectors, axis=1)
ranked_amounts = sector_amounts_scaled.apply(rank_sectors, axis=1)
ranked_composite = sector_composite.apply(rank_sectors, axis=1)

# Combine the rankings into a single DataFrame for easier export
combined_rankings = pd.concat([ranked_frequency.add_suffix('_FreqRank'), 
                               ranked_amounts.add_suffix('_AmtRank'), 
                               ranked_composite.add_suffix('_CompRank')], axis=1)

# Save the combined rankings to a CSV file
combined_rankings.to_csv("Customer_Sector_Rankings.csv")

print("Rankings saved to Customer_Sector_Rankings.csv")








import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load the data
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create pivot tables for transaction frequencies and total amounts per sector
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

# Normalize the frequency and amounts to ensure they are on the same scale
scaler = MinMaxScaler()
sector_frequency_scaled = pd.DataFrame(scaler.fit_transform(sector_frequency), index=sector_frequency.index, columns=sector_frequency.columns)
sector_amounts_scaled = pd.DataFrame(scaler.fit_transform(sector_amounts), index=sector_amounts.index, columns=sector_amounts.columns)

# Calculate a composite score by averaging the scaled frequency and amount scores
sector_composite = (sector_frequency_scaled + sector_amounts_scaled) / 2

# Define a function to rank sectors within a DataFrame row
def rank_sectors(row):
    return row.rank(ascending=False, method='min')

# Apply the ranking function to each DataFrame
ranked_frequency = sector_frequency_scaled.apply(rank_sectors, axis=1)
ranked_amounts = sector_amounts_scaled.apply(rank_sectors, axis=1)
ranked_composite = sector_composite.apply(rank_sectors, axis=1)

# Example: Print the rankings for a specific customer
customer_id = '12345'  # Example customer ID, adjust to your dataset
if customer_id in ranked_frequency.index:
    print(f"Frequency Rankings for Customer {customer_id}:\n{ranked_frequency.loc[customer_id]}")
    print(f"Amount Rankings for Customer {customer_id}:\n{ranked_amounts.loc[customer_id]}")
    print(f"Composite Score Rankings for Customer {customer_id}:\n{ranked_composite.loc[customer_id]}")
else:
    print("Customer ID not found in dataset.")





import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Read the data from the CSV file
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create a pivot table to get transaction frequencies and total amounts per sector
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

# Combine frequency and amount for a composite score
# Adjust the weighting as necessary to balance frequency vs. amount importance
sector_composite = (sector_frequency + sector_amounts) / 2

# Function for Cosine Similarity (for composite scores)
def calculate_cosine_similarity(data):
    similarity_matrix = cosine_similarity(data)
    return similarity_matrix

# Function to find the dominant sector based on highest value
def find_dominant_sector(data):
    return data.idxmax(axis=1)

# Prepare results DataFrame
results = pd.DataFrame(index=sector_frequency.index)

# Find dominant sectors based on frequency, amount, and composite score
results['Dominant_Frequency'] = find_dominant_sector(sector_frequency)
results['Dominant_Amount'] = find_dominant_sector(sector_amounts)
results['Dominant_Composite'] = find_dominant_sector(sector_composite)

# Print or save results to CSV
print(results.head())  # For quick inspection
results.to_csv("Dominant_Sectors_Analysis.csv")

print("Analysis saved to Dominant_Sectors_Analysis.csv")
