import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])

# Splitting dataset into training and testing
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)

# Creating Interaction Matrices for Counts and Amounts (Training Data)
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Normalize the interaction matrices
scaler = MinMaxScaler()
interaction_counts_normalized_train = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized_train = scaler.fit_transform(interaction_amounts_train)

# Combine normalized matrices with equal weight
combined_interaction_train = (interaction_counts_normalized_train + interaction_amounts_normalized_train) / 2

# Convert combined interaction matrix back to DataFrame for easier handling
combined_interaction_df_train = pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, columns=interaction_counts_train.columns)

# Calculate sector-to-sector similarity matrix from the combined interaction matrix
sector_similarity = cosine_similarity(combined_interaction_train.T)
sector_similarity_df = pd.DataFrame(sector_similarity, index=interaction_counts_train.columns, columns=interaction_counts_train.columns)

# Function to generate recommendations for all customers
def generate_recommendations_for_all(interaction_matrix, sector_similarity_matrix, sectors, top_n=5):
    recommendations = {}
    sector_names = sectors.tolist()
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[account].values.reshape(1, -1)
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommended_sectors = [sector_names[i] for i in top_sector_indices]
        recommendations[account] = recommended_sectors
    return recommendations

all_recommendations = generate_recommendations_for_all(combined_interaction_df_train, sector_similarity, interaction_counts_train.columns)

# Preparing DataFrame for recommendations
recommendations_df = pd.DataFrame(list(all_recommendations.items()), columns=['ACCT', 'Recommended Sectors'])

# Preparing normalized interaction counts and amounts DataFrames
interaction_counts_df = pd.DataFrame(interaction_counts_normalized_train, index=interaction_counts_train.index, columns=interaction_counts_train.columns)
interaction_amounts_df = pd.DataFrame(interaction_amounts_normalized_train, index=interaction_amounts_train.index, columns=interaction_amounts_train.columns)

# Function to calculate average precision (for completeness, implementation would be similar to provided examples)

# Export to Excel with separate sheets
with pd.ExcelWriter('recommendations_and_matrices.xlsx', engine='openpyxl') as writer:
    recommendations_df.to_excel(writer, sheet_name='Customer Recommendations')
    interaction_counts_df.to_excel(writer, sheet_name='Interaction Counts')
    interaction_amounts_df.to_excel(writer, sheet_name='Interaction Amounts')
    combined_interaction_df_train.to_excel(writer, sheet_name='Combined Interaction')
    sector_similarity_df.to_excel(writer, sheet_name='Sector Similarity')

print("Exported all data to 'recommendations_and_matrices.xlsx'.")






import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split

# Load and preprocess the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])

# Splitting dataset into training and testing
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)

# Creating and normalizing interaction matrices for training data
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
scaler = MinMaxScaler()
interaction_counts_normalized_train = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized_train = scaler.fit_transform(interaction_amounts_train)
combined_interaction_train = (interaction_counts_normalized_train + interaction_amounts_normalized_train) / 2
sectors = interaction_counts_train.columns

# Sector-to-sector similarity matrix
sector_similarity = cosine_similarity(combined_interaction_train.T)
sector_similarity_df = pd.DataFrame(sector_similarity, index=sectors, columns=sectors)

# Generate recommendations
def generate_recommendations_for_all(interaction_matrix, sector_similarity_matrix, top_n=5):
    recommendations = {}
    sector_names = interaction_matrix.columns
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[account].values.reshape(1, -1)
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommended_sectors = sector_names[top_sector_indices]
        recommendations[account] = recommended_sectors.tolist()
    return recommendations

combined_interaction_df_train = pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, columns=sectors)
all_recommendations = generate_recommendations_for_all(combined_interaction_df_train, sector_similarity, sectors)

# Analysis and Export
interaction_counts_test = test_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_test = test_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Prepare and export analysis
def prepare_export_analysis(recommendations, interaction_df, filename_prefix):
    analysis_df = pd.DataFrame()
    for acct, recommended_sectors in recommendations.items():
        for sector in recommended_sectors:
            value = interaction_df.loc[acct, sector] if acct in interaction_df.index and sector in interaction_df.columns else 0
            analysis_df = analysis_df.append({
                'ACCT': acct, 'Sector': sector, f'{filename_prefix} Value': value
            }, ignore_index=True)
    analysis_df.to_csv(f'{filename_prefix}_analysis.csv', index=False)

# Preparing and exporting analysis
prepare_export_analysis(all_recommendations, interaction_counts_test, 'test_sector_count')
prepare_export_analysis(all_recommendations, interaction_amounts_test, 'test_sector_amount')

# Export initial analysis matrices and recommendations
with pd.ExcelWriter('recommendations_and_matrices.xlsx', engine='openpyxl') as writer:
    recommendations_df.to_excel(writer, sheet_name='Customer Recommendations')
    interaction_counts_df.to_excel(writer, sheet_name='Interaction Counts Train')
    interaction_amounts_df.to_excel(writer, sheet_name='Interaction Amounts Train')
    combined_interaction_df.to_excel(writer, sheet_name='Combined Interaction Train')
    sector_similarity_df.to_excel(writer, sheet_name='Sector Similarity')

print("Exported all data including detailed test sector analysis.")










import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

# Load the dataset
df = pd.read_csv('main6.csv')
df['DATE'] = pd.to_datetime(df['DATE'])

# Splitting dataset into training and testing
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)

# Creating Interaction Matrices for Counts and Amounts (Training Data)
interaction_counts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
interaction_amounts_train = train_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)

# Normalize the interaction matrices
scaler = MinMaxScaler()
interaction_counts_normalized_train = scaler.fit_transform(interaction_counts_train)
interaction_amounts_normalized_train = scaler.fit_transform(interaction_amounts_train)

# Combine normalized matrices with equal weight
combined_interaction_train = (interaction_counts_normalized_train + interaction_amounts_normalized_train) / 2
sectors = interaction_counts_train.columns

# Calculate sector-to-sector similarity matrix from the combined interaction matrix
sector_similarity = cosine_similarity(combined_interaction_train.T)
sector_similarity_df = pd.DataFrame(sector_similarity, index=sectors, columns=sectors)

# Generate recommendations for all customers based on sector similarity
def generate_recommendations_for_all(interaction_matrix, sector_similarity_matrix, top_n=5):
    recommendations = {}
    sector_names = sectors.tolist()
    for account in interaction_matrix.index:
        user_vector = interaction_matrix.loc[[account]].values
        user_sector_similarity = np.dot(user_vector, sector_similarity_matrix)[0]
        top_sector_indices = np.argsort(-user_sector_similarity)[:top_n]
        recommended_sectors = [sector_names[i] for i in top_sector_indices]
        recommendations[account] = recommended_sectors
    return recommendations

interaction_matrix_df_train = pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, columns=sectors)
all_recommendations = generate_recommendations_for_all(interaction_matrix_df_train, sector_similarity, sectors)

# Function to calculate average precision
def calculate_average_precision(recommendations, test_data):
    hits = 0
    total_recommendations = 0
    for account, recommended_sectors in recommendations.items():
        actual_sectors = test_data.loc[test_data['ACCT'] == account, 'Sector'].unique()
        hits += len(set(recommended_sectors) & set(actual_sectors))
        total_recommendations += len(recommended_sectors)
    return hits / total_recommendations if total_recommendations else 0

average_precision = calculate_average_precision(all_recommendations, test_df)
print(f"Average Precision Score: {average_precision:.4f}")

# Calculate and export total and recommended sector spending during the test period
total_spent_test = test_df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
amounts_spent_in_recommended_sectors = pd.DataFrame(columns=['ACCT', 'Sector', 'Amount Spent'])

for acct, recommended_sectors in all_recommendations.items():
    for sector in recommended_sectors:
        amount_spent = total_spent_test.loc[acct, sector] if acct in total_spent_test.index and sector in total_spent_test.columns else 0
        amounts_spent_in_recommended_sectors = amounts_spent_in_recommended_sectors.append({
            'ACCT': acct, 
            'Sector': sector, 
            'Amount Spent': amount_spent
        }, ignore_index=True)

# Exporting DataFrames to CSV files
interaction_counts_train.to_csv('interaction_counts_train.csv')
interaction_amounts_train.to_csv('interaction_amounts_train.csv')
pd.DataFrame(combined_interaction_train, index=interaction_counts_train.index, columns=sectors).to_csv('combined_interaction_train.csv')
sector_similarity_df.to_csv('sector_similarity.csv')
pd.DataFrame.from_dict(all_recommendations, orient='index').to_csv('all_recommendations.csv')
total_spent_test.to_csv('total_spent_test.csv')
amounts_spent_in_recommended_sectors.to_csv('amounts_spent_in_recommended_sectors.csv')

print("Exported all data to CSV files including detailed spending analysis.")










import pandas as pd
from implicit.als import AlternatingLeastSquares
import numpy as np 

# Load your data
df = pd.read_csv("transactions.csv")

# Preprocessing - Discretize Age
def discretize_age(age):
    if age <= 25:
        return "young"
    elif age <= 40:
        return "mid"
    else:
        return "senior"

df['Age_group'] = df['Age'].apply(discretize_age)

# Preprocessing - Weekend Calculation
def is_weekend(date):
    return date.weekday() >= 5 

df['is_weekend'] = df['Date'].apply(is_weekend)

# One-hot Encode Categorical Features
df = pd.get_dummies(df, columns=['Occupation', 'Age_group'])

# Feature Engineering
df['avg_spending'] = df.groupby('ACCT')['TRAN_AMT'].transform('mean')
df['weekend_ratio'] = df.groupby('ACCT')['is_weekend'].transform(lambda x: x.sum() / len(x))

# Customer-Sector Matrix
cust_sector_matrix = df.pivot_table(
    index='ACCT', columns='Sector', values='TRAN_AMT', fill_value=0
)

# Include Additional Features
cust_sector_matrix = pd.concat([cust_sector_matrix, df[['avg_spending', 'weekend_ratio']]], axis=1)

# Confidence Weighting 
confidence_matrix = cust_sector_matrix * 40 

# Model Training
model = AlternatingLeastSquares(factors=50, regularization=0.01) 
model.fit(confidence_matrix)

# Get Recommendations 
customer_id = 12345
recommendations = model.recommend(userid=customer_id, user_items=cust_sector_matrix[customer_id])
top_sector = recommendations[0][0]
print(f"Top predicted sector for customer {customer_id}: {top_sector}")











import pandas as pd
import spacy
from datasets import Dataset, DatasetDict
from transformers import RobertaTokenizerFast, RobertaForTokenClassification, Trainer, TrainingArguments

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

def preprocess_with_ner(address):
    # Use spaCy for NER to identify potential merchant names
    doc = nlp(address.lower())
    tokens = [token.text for token in doc]
    labels = [0] * len(tokens)  # Initialize labels
    
    for ent in doc.ents:
        # Expand entity types that could be relevant to merchant names
        if ent.label_ in ["ORG", "GPE", "PRODUCT", "EVENT"]:
            for i in range(ent.start, ent.end):
                labels[i] = 1  # Mark as merchant name
    
    return {'tokens': tokens, 'ner_labels': labels}

# Load your dataset
df = pd.read_csv('your_dataset.csv', usecols=['address'])

# Apply preprocessing to each address
preprocessed_data = [preprocess_with_ner(address) for address in df['address']]
tokens = [item['tokens'] for item in preprocessed_data]
labels = [item['ner_labels'] for item in preprocessed_data]

# Prepare dataset for token classification
data = DatasetDict({
    'train': Dataset.from_dict({'tokens': tokens, 'ner_labels': labels})
})

# Initialize RoBERTa tokenizer
tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')

# Tokenize and align labels
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, padding='max_length', is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples['ner_labels']):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = [-100 if id is None else label[id] for id in word_ids]
        labels.append(label_ids)
    tokenized_inputs['labels'] = labels
    return tokenized_inputs

tokenized_data = data.map(tokenize_and_align_labels, batched=True)

# Define RoBERTa model for token classification
model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=2)

# Training arguments
training_args = TrainingArguments(
    output_dir='./roberta_merchant_name_model',
    evaluation_strategy='epoch',
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize and train the model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data['train'],
)

trainer.train()

# Save the trained model and tokenizer
model.save_pretrained('./roberta_merchant_name_model')
tokenizer.save_pretrained('./roberta_merchant_name_model')







import pandas as pd
import spacy
from datasets import Dataset
from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments

# Load spaCy English model for NER
nlp = spacy.load("en_core_web_sm")

def preprocess_with_ner(address):
    doc = nlp(address.lower())
    tokens = [token.text for token in doc]
    labels = [0] * len(tokens)  # Initialize all tokens as non-merchant
    
    # Use NER to improve merchant name identification
    for ent in doc.ents:
        # Include more entity types that could be relevant to merchant names
        if ent.label_ in ["ORG", "GPE", "PRODUCT", "EVENT"]:
            for i in range(ent.start, ent.end):
                labels[i] = 1  # Mark as merchant name

    return tokens, labels

# Load your dataset
df = pd.read_csv('your_dataset.csv', usecols=['address'])

# Apply the preprocessing
processed_data = df['address'].apply(preprocess_with_ner)
df['tokens'], df['labels'] = zip(*processed_data)

# Convert processed data to a Hugging Face Dataset and tokenize
data = Dataset.from_pandas(pd.DataFrame({
    'tokens': df['tokens'].tolist(),
    'labels': df['labels'].tolist()
}))

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, padding='max_length', is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples['labels']):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        label_ids = [-100 if id is None else label[id] for id in word_ids]
        labels.append(label_ids)
    tokenized_inputs['labels'] = labels
    return tokenized_inputs

tokenized_data = data.map(tokenize_and_align_labels, batched=True)

# Define the model for token classification
model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Training arguments
training_args = TrainingArguments(
    output_dir='./merchant_name_model_final',
    evaluation_strategy='epoch',
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data,
)

# Train the model
trainer.train()

# Save the trained model and tokenizer
model.save_pretrained('./merchant_name_model_final')
tokenizer.save_pretrained('./merchant_name_model_final')














import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Read the data from the CSV file
df = pd.read_csv("IF_MAIN6.csv")

# Assuming 'Age' is a column in your dataframe and 'Occupation' is a categorical feature
# First, one-hot encode 'Occupation' and standardize 'Age'
column_transformer = ColumnTransformer(transformers=[
    ('numerical', StandardScaler(), ['Age']),
    ('categorical', OneHotEncoder(), ['Occupation'])
], remainder='passthrough')

# Apply the transformations and create a new DataFrame
transformed_features = column_transformer.fit_transform(df[['Age', 'Occupation']])
transformed_features_df = pd.DataFrame(transformed_features)

# Assuming the rest of your dataframe ('df') consists of the transaction data needed for the pivot table
# Concatenate the transformed features with the pivot table data
# You might need to adjust this part depending on the structure of your original dataframe
sector_frequency = df.pivot_table(
    index="ACCT",
    columns="Sector",
    values="TRAN_AMT",
    aggfunc='count',
    fill_value=0
)

# Combine the sector frequency data with the transformed features
combined_features = pd.concat([transformed_features_df, sector_frequency.reset_index(drop=True)], axis=1)

# Continue with your cosine similarity calculations using 'combined_features'
def calculate_cosine_similarity(data):
    similarity_matrix = cosine_similarity(data)
    return similarity_matrix

# The rest of your code for identifying personas would remain largely the same,
# but ensure to use the combined features for similarity calculations










import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler

# Load and preprocess the data
df = pd.read_csv("IF_MAIN6.csv")
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])

# Create pivot tables for transaction metrics
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_sum = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)
sector_avg = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='mean', fill_value=0)

# Normalize the matrices
scaler = MinMaxScaler()
frequency_scaled, sum_scaled, avg_scaled = [scaler.fit_transform(matrix) for matrix in [sector_frequency, sector_sum, sector_avg]]

# Calculate the composite score
composite_scaled = (frequency_scaled + sum_scaled + avg_scaled) / 3

# Compute cosine similarity for each matrix and the composite
similarity_matrices = [cosine_similarity(matrix) for matrix in [frequency_scaled, sum_scaled, avg_scaled, composite_scaled]]

# Save the cosine similarity matrices to CSV files
similarity_labels = ['Frequency', 'Sum', 'Average', 'Composite']
for similarity_matrix, label in zip(similarity_matrices, similarity_labels):
    similarity_df = pd.DataFrame(similarity_matrix, index=sector_frequency.index, columns=sector_frequency.index)
    similarity_df.to_csv(f"Cosine_Similarity_{label}.csv")

# Function to suggest dominant sectors
def suggest_dominant_sectors(customer_index, similarity_matrix, top_n=5):
    sorted_indices = similarity_matrix[customer_index].argsort()[::-1][1:top_n+1]
    similar_customer_ids = sector_frequency.index[sorted_indices].tolist()
    similar_sectors = sector_frequency.loc[similar_customer_ids].sum().rank(ascending=False).sort_values().index[:3].tolist()
    return similar_sectors

# Initialize an empty DataFrame to store the results
results_df = pd.DataFrame()

# Iterate over all customers to suggest dominant sectors based on the composite score
for customer_id in sector_frequency.index:
    customer_index = sector_frequency.index.get_loc(customer_id)
    composite_sectors = suggest_dominant_sectors(customer_index, similarity_matrices[-1])  # Using composite similarity
    temp_df = pd.DataFrame({
        'ACCT': [customer_id], 
        'Top_Composite_Sectors': [', '.join(composite_sectors)]
    })
    # Use pandas.concat instead of DataFrame.append
    results_df = pd.concat([results_df, temp_df], ignore_index=True)

# Save the results to a CSV file
results_df.to_csv("All_Customers_Dominant_Sectors_Composite.csv", index=False)

print("Dominant sectors based on composite scores for all customers have been saved to All_Customers_Dominant_Sectors_Composite.csv")
print("Cosine similarity matrices saved as CSV files.")















import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler

# Assuming all previous steps are the same and focusing on generating output for all customers

# Load and preprocess the data
df = pd.read_csv("IF_MAIN6.csv")
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])

# Create pivot tables for transaction metrics
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_sum = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)
sector_avg = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='mean', fill_value=0)

# Normalize, compute cosine similarity, and combine as before
scaler = MinMaxScaler()
frequency_scaled, sum_scaled, avg_scaled = [scaler.fit_transform(matrix) for matrix in [sector_frequency, sector_sum, sector_avg]]
similarity_matrices = [cosine_similarity(matrix) for matrix in [frequency_scaled, sum_scaled, avg_scaled]]
combined_similarity = sum(similarity_matrices) / len(similarity_matrices)

# Define the function to suggest dominant sectors based on similar customers
def suggest_dominant_sectors(combined_similarity_matrix, customer_index, top_n=5):
    sorted_indices = combined_similarity_matrix[customer_index].argsort()[::-1][1:top_n+1]  # Exclude self
    similar_customer_ids = sector_frequency.index[sorted_indices].tolist()
    # Aggregate sector data for similar customers
    similar_sector_frequency = sector_frequency.loc[similar_customer_ids].sum().rank(ascending=False).sort_values().index[:3].tolist()
    similar_sector_sum = sector_sum.loc[similar_customer_ids].sum().rank(ascending=False).sort_values().index[:3].tolist()
    similar_sector_avg = sector_avg.loc[similar_customer_ids].sum().rank(ascending=False).sort_values().index[:3].tolist()
    return similar_sector_frequency, similar_sector_sum, similar_sector_avg

# Prepare a DataFrame to store the results
results_df = pd.DataFrame(columns=['ACCT', 'Top_Frequency_Sectors', 'Top_Sum_Sectors', 'Top_Average_Sectors'])

# Iterate over all customers
for customer_id in sector_frequency.index:
    customer_index = sector_frequency.index.get_loc(customer_id)
    freq_sectors, sum_sectors, avg_sectors = suggest_dominant_sectors(combined_similarity, customer_index)
    results_df = results_df.append({
        'ACCT': customer_id, 
        'Top_Frequency_Sectors': ', '.join(freq_sectors),
        'Top_Sum_Sectors': ', '.join(sum_sectors),
        'Top_Average_Sectors': ', '.join(avg_sectors)
    }, ignore_index=True)

# Save the results to a CSV file
results_df.to_csv("All_Customers_Dominant_Sectors.csv", index=False)

print("Dominant sectors for all customers have been saved to All_Customers_Dominant_Sectors.csv")










import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load the data
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create pivot tables for transaction frequencies and total amounts per sector
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

# Create separate pivot tables for weekday and weekend
sector_frequency_weekday = df[df['Day_Type'] == 'Weekday'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts_weekday = df[df['Day_Type'] == 'Weekday'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

sector_frequency_weekend = df[df['Day_Type'] == 'Weekend'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts_weekend = df[df['Day_Type'] == 'Weekend'].pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

# Normalize the frequency and amounts to ensure they are on the same scale
scaler = MinMaxScaler()
# Overall
sector_frequency_scaled = pd.DataFrame(scaler.fit_transform(sector_frequency), index=sector_frequency.index, columns=sector_frequency.columns)
sector_amounts_scaled = pd.DataFrame(scaler.fit_transform(sector_amounts), index=sector_amounts.index, columns=sector_amounts.columns)

# Weekday
sector_frequency_weekday_scaled = pd.DataFrame(scaler.fit_transform(sector_frequency_weekday), index=sector_frequency_weekday.index, columns=sector_frequency_weekday.columns)
sector_amounts_weekday_scaled = pd.DataFrame(scaler.fit_transform(sector_amounts_weekday), index=sector_amounts_weekday.index, columns=sector_amounts_weekday.columns)

# Weekend
sector_frequency_weekend_scaled = pd.DataFrame(scaler.fit_transform(sector_frequency_weekend), index=sector_frequency_weekend.index, columns=sector_frequency_weekend.columns)
sector_amounts_weekend_scaled = pd.DataFrame(scaler.fit_transform(sector_amounts_weekend), index=sector_amounts_weekend.index, columns=sector_amounts_weekend.columns)

# Define a function to rank sectors within a DataFrame row
def rank_sectors(row):
    return row.rank(ascending=False, method='min')

# Apply the ranking function to each DataFrame
ranked_frequency = sector_frequency_scaled.apply(rank_sectors, axis=1)
ranked_amounts = sector_amounts_scaled.apply(rank_sectors, axis=1)
ranked_composite = ((sector_frequency_scaled + sector_amounts_scaled) / 2).apply(rank_sectors, axis=1)

ranked_frequency_weekday = sector_frequency_weekday_scaled.apply(rank_sectors, axis=1)
ranked_amounts_weekday = sector_amounts_weekday_scaled.apply(rank_sectors, axis=1)

ranked_frequency_weekend = sector_frequency_weekend_scaled.apply(rank_sectors, axis=1)
ranked_amounts_weekend = sector_amounts_weekend_scaled.apply(rank_sectors, axis=1)

# Combine the rankings into a single DataFrame for easier export
combined_rankings = pd.concat([
    ranked_frequency.add_suffix('_FreqRank'), 
    ranked_amounts.add_suffix('_AmtRank'), 
    ranked_composite.add_suffix('_CompRank'),
    ranked_frequency_weekday.add_suffix('_FreqRank_Weekday'), 
    ranked_amounts_weekday.add_suffix('_AmtRank_Weekday'),
    ranked_frequency_weekend.add_suffix('_FreqRank_Weekend'), 
    ranked_amounts_weekend.add_suffix('_AmtRank_Weekend'),
], axis=1)

# Save the combined rankings to a CSV file
combined_rankings.to_csv("Customer_Sector_Rankings_Inclusive.csv")

print("Rankings saved to Customer_Sector_Rankings_Inclusive.csv")










import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load the data
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create pivot tables for transaction frequencies and total amounts per sector
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

# Normalize the frequency and amounts to ensure they are on the same scale
scaler = MinMaxScaler()
sector_frequency_scaled = pd.DataFrame(scaler.fit_transform(sector_frequency), index=sector_frequency.index, columns=sector_frequency.columns)
sector_amounts_scaled = pd.DataFrame(scaler.fit_transform(sector_amounts), index=sector_amounts.index, columns=sector_amounts.columns)

# Calculate a composite score by averaging the scaled frequency and amount scores
sector_composite = (sector_frequency_scaled + sector_amounts_scaled) / 2

# Define a function to rank sectors within a DataFrame row
def rank_sectors(row):
    return row.rank(ascending=False, method='min')

# Apply the ranking function to each DataFrame
ranked_frequency = sector_frequency_scaled.apply(rank_sectors, axis=1)
ranked_amounts = sector_amounts_scaled.apply(rank_sectors, axis=1)
ranked_composite = sector_composite.apply(rank_sectors, axis=1)

# Combine the rankings into a single DataFrame for easier export
combined_rankings = pd.concat([ranked_frequency.add_suffix('_FreqRank'), 
                               ranked_amounts.add_suffix('_AmtRank'), 
                               ranked_composite.add_suffix('_CompRank')], axis=1)

# Save the combined rankings to a CSV file
combined_rankings.to_csv("Customer_Sector_Rankings.csv")

print("Rankings saved to Customer_Sector_Rankings.csv")








import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load the data
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create pivot tables for transaction frequencies and total amounts per sector
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

# Normalize the frequency and amounts to ensure they are on the same scale
scaler = MinMaxScaler()
sector_frequency_scaled = pd.DataFrame(scaler.fit_transform(sector_frequency), index=sector_frequency.index, columns=sector_frequency.columns)
sector_amounts_scaled = pd.DataFrame(scaler.fit_transform(sector_amounts), index=sector_amounts.index, columns=sector_amounts.columns)

# Calculate a composite score by averaging the scaled frequency and amount scores
sector_composite = (sector_frequency_scaled + sector_amounts_scaled) / 2

# Define a function to rank sectors within a DataFrame row
def rank_sectors(row):
    return row.rank(ascending=False, method='min')

# Apply the ranking function to each DataFrame
ranked_frequency = sector_frequency_scaled.apply(rank_sectors, axis=1)
ranked_amounts = sector_amounts_scaled.apply(rank_sectors, axis=1)
ranked_composite = sector_composite.apply(rank_sectors, axis=1)

# Example: Print the rankings for a specific customer
customer_id = '12345'  # Example customer ID, adjust to your dataset
if customer_id in ranked_frequency.index:
    print(f"Frequency Rankings for Customer {customer_id}:\n{ranked_frequency.loc[customer_id]}")
    print(f"Amount Rankings for Customer {customer_id}:\n{ranked_amounts.loc[customer_id]}")
    print(f"Composite Score Rankings for Customer {customer_id}:\n{ranked_composite.loc[customer_id]}")
else:
    print("Customer ID not found in dataset.")





import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

# Read the data from the CSV file
df = pd.read_csv("IF_MAIN6.csv")

# Convert 'TRAN_DATE' to datetime and categorize transactions into 'Weekday' or 'Weekend'
df['TRAN_DATE'] = pd.to_datetime(df['TRAN_DATE'])
df['Day_Type'] = df['TRAN_DATE'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Create a pivot table to get transaction frequencies and total amounts per sector
sector_frequency = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='count', fill_value=0)
sector_amounts = df.pivot_table(index="ACCT", columns="Sector", values="TRAN_AMT", aggfunc='sum', fill_value=0)

# Combine frequency and amount for a composite score
# Adjust the weighting as necessary to balance frequency vs. amount importance
sector_composite = (sector_frequency + sector_amounts) / 2

# Function for Cosine Similarity (for composite scores)
def calculate_cosine_similarity(data):
    similarity_matrix = cosine_similarity(data)
    return similarity_matrix

# Function to find the dominant sector based on highest value
def find_dominant_sector(data):
    return data.idxmax(axis=1)

# Prepare results DataFrame
results = pd.DataFrame(index=sector_frequency.index)

# Find dominant sectors based on frequency, amount, and composite score
results['Dominant_Frequency'] = find_dominant_sector(sector_frequency)
results['Dominant_Amount'] = find_dominant_sector(sector_amounts)
results['Dominant_Composite'] = find_dominant_sector(sector_composite)

# Print or save results to CSV
print(results.head())  # For quick inspection
results.to_csv("Dominant_Sectors_Analysis.csv")

print("Analysis saved to Dominant_Sectors_Analysis.csv")
