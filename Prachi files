from dash import Dash, html, dcc, Input, Output
import plotly.graph_objects as go
import pandas as pd

# Load data from Excel file and preprocess
df = pd.read_excel('map_d.xlsx', engine='openpyxl')
# Ensure proper data types
df['Date'] = pd.to_datetime(df['Date'])  # Converting to datetime
df['count'] = df['count'].astype(float)  # Ensuring 'count' is float for consistency in plotting

# Sorting DataFrame by Date to ensure consistent slider behavior
df.sort_values('Date', inplace=True)

app = Dash(__name__)

app.layout = html.Div([
    html.H1('3D Map Visualization with Time Slider'),
    dcc.Graph(id='3d-map'),
    dcc.Slider(
        id='date-slider',
        min=0,
        max=len(df['Date'].dt.date.unique()) - 1,
        value=0,
        marks={i: str(date) for i, date in enumerate(df['Date'].dt.date.unique())},
        step=1
    ),
    # Placeholder for dynamic JavaScript
    html.Div(id='dynamic-js', style={'display': 'none'})
])

@app.callback(
    Output('3d-map', 'figure'),
    [Input('date-slider', 'value')]
)
def update_figure(selected_date_index):
    # Filtering data based on the slider input
    unique_dates = sorted(df['Date'].dt.date.unique())
    selected_date = unique_dates[selected_date_index]
    filtered_df = df[df['Date'].dt.date == selected_date]

    # Creating the 3D scatter plot
    fig = go.Figure(data=[go.Scatter3d(
        x=filtered_df['Longitude'],
        y=filtered_df['Latitude'],
        z=filtered_df['count'],
        text=filtered_df['City'],  # Adding city names as hover text
        mode='markers',
        marker=dict(
            size=filtered_df['count'],
            sizemode='diameter',
            opacity=0.8,
            color=filtered_df['count'],
            colorscale='Viridis',
            colorbar_title='Transactions'
        )
    )])
    
    fig.update_layout(
        margin=dict(l=0, r=0, b=0, t=30),
        scene=dict(
            xaxis_title='Longitude',
            yaxis_title='Latitude',
            zaxis_title='Transaction Count',
            # Setting the aspect ratio for better visualization
            aspectmode='cube'
        ),
        title=f'Transactions on {selected_date}'
    )
    return fig

@app.callback(
    Output('dynamic-js', 'children'),
    Input('date-slider', 'value')
)
def inject_js(selected_date_index):
    # This function dynamically injects JS. It's a simple example, replace or expand as needed.
    return html.Script('''
        console.log('Custom JS code injected for enhanced interaction.');
        // Add your custom JavaScript here for more advanced interactions or visual effects.
    ''')

if __name__ == '__main__':
    app.run_server(debug=True)










import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import pandas as pd
import plotly.express as px

# Dummy Data Preparation (replace with your actual dataset)
df = pd.DataFrame({
    'City name': ['Mumbai', 'Delhi', 'Bangalore'],
    'Latitude': [19.0760, 28.7041, 12.9716],
    'Longitude': [72.8777, 77.1025, 77.5946],
    'Transaction date': pd.to_datetime(['2024-01-01', '2024-01-02', '2024-01-03']),
    'Transaction count': [100, 150, 200]
})

# Initialize Dash app
app = dash.Dash(__name__)

app.layout = html.Div([
    dcc.Graph(id='map-visualization'),
    html.Button('Play', id='play-button', n_clicks=0),
    dcc.Slider(
        id='date-slider',
        min=0,
        max=len(df['Transaction date'].unique()) - 1,
        value=0,
        marks={i: date.strftime('%Y-%m-%d') for i, date in enumerate(df['Transaction date'].dt.date.unique())},
        step=None
    ),
    dcc.Interval(
        id='auto-play-interval',
        interval=2000,  # 2 seconds per step
        n_intervals=0,
        disabled=True  # Disabled by default
    )
], style={'width': '50%', 'padding': '20px', 'align-items': 'center'})

@app.callback(
    Output('map-visualization', 'figure'),
    [Input('date-slider', 'value')]
)
def update_map(selected_date_index):
    filtered_df = df.iloc[[selected_date_index]]  # Using iloc to simulate date filtering
    fig = px.scatter_mapbox(filtered_df,
                            lat='Latitude',
                            lon='Longitude',
                            size='Transaction count',
                            color='Transaction count',
                            color_continuous_scale=px.colors.cyclical.IceFire,
                            size_max=15,
                            zoom=4,
                            mapbox_style='carto-positron')  # Use your Mapbox style or 'open-street-map'
    fig.update_layout(mapbox_accesstoken='YOUR_MAPBOX_ACCESS_TOKEN')  # Optional for certain Mapbox styles
    return fig

@app.callback(
    Output('auto-play-interval', 'disabled'),
    [Input('play-button', 'n_clicks')]
)
def toggle_play_pause(n_clicks):
    return not n_clicks % 2  # Toggle based on odd/even clicks

@app.callback(
    Output('date-slider', 'value'),
    [Input('auto-play-interval', 'n_intervals')],
    [State('date-slider', 'value'), State('date-slider', 'max')]
)
def auto_advance_slider(n, current_value, max_value):
    return (current_value + 1) % (max_value + 1)  # Loop back to start after reaching the end

if __name__ == '__main__':
    app.run_server(debug=True)










%macro process_daily_reports(basePath, masterFilePath, logFilePath);
    /* Calculate yesterday's date and format it for the folder name */
    %let yesterday = %sysfunc(intnx(day, %sysfunc(today()), -1), yymmddn8.);
    %let folderDate = %sysfunc(putn(&yesterday, yymmddn8.));
    %let folderName = email_%substr(&folderDate, 7, 2)_%substr(&folderDate, 5, 2)_%substr(&folderDate, 1, 4);

    /* Known file name patterns */
    %let file1 = 12345_detailed_report.csv;
    %let file2 = 34567_detailed_report.csv;
    %let file3 = 56789_detailed_report.csv;

    /* Attempt to import each file */
    %do i = 1 %to 3;
        %let currentFile = %scan(&file1 &file2 &file3, &i);
        %let fullPath = &basePath\&folderName\&currentFile;
        
        /* Check if the file exists to avoid unnecessary PROC IMPORT errors */
        %if %sysfunc(fexist(&fullPath)) %then %do;
            proc import datafile="&fullPath"
                out=work.imported&i
                dbms=csv
                replace;
                getnames=yes;
            run;
        %end;
        %else %do;
            %put File &fullPath does not exist.;
        %end;
    %end;

    /* Combine imported datasets if they exist */
    data work.combined;
        %do i = 1 %to 3;
            %if %sysfunc(exist(work.imported&i)) %then %do;
                set work.imported&i indsname=src;
                by notsorted;
                if _n_ = 1 then source = src;
            %end;
        %end;
    run;

    /* Append the combined data to the master dataset */
    proc append base=&masterFilePath data=work.combined force;
    run;

    /* Optional: Update a log file with the processed folder name */
    data _null_;
        file "&logFilePath" mod;
        put "&folderName";
    run;

%mend process_daily_reports;

/* Specify your paths and execute the macro */
%let basePath = C:\path\to\your\winscp\folder;
%let masterFilePath = C:\path\to\master\master_dataset.sas7bdat;
%let logFilePath = C:\path\to\your\log\process_log.txt;
%process_daily_reports(&basePath, &masterFilePath, &logFilePath);










%macro process_latest_folder(basePath, masterFilePath, logFilePath);
    /* Determine the latest folder based on today's date */
    data _null_;
        day = put(day(today()), z2.);
        month = put(month(today()), z2.);
        year = put(year(today()), 4.);
        folderName = cats('email_', day, '_', month, '_', year);
        call symputx('latestFolderName', trim(folderName));
    run;

    /* Check if this folder has already been processed by consulting the log file */
    %let isProcessed = 0;
    %if %sysfunc(fileexist(&logFilePath)) %then %do;
        data _null_;
            infile "&logFilePath";
            input lastFolder $100.;
            if lastFolder = "&latestFolderName" then call symput('isProcessed', '1');
        run;
    %end;

    %if &isProcessed = 0 %then %do;
        /* Process the folder's CSV files */
        %let folderPath = &basePath\&latestFolderName;
        filename csvfiles "&folderPath\*.csv";
        data combined;
            length filePath $256;
            infile csvfiles filename=filePath truncover;
            input;
            /* Adapt this part to match your CSV structure */
            /* Example: input Name $ Age Height; */
        run;
        
        /* Append the combined data to the master dataset */
        proc append base=&masterFilePath data=combined force;
        run;

        /* Update the log file */
        data _null_;
            file "&logFilePath" mod;
            put "&latestFolderName";
        run;
    %end;
    %else %do;
        %put NOTE: Folder &latestFolderName has already been processed. Skipping...;
    %end;
%mend process_latest_folder;

/* Set your specific paths here */
%let basePath = C:\path\to\your\source\folder;
%let masterFilePath = C:\path\to\your\master\master_dataset.sas7bdat;
%let logFilePath = C:\path\to\your\source\folder\process_log.txt;

/* Execute the macro */
%process_latest_folder(&basePath, &masterFilePath, &logFilePath);










Given the updated details that the files are in CSV format and the "main" file is essentially the first file from the first folder created, and that you are using SAS Enterprise Guide (SAS EG), the approach will slightly change. Hereâ€™s how you can manage this process in SAS EG:

1. **Combine CSV Files Within Each Daily Folder**: Since the files are in CSV format, you will read them into SAS datasets and then combine (append) them if there are more than one.

2. **Merge the Combined Daily Dataset with the Main Dataset**: The main dataset, in this case, would be the first CSV file that was converted into a SAS dataset from the start of this process. Subsequent daily combined datasets will be appended to this main dataset.

For this process, you'll need to slightly adapt the approach to handle CSV files, and also, the identification of the main file will be based on a one-time setup assumption that the first dataset created is your main dataset.

### Example SAS Code

Hereâ€™s an illustrative example of how you might automate this process in SAS EG, assuming you are manually identifying the daily folder to process:

```sas
* Example macro to import CSV files from a folder and append them;
%macro import_and_append_csvs(folderPath);
    * Use a filename statement to reference the directory listing;
    filename dirlist pipe "dir &folderPath\*.csv /b";

    * Create a temporary dataset to store file names;
    data filenames(keep=filename);
        length filename $256;
        infile dirlist length=reclen;
        input filename $varying256. reclen;
    run;

    * Import and append CSV files;
    %let dsid = %sysfunc(open(filenames));
    %let rc = %sysfunc(fetchobs(&dsid, 1));
    %let firstFile = 1;

    do while(&rc = 0);
        filename = "";
        call symputx('filename', filename);
        %if &firstFile %then %do;
            proc import datafile="&folderPath\&filename" 
                out=mainDataset
                dbms=csv
                replace;
            run;
            %let firstFile = 0;
        %end;
        %else %do;
            proc import datafile="&folderPath\&filename" 
                out=tempDataset
                dbms=csv
                replace;
            run;
            proc append base=mainDataset data=tempDataset force; 
            run;
        %end;
        %let rc = %sysfunc(fetch(&dsid));
    end;
    %let rc = %sysfunc(close(&dsid));
%mend import_and_append_csvs;

* Call the macro for today's folder (update path as necessary);
%import_and_append_csvs(C:\path\to\topfile\YYYYMMDD);
```

In this script:
- The macro `import_and_append_csvs` is designed to handle the importation and combination of CSV files from a specified folder.
- It uses a temporary dataset `filenames` to store the list of CSV files in the given folder.
- It processes each CSV file in the folder, importing the first file directly into `mainDataset` (assuming the first call to this macro points to the folder with your initial main file). Subsequent files are imported into a temporary dataset `tempDataset` and then appended to `mainDataset`.
- Replace `C:\path\to\topfile\YYYYMMDD` with the actual path to today's folder.

Remember to adjust the path to match your actual daily folder structure, potentially using a dynamic way to specify the `YYYYMMDD` part based on the current date or an input parameter if you want to automate or streamline the process further.










data smss;
    set sms;
    by 'Message id'n 'Mobile Number'n;
    retain ASSIGNED_PRIORITY;

    if first.'Message id'n then do;
        ASSIGNED_PRIORITY = 0;
        put "Resetting for 'Message id'n=" 'Message id'n;
    end;

    if Segment = 'GPB' then do;
        put "GPB Segment for 'Message id'n=" 'Message id'n;
        if NR_Flag = 1 and ASSIGNED_PRIORITY = 0 then do;
            ASSIGNED_PRIORITY = 1;
            put "Assigned Priority 1 for 'Message id'n=" 'Message id'n;
        end;
        else if NR_Flag = 0 and ASSIGNED_PRIORITY = 0 then do;
            ASSIGNED_PRIORITY = 2;
            put "Assigned Priority 2 for 'Message id'n=" 'Message id'n;
        end;
    end;
    /* Repeat for other segments */
run;
