import matplotlib.pyplot as plt
import seaborn as sns

# Distribution of transaction amounts
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='TRAN_AMT', bins=50, kde=True)
plt.title('Distribution of Transaction Amounts')
plt.xlabel('Transaction Amount')
plt.ylabel('Frequency')
plt.show()

# Boxplot to check transaction amounts across different sectors
plt.figure(figsize=(12, 8))
sns.boxplot(data=df, x='Sector', y='TRAN_AMT')
plt.title('Transaction Amounts by Sector')
plt.xticks(rotation=45)
plt.show()



# Trend of transactions over time for different sectors
df['Date'] = pd.to_datetime(df['Date'])  # Ensure 'Date' is datetime type
df.sort_values('Date', inplace=True)

plt.figure(figsize=(14, 8))
sns.lineplot(data=df, x='Date', y='TRAN_AMT', hue='Sector')
plt.title('Transaction Amounts Over Time by Sector')
plt.xlabel('Date')
plt.ylabel('Transaction Amount')
plt.legend(title='Sector')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split, GridSearchCV
from surprise.accuracy import rmse

# Load dataset
df = pd.read_csv('credit.csv')
df['Date'] = pd.to_datetime(df['Date'])

# Ensure that the dataset only contains the 5 specific sectors
sectors = ['Entertainment', 'Travel', 'Shopping', 'Restaurant', 'Groceries']
df = df[df['Sector'].isin(sectors)]

# Preprocessing and feature engineering
# Aggregating transaction amounts by customer and sector
agg_df = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].agg(['sum', 'mean', 'count']).reset_index().rename(columns={'sum': 'total_amount', 'mean': 'avg_amount', 'count': 'transaction_count'})

# Normalizing total_amount for each customer
agg_df['normalized_total_amount'] = (agg_df['total_amount'] - agg_df['total_amount'].min()) / (agg_df['total_amount'].max() - agg_df['total_amount'].min())

# Convert to Surprise data format
reader = Reader(rating_scale=(0, 1))  # Assuming normalized ratings
data = Dataset.load_from_df(agg_df[['ACCT', 'Sector', 'normalized_total_amount']], reader)

# Train/Test Split
trainset, testset = train_test_split(data, test_size=0.25)

# Algorithm Selection: Using SVD
model = SVD()

# Train the model
model.fit(trainset)

# Predictions
predictions = model.test(testset)

# Calculate RMSE
accuracy_rmse = rmse(predictions)

# Hyperparameter Tuning
param_grid = {
    'n_factors': [50, 100, 150],
    'n_epochs': [20, 30], 
    'lr_all': [0.005, 0.01],
    'reg_all': [0.02, 0.1]
}
gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)
gs.fit(data)

# Best parameters and results
best_params = gs.best_params['rmse']
best_score = gs.best_score['rmse']

print(f"Best RMSE: {best_score}, Best params: {best_params}")

# Re-train with best parameters
final_model = SVD(**best_params)
trainset = data.build_full_trainset()  # Using the full dataset for training
final_model.fit(trainset)

# Final model is now tuned and ready for predictions, specifically adapted to handle the five sectors.

# Histogram with a log scale
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='TRAN_AMT', bins=50, kde=True, log_scale=True)
plt.title('Log-Scaled Distribution of Transaction Amounts')
plt.xlabel('Transaction Amount (log scale)')
plt.ylabel('Frequency')
plt.show()

# Boxplot with outliers
plt.figure(figsize=(12, 8))
sns.boxplot(data=df, x='Sector', y='TRAN_AMT', showfliers=False)
plt.title('Transaction Amounts by Sector without Outliers')
plt.xticks(rotation=45)
plt.show()


# Calculate the median transaction amount over time for each sector
df['TRAN_AMT_MEDIAN'] = df.groupby(['Date', 'Sector'])['TRAN_AMT'].transform('median')

# Plotting the median transaction amounts
plt.figure(figsize=(14, 8))
sns.lineplot(data=df, x='Date', y='TRAN_AMT_MEDIAN', hue='Sector')
plt.title('Median Transaction Amounts Over Time by Sector')
plt.xlabel('Date')
plt.ylabel('Median Transaction Amount')
plt.legend(title='Sector')
plt.tight_layout()  # Adjust layout for better fit
plt.show()



import pandas as pd
import numpy as np
from pandas.tseries.holiday import USFederalHolidayCalendar as Calendar
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from surprise import SVD, Dataset, Reader, accuracy
from surprise.model_selection import cross_validate, train_test_split as surprise_train_test_split

# Load dataset
df = pd.read_csv('credit.csv')
df['Date'] = pd.to_datetime(df['Date'])

# Temporal Features
df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.dayofweek
calendar = Calendar()
holidays = calendar.holidays(start=df['Date'].min(), end=df['Date'].max())
df['IsHoliday'] = df['Date'].isin(holidays)
df['IsWeekend'] = df['DayOfWeek'] >= 5

# Customer Profiles
customer_profiles = df.groupby(['ACCT', 'Sector']).agg({
    'TRAN_AMT': ['mean', 'median', 'count']
}).reset_index()
customer_profiles.columns = ['ACCT', 'Sector', 'Average_Transaction', 'Median_Transaction', 'Total_Transactions']

# Normalization and Scaling
scaler = RobustScaler()
customer_profiles[['Average_Transaction', 'Median_Transaction']] = scaler.fit_transform(
    customer_profiles[['Average_Transaction', 'Median_Transaction']]
)

# Encoding categorical 'Sector'
encoder = OneHotEncoder(sparse=False)
sectors_encoded = encoder.fit_transform(customer_profiles[['Sector']])
sectors_encoded_df = pd.DataFrame(sectors_encoded, columns=encoder.get_feature_names_out(['Sector']))
customer_profiles = pd.concat([customer_profiles, sectors_encoded_df], axis=1).drop('Sector', axis=1)

# Prepare dataset for SVD
reader = Reader(rating_scale=(df['TRAN_AMT'].min(), df['TRAN_AMT'].max()))
data = Dataset.load_from_df(df[['ACCT', 'Sector', 'TRAN_AMT']], reader)
trainset, testset = surprise_train_test_split(data, test_size=0.25)

# Train SVD for collaborative filtering
svd_model = SVD()
svd_model.fit(trainset)
predictions = svd_model.test(testset)

# Get latent features for each user
trainset_full = data.build_full_trainset()
svd_model_full = SVD()
svd_model_full.fit(trainset_full)
user_factors = pd.DataFrame([svd_model_full.qi[i] for i in trainset_full.all_users()], index=trainset_full.to_raw_uid(range(trainset_full.n_users)))

# Prepare dataset for the Gradient Boosting Regressor
# This includes the original features plus the latent features from SVD
X = customer_profiles.merge(user_factors, left_on='ACCT', right_index=True)
y = customer_profiles['Average_Transaction']  # or any other target variable you're interested in

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X.drop(['ACCT', 'Average_Transaction'], axis=1), y, test_size=0.25, random_state=42)

# Train Gradient Boosting Regressor
gbr_model = GradientBoostingRegressor()
gbr_model.fit(X_train, y_train)

# Make predictions and evaluate
predictions = gbr_model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
print(f"RMSE: {rmse}")

# Your model is now trained and evaluated. You can use gbr_model to make new predictions.




import pandas as pd
import numpy as np
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from scipy.sparse import csr_matrix
from pandas.tseries.holiday import USFederalHolidayCalendar as Calendar

# Load your data into a DataFrame
df = pd.read_csv('credit.csv')

# Ensure correct data types
df['Date'] = pd.to_datetime(df['Date'])
df['ACCT'] = df['ACCT'].astype(str)
df['TRAN_AMT'] = pd.to_numeric(df['TRAN_AMT'], errors='coerce')

# Temporal Features
df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.dayofweek
calendar = Calendar()
holidays = calendar.holidays(start=df['Date'].min(), end=df['Date'].max())
df['IsHoliday'] = df['Date'].isin(holidays)
df['IsWeekend'] = df['DayOfWeek'] >= 5

# Create customer profiles
customer_profiles = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].agg(['mean', 'median', 'count']).reset_index()
customer_profiles.columns = ['ACCT', 'Sector', 'Avg_Transaction', 'Median_Transaction', 'Total_Transactions']

# Normalization and Scaling
scaler = RobustScaler()
customer_profiles[['Avg_Transaction', 'Median_Transaction']] = scaler.fit_transform(
    customer_profiles[['Avg_Transaction', 'Median_Transaction']]
)

# One-Hot Encoding for 'Sector'
encoder = OneHotEncoder()
sectors_encoded = encoder.fit_transform(customer_profiles[['Sector']])
sectors_encoded_df = pd.DataFrame(sectors_encoded.toarray(), columns=encoder.get_feature_names(['Sector']))
customer_profiles = pd.concat([customer_profiles, sectors_encoded_df], axis=1)
customer_profiles.drop('Sector', axis=1, inplace=True)

# Create user-item interaction matrix
interaction_matrix = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', fill_value=0)
user_item_matrix = csr_matrix(interaction_matrix.values)

# Apply Truncated SVD
svd = TruncatedSVD(n_components=20, random_state=42)
latent_matrix = svd.fit_transform(user_item_matrix)

# Create DataFrame for latent features for users
user_features = pd.DataFrame(latent_matrix, index=interaction_matrix.index).reset_index()

# Merge user features with the customer profiles
customer_profiles = customer_profiles.merge(user_features, left_on='ACCT', right_on='ACCT')

# Prepare the final dataset for the model
X = customer_profiles.drop(['ACCT', 'Avg_Transaction'], axis=1)
y = customer_profiles['Avg_Transaction']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Train Gradient Boosting Regressor
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbr.fit(X_train, y_train)

# Make predictions and evaluate
predictions = gbr.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f'RMSE: {rmse}')

# The model is now trained and can be used to make predictions on new data
