import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import TruncatedSVD
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, cosine_similarity
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

# Load dataset
df = pd.read_csv('transactions.csv')

# Basic preprocessing
df['DATE'] = pd.to_datetime(df['DATE'])
df['SectorID'] = LabelEncoder().fit_transform(df['Sector'])
df['Frequency'] = df.groupby(['ACCT', 'Sector'])['DATE'].transform('count')
df['Total_Amount'] = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].transform('sum')
df = df.drop_duplicates(subset=['ACCT', 'SectorID'])

# Normalize continuous features
features_to_scale = ['AGE', 'Frequency', 'Total_Amount']
scaler = StandardScaler()
df[features_to_scale] = scaler.fit_transform(df[features_to_scale])

# Create a user-sector interaction matrix for collaborative filtering
interaction_matrix = df.pivot_table(index='ACCT', columns='SectorID', values='Frequency', fill_value=0)

# Apply SVD for dimensionality reduction
svd = TruncatedSVD(n_components=100, random_state=42)  # Increased components for better capturing of latent features
latent_features = svd.fit_transform(interaction_matrix)

# Prepare a pipeline for sector models
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Ensure features are scaled within the pipeline
    ('clf', RandomForestClassifier(random_state=42))  # Switched to a more complex model for potentially better performance
])

# Enhanced sector-specific model training with grid search for hyperparameter tuning
sector_models = {}
X = df[['AGE', 'Frequency', 'Total_Amount']]  # Feature set

param_grid = {
    'clf__n_estimators': [100, 200],
    'clf__max_depth': [10, 20, None]
}

for sector in df['SectorID'].unique():
    y = (df['SectorID'] == sector).astype(int)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
    
    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')
    grid_search.fit(X_train, y_train)
    
    print(f"Best parameters for sector {sector}: {grid_search.best_params_}")
    print(f"Classification report for sector {sector}:\n{classification_report(y_test, grid_search.predict(X_test))}")
    
    sector_models[sector] = grid_search.best_estimator_

# Define a function to refine recommendations using model probabilities
def generate_refined_recommendations(user_id, user_features, sector_models):
    predictions = {}
    for sector, model in sector_models.items():
        proba = model.predict_proba(user_features.reshape(1, -1))[0][1]  # Predict engagement probability
        predictions[sector] = proba

    # Rank sectors by engagement probability
    ranked_sectors = sorted(predictions, key=predictions.get, reverse=True)
    return ranked_sectors[:5]  # Return top 5 sector recommendations

# Example: Generating recommendations
user_id = 'example_user_id'
user_index = interaction_matrix.index.get_loc(user_id)  # Locate user index in interaction matrix
user_features = latent_features[user_index]  # Get user's latent features from SVD

# Convert user's latent features back to original feature space for prediction
# Note: This step might need adjustments based on your actual feature set and models
refined_recommendations = generate_refined_recommendations(user_id, user_features, sector_models)

# Translate sector IDs back to sector names (if needed)
sector_names = label_encoder.inverse_transform(refined_recommendations)
print("Top 5 refined sector recommendations:", sector_names)









import matplotlib.pyplot as plt
import seaborn as sns

# Distribution of transaction amounts
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='TRAN_AMT', bins=50, kde=True)
plt.title('Distribution of Transaction Amounts')
plt.xlabel('Transaction Amount')
plt.ylabel('Frequency')
plt.show()

# Boxplot to check transaction amounts across different sectors
plt.figure(figsize=(12, 8))
sns.boxplot(data=df, x='Sector', y='TRAN_AMT')
plt.title('Transaction Amounts by Sector')
plt.xticks(rotation=45)
plt.show()



# Trend of transactions over time for different sectors
df['Date'] = pd.to_datetime(df['Date'])  # Ensure 'Date' is datetime type
df.sort_values('Date', inplace=True)

plt.figure(figsize=(14, 8))
sns.lineplot(data=df, x='Date', y='TRAN_AMT', hue='Sector')
plt.title('Transaction Amounts Over Time by Sector')
plt.xlabel('Date')
plt.ylabel('Transaction Amount')
plt.legend(title='Sector')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split, GridSearchCV
from surprise.accuracy import rmse

# Load dataset
df = pd.read_csv('credit.csv')
df['Date'] = pd.to_datetime(df['Date'])

# Ensure that the dataset only contains the 5 specific sectors
sectors = ['Entertainment', 'Travel', 'Shopping', 'Restaurant', 'Groceries']
df = df[df['Sector'].isin(sectors)]

# Preprocessing and feature engineering
# Aggregating transaction amounts by customer and sector
agg_df = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].agg(['sum', 'mean', 'count']).reset_index().rename(columns={'sum': 'total_amount', 'mean': 'avg_amount', 'count': 'transaction_count'})

# Normalizing total_amount for each customer
agg_df['normalized_total_amount'] = (agg_df['total_amount'] - agg_df['total_amount'].min()) / (agg_df['total_amount'].max() - agg_df['total_amount'].min())

# Convert to Surprise data format
reader = Reader(rating_scale=(0, 1))  # Assuming normalized ratings
data = Dataset.load_from_df(agg_df[['ACCT', 'Sector', 'normalized_total_amount']], reader)

# Train/Test Split
trainset, testset = train_test_split(data, test_size=0.25)

# Algorithm Selection: Using SVD
model = SVD()

# Train the model
model.fit(trainset)

# Predictions
predictions = model.test(testset)

# Calculate RMSE
accuracy_rmse = rmse(predictions)

# Hyperparameter Tuning
param_grid = {
    'n_factors': [50, 100, 150],
    'n_epochs': [20, 30], 
    'lr_all': [0.005, 0.01],
    'reg_all': [0.02, 0.1]
}
gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)
gs.fit(data)

# Best parameters and results
best_params = gs.best_params['rmse']
best_score = gs.best_score['rmse']

print(f"Best RMSE: {best_score}, Best params: {best_params}")

# Re-train with best parameters
final_model = SVD(**best_params)
trainset = data.build_full_trainset()  # Using the full dataset for training
final_model.fit(trainset)

# Final model is now tuned and ready for predictions, specifically adapted to handle the five sectors.

# Histogram with a log scale
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='TRAN_AMT', bins=50, kde=True, log_scale=True)
plt.title('Log-Scaled Distribution of Transaction Amounts')
plt.xlabel('Transaction Amount (log scale)')
plt.ylabel('Frequency')
plt.show()

# Boxplot with outliers
plt.figure(figsize=(12, 8))
sns.boxplot(data=df, x='Sector', y='TRAN_AMT', showfliers=False)
plt.title('Transaction Amounts by Sector without Outliers')
plt.xticks(rotation=45)
plt.show()


# Calculate the median transaction amount over time for each sector
df['TRAN_AMT_MEDIAN'] = df.groupby(['Date', 'Sector'])['TRAN_AMT'].transform('median')

# Plotting the median transaction amounts
plt.figure(figsize=(14, 8))
sns.lineplot(data=df, x='Date', y='TRAN_AMT_MEDIAN', hue='Sector')
plt.title('Median Transaction Amounts Over Time by Sector')
plt.xlabel('Date')
plt.ylabel('Median Transaction Amount')
plt.legend(title='Sector')
plt.tight_layout()  # Adjust layout for better fit
plt.show()



import pandas as pd
import numpy as np
from pandas.tseries.holiday import USFederalHolidayCalendar as Calendar
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from surprise import SVD, Dataset, Reader, accuracy
from surprise.model_selection import cross_validate, train_test_split as surprise_train_test_split

# Load dataset
df = pd.read_csv('credit.csv')
df['Date'] = pd.to_datetime(df['Date'])

# Temporal Features
df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.dayofweek
calendar = Calendar()
holidays = calendar.holidays(start=df['Date'].min(), end=df['Date'].max())
df['IsHoliday'] = df['Date'].isin(holidays)
df['IsWeekend'] = df['DayOfWeek'] >= 5

# Customer Profiles
customer_profiles = df.groupby(['ACCT', 'Sector']).agg({
    'TRAN_AMT': ['mean', 'median', 'count']
}).reset_index()
customer_profiles.columns = ['ACCT', 'Sector', 'Average_Transaction', 'Median_Transaction', 'Total_Transactions']

# Normalization and Scaling
scaler = RobustScaler()
customer_profiles[['Average_Transaction', 'Median_Transaction']] = scaler.fit_transform(
    customer_profiles[['Average_Transaction', 'Median_Transaction']]
)

# Encoding categorical 'Sector'
encoder = OneHotEncoder(sparse=False)
sectors_encoded = encoder.fit_transform(customer_profiles[['Sector']])
sectors_encoded_df = pd.DataFrame(sectors_encoded, columns=encoder.get_feature_names_out(['Sector']))
customer_profiles = pd.concat([customer_profiles, sectors_encoded_df], axis=1).drop('Sector', axis=1)

# Prepare dataset for SVD
reader = Reader(rating_scale=(df['TRAN_AMT'].min(), df['TRAN_AMT'].max()))
data = Dataset.load_from_df(df[['ACCT', 'Sector', 'TRAN_AMT']], reader)
trainset, testset = surprise_train_test_split(data, test_size=0.25)

# Train SVD for collaborative filtering
svd_model = SVD()
svd_model.fit(trainset)
predictions = svd_model.test(testset)

# Get latent features for each user
trainset_full = data.build_full_trainset()
svd_model_full = SVD()
svd_model_full.fit(trainset_full)
user_factors = pd.DataFrame([svd_model_full.qi[i] for i in trainset_full.all_users()], index=trainset_full.to_raw_uid(range(trainset_full.n_users)))

# Prepare dataset for the Gradient Boosting Regressor
# This includes the original features plus the latent features from SVD
X = customer_profiles.merge(user_factors, left_on='ACCT', right_index=True)
y = customer_profiles['Average_Transaction']  # or any other target variable you're interested in

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X.drop(['ACCT', 'Average_Transaction'], axis=1), y, test_size=0.25, random_state=42)

# Train Gradient Boosting Regressor
gbr_model = GradientBoostingRegressor()
gbr_model.fit(X_train, y_train)

# Make predictions and evaluate
predictions = gbr_model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
print(f"RMSE: {rmse}")

# Your model is now trained and evaluated. You can use gbr_model to make new predictions.




import pandas as pd
import numpy as np
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from scipy.sparse import csr_matrix
from pandas.tseries.holiday import USFederalHolidayCalendar as Calendar

# Load your data into a DataFrame
df = pd.read_csv('credit.csv')

# Ensure correct data types
df['Date'] = pd.to_datetime(df['Date'])
df['ACCT'] = df['ACCT'].astype(str)
df['TRAN_AMT'] = pd.to_numeric(df['TRAN_AMT'], errors='coerce')

# Temporal Features
df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.dayofweek
calendar = Calendar()
holidays = calendar.holidays(start=df['Date'].min(), end=df['Date'].max())
df['IsHoliday'] = df['Date'].isin(holidays)
df['IsWeekend'] = df['DayOfWeek'] >= 5

# Create customer profiles
customer_profiles = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].agg(['mean', 'median', 'count']).reset_index()
customer_profiles.columns = ['ACCT', 'Sector', 'Avg_Transaction', 'Median_Transaction', 'Total_Transactions']

# Normalization and Scaling
scaler = RobustScaler()
customer_profiles[['Avg_Transaction', 'Median_Transaction']] = scaler.fit_transform(
    customer_profiles[['Avg_Transaction', 'Median_Transaction']]
)

# One-Hot Encoding for 'Sector'
encoder = OneHotEncoder()
sectors_encoded = encoder.fit_transform(customer_profiles[['Sector']])
sectors_encoded_df = pd.DataFrame(sectors_encoded.toarray(), columns=encoder.get_feature_names(['Sector']))
customer_profiles = pd.concat([customer_profiles, sectors_encoded_df], axis=1)
customer_profiles.drop('Sector', axis=1, inplace=True)

# Create user-item interaction matrix
interaction_matrix = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', fill_value=0)
user_item_matrix = csr_matrix(interaction_matrix.values)

# Apply Truncated SVD
svd = TruncatedSVD(n_components=20, random_state=42)
latent_matrix = svd.fit_transform(user_item_matrix)

# Create DataFrame for latent features for users
user_features = pd.DataFrame(latent_matrix, index=interaction_matrix.index).reset_index()

# Merge user features with the customer profiles
customer_profiles = customer_profiles.merge(user_features, left_on='ACCT', right_on='ACCT')

# Prepare the final dataset for the model
X = customer_profiles.drop(['ACCT', 'Avg_Transaction'], axis=1)
y = customer_profiles['Avg_Transaction']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Train Gradient Boosting Regressor
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbr.fit(X_train, y_train)

# Make predictions and evaluate
predictions = gbr.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f'RMSE: {rmse}')

# The model is now trained and can be used to make predictions on new data





import pandas as pd
import numpy as np
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from scipy.sparse import csr_matrix

# Load your data into a DataFrame
df = pd.read_csv('credit.csv')

# Ensure correct data types
df['Date'] = pd.to_datetime(df['Date'])
df['ACCT'] = df['ACCT'].astype(str)
df['TRAN_AMT'] = pd.to_numeric(df['TRAN_AMT'], errors='coerce')

# Temporal Features
df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['IsWeekend'] = df['DayOfWeek'] >= 5

# Create customer profiles
customer_profiles = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].agg(['mean', 'median', 'count']).reset_index()
customer_profiles.columns = ['ACCT', 'Sector', 'Avg_Transaction', 'Median_Transaction', 'Total_Transactions']

# Normalization and Scaling
scaler = RobustScaler()
customer_profiles[['Avg_Transaction', 'Median_Transaction']] = scaler.fit_transform(
    customer_profiles[['Avg_Transaction', 'Median_Transaction']]
)

# One-Hot Encoding for 'Sector'
encoder = OneHotEncoder()
sectors_encoded = encoder.fit_transform(customer_profiles[['Sector']])
sectors_encoded_df = pd.DataFrame(sectors_encoded.toarray(), columns=encoder.get_feature_names(['Sector']))
customer_profiles = pd.concat([customer_profiles, sectors_encoded_df], axis=1)
customer_profiles.drop('Sector', axis=1, inplace=True)

# Prepare user-item interaction matrix for TruncatedSVD
interaction_matrix = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', fill_value=0)
user_item_matrix = csr_matrix(interaction_matrix.values)

# Apply Truncated SVD to obtain latent features
svd = TruncatedSVD(n_components=20, random_state=42)
latent_matrix = svd.fit_transform(user_item_matrix)

# Append latent features to customer profiles
latent_features_df = pd.DataFrame(latent_matrix, index=interaction_matrix.index).reset_index()
merged_profiles = pd.merge(customer_profiles, latent_features_df, on='ACCT', how='left')

# Prepare the dataset for the Gradient Boosting Regressor
X = merged_profiles.drop(['ACCT', 'Avg_Transaction'], axis=1)
y = merged_profiles['Avg_Transaction']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Train the Gradient Boosting Regressor
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbr.fit(X_train, y_train)

# Make predictions and evaluate the model
predictions = gbr.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f'RMSE: {rmse}')

# The model is now trained and can be used to make predictions on new data.



import pandas as pd
import numpy as np
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from scipy.sparse import csr_matrix

# Load your data into a DataFrame
df = pd.read_csv('credit.csv')

# Ensure correct data types
df['Date'] = pd.to_datetime(df['Date'])
df['ACCT'] = df['ACCT'].astype(str)
df['TRAN_AMT'] = pd.to_numeric(df['TRAN_AMT'], errors='coerce')

# Temporal Features
df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['IsWeekend'] = df['DayOfWeek'] >= 5

# Create customer profiles
customer_profiles = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].agg(['mean', 'median', 'count']).reset_index()
customer_profiles.columns = ['ACCT', 'Sector', 'Avg_Transaction', 'Median_Transaction', 'Total_Transactions']

# Normalization and Scaling
scaler = RobustScaler()
customer_profiles[['Avg_Transaction', 'Median_Transaction']] = scaler.fit_transform(
    customer_profiles[['Avg_Transaction', 'Median_Transaction']]
)

# One-Hot Encoding for 'Sector'
encoder = OneHotEncoder()
sectors_encoded = encoder.fit_transform(customer_profiles[['Sector']])
sectors_encoded_df = pd.DataFrame(sectors_encoded.toarray(), columns=encoder.get_feature_names(['Sector']))
customer_profiles = pd.concat([customer_profiles, sectors_encoded_df], axis=1)
customer_profiles.drop('Sector', axis=1, inplace=True)

# Prepare user-item interaction matrix for TruncatedSVD
interaction_matrix = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', fill_value=0)
user_item_matrix = csr_matrix(interaction_matrix.values)

# Apply Truncated SVD to obtain latent features
svd = TruncatedSVD(n_components=min(interaction_matrix.shape[1] - 1, 20), random_state=42)
latent_matrix = svd.fit_transform(user_item_matrix)

# Append latent features to customer profiles
latent_features_df = pd.DataFrame(latent_matrix, index=interaction_matrix.index).reset_index()
merged_profiles = pd.merge(customer_profiles, latent_features_df, on='ACCT', how='left')

# Prepare the dataset for the Gradient Boosting Regressor
X = merged_profiles.drop(['ACCT', 'Avg_Transaction'], axis=1)
y = merged_profiles['Avg_Transaction']

# Check for NaN values and handle them
X = X.fillna(X.median())
y = y.fillna(y.median())

# Replace infinite values if they are present
X.replace([np.inf, -np.inf], np.nan, inplace=True)
X.fillna(X.median(), inplace=True)
y.replace([np.inf, -np.inf], np.nan, inplace=True)
y.fillna(y.median(), inplace=True)

# Ensure that the data types are appropriate and there are no NaNs or infs
X = X.astype(np.float64)
y = y.astype(np.float64)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Fit the Gradient Boosting Regressor
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbr.fit(X_train, y_train)

# Make predictions and evaluate the model
predictions = gbr.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f'RMSE: {rmse}'




import pandas as pd
import numpy as np
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.ensemble import GradientBoostingRegressor, StackingRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from scipy.sparse import csr_matrix

# Load the data
df = pd.read_csv('credit.csv')

# Preprocessing and feature engineering
df['Date'] = pd.to_datetime(df['Date'])
df['ACCT'] = df['ACCT'].astype(str)
df['TRAN_AMT'] = pd.to_numeric(df['TRAN_AMT'], errors='coerce')

df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['IsWeekend'] = df['DayOfWeek'] >= 5
df['DaysSinceFirst'] = (df['Date'] - df.groupby('ACCT')['Date'].transform('min')).dt.days
df['TransactionFrequency'] = df.groupby('ACCT')['Date'].transform(lambda x: 1 / x.diff().dt.days.replace(0, 1))

customer_profiles = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].agg(['mean', 'median', 'sum', 'count']).reset_index()
customer_profiles.columns = ['ACCT', 'Sector', 'Avg_Transaction', 'Median_Transaction', 'Total_Transactions', 'DaysSinceFirst', 'TransactionFrequency']

scaler = RobustScaler()
customer_profiles[['Avg_Transaction', 'Median_Transaction', 'Total_Transactions', 'DaysSinceFirst', 'TransactionFrequency']] = scaler.fit_transform(customer_profiles[['Avg_Transaction', 'Median_Transaction', 'Total_Transactions', 'DaysSinceFirst', 'TransactionFrequency']])

encoder = OneHotEncoder()
sectors_encoded = encoder.fit_transform(customer_profiles[['Sector']])
sectors_encoded_df = pd.DataFrame(sectors_encoded.toarray(), columns=encoder.get_feature_names(['Sector']))
customer_profiles = pd.concat([customer_profiles, sectors_encoded_df], axis=1)
customer_profiles.drop(['Sector'], axis=1, inplace=True)

interaction_matrix = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', fill_value=0)
user_item_matrix = csr_matrix(interaction_matrix.values)
svd = TruncatedSVD(n_components=min(user_item_matrix.shape[1] - 1, 20), random_state=42)
latent_features = svd.fit_transform(user_item_matrix)

merged_profiles = customer_profiles.merge(pd.DataFrame(latent_features, index=interaction_matrix.index), left_on='ACCT', right_index=True)
X = merged_profiles.drop(['ACCT', 'Avg_Transaction'], axis=1)
y = merged_profiles['Avg_Transaction']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Hyperparameter tuning with GridSearchCV
gbr = GradientBoostingRegressor(random_state=42)
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1]
}
grid_search = GridSearchCV(gbr, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Ensemble model using Stacking
estimators = [
    ('ridge', Ridge()),
    ('gbr', grid_search.best_estimator_)
]
stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=Ridge(), cv=5)
stacking_regressor.fit(X_train, y_train)

# Cross-validation and evaluation
cv_scores = cross_val_score(stacking_regressor, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')
stacked_predictions = stacking_regressor.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, stacked_predictions))
mae = mean_absolute_error(y_test, stacked_predictions)
r2 = r2_score(y_test, stacked_predictions)

# Print evaluation metrics
print(f'Cross-Validated RMSE: {-cv_scores.mean()}')
print(f'Cross-Validated RMSE Std: {cv_scores.std()}')
print(f'Test RMSE: {rmse}')
print(f'Test MAE: {mae}')
print(f'Test R^2: {r2}')
# The model is now trained and can be used to make predictions on new data.
