import numpy as np
import pandas as pd

# âœ… Step 1: Ensure Unique Score Buckets (Avoid Duplicate Bin Edges)
score_bins = np.percentile(Dev_data1["score"], [0, 20, 40, 60, 80, 100], method="linear")
score_bins = np.unique(score_bins)  # Ensure unique bin edges

# âœ… Ensure Labels Are One Less Than Bins
score_labels = [
    f"A. > {round(score_bins[-1], 2)}",
    f"B. {round(score_bins[-2], 2)} - {round(score_bins[-1], 2)}",
    f"C. {round(score_bins[-3], 2)} - {round(score_bins[-2], 2)}",
    f"D. {round(score_bins[-4], 2)} - {round(score_bins[-3], 2)}",
]

# âœ… Step 2: Assign Score Buckets Correctly (Ensuring Matching Bins & Labels)
Dev_data1["Score_Bucket"] = pd.cut(Dev_data1["score"], bins=score_bins, labels=score_labels, include_lowest=True)

# âœ… Step 3: Identify Top 6 Features
top_6_features = feature_importance.nlargest(6).index.tolist()

# âœ… Step 4: Create Feature Value Buckets Dynamically with Unique Bin Edges
feature_ranges = {}  # Dictionary to store feature-wise range mappings

for feature in top_6_features:
    feature_bins = np.percentile(Dev_data1[feature], [0, 20, 40, 60, 80, 100], method="linear")
    feature_bins = np.unique(feature_bins)  # Ensure unique bin edges

    # âœ… Ensure Labels Are One Less Than Bins
    feature_labels = [
        f"A. > {round(feature_bins[-1], 2)}",
        f"B. {round(feature_bins[-2], 2)} - {round(feature_bins[-1], 2)}",
        f"C. {round(feature_bins[-3], 2)} - {round(feature_bins[-2], 2)}",
        f"D. {round(feature_bins[-4], 2)} - {round(feature_bins[-3], 2)}",
    ]

    feature_ranges[feature] = dict(zip(feature_labels, feature_bins[:-1]))  # Assign only valid bins

    # âœ… Assign Feature Value Buckets Correctly (Ensuring Matching Bins & Labels)
    Dev_data1[f"{feature}_Bucket"] = pd.cut(Dev_data1[feature], bins=feature_bins, labels=feature_labels, include_lowest=True)

# âœ… Step 5: Function to Create Tables for Each Feature
def create_feature_table(df, feature):
    table = df.groupby(["Score_Bucket", f"{feature}_Bucket"]).size().reset_index(name="Customer_Count")

    # Calculate percentage
    total_customers = table["Customer_Count"].sum()
    table["Customer_Percentage"] = (table["Customer_Count"] / total_customers) * 100

    # Sort for better readability
    table = table.sort_values(by=["Score_Bucket", f"{feature}_Bucket"])
    
    return table

# âœ… Step 6: Generate 6 Tables from `Dev_data1`
feature_tables = {}
for feature in top_6_features:
    feature_tables[feature] = create_feature_table(Dev_data1, feature)

# âœ… Step 7: Display the Tables with Proper Ranges
print("\nâœ… Score Bucket Ranges Based on Actual Distribution:", score_labels)
for feature, table in feature_tables.items():
    print(f"\nðŸ”¹ Table for Feature: {feature} (Feature Bucket Ranges)")
    print(f"Feature Ranges: {feature_ranges[feature]}")
    print(table)

















import numpy as np
import pandas as pd

# âœ… Step 1: Ensure Unique Score Buckets (Avoid Duplicate Bin Edges)
score_bins = np.percentile(Dev_data1["score"], [0, 20, 40, 60, 80, 100], method="linear")
score_bins = np.unique(score_bins)  # Ensure bin edges are unique

# âœ… Ensure Labels Are One Less Than Bins
score_labels = [
    f"A. > {round(score_bins[-1], 2)}",
    f"B. {round(score_bins[-2], 2)} - {round(score_bins[-1], 2)}",
    f"C. {round(score_bins[-3], 2)} - {round(score_bins[-2], 2)}",
    f"D. {round(score_bins[-4], 2)} - {round(score_bins[-3], 2)}",
    f"E. {round(score_bins[0], 2)} - {round(score_bins[-4], 2)}"
]

# âœ… Step 2: Assign Score Buckets Correctly
Dev_data1["Score_Bucket"] = pd.cut(Dev_data1["score"], bins=score_bins, labels=score_labels[:-1], include_lowest=True)

# âœ… Step 3: Identify Top 6 Features
top_6_features = feature_importance.nlargest(6).index.tolist()

# âœ… Step 4: Create Feature Value Buckets Dynamically with Unique Bin Edges
feature_ranges = {}  # Dictionary to store feature-wise range mappings

for feature in top_6_features:
    feature_bins = np.percentile(Dev_data1[feature], [0, 20, 40, 60, 80, 100], method="linear")
    feature_bins = np.unique(feature_bins)  # Ensure unique bin edges

    # âœ… Ensure Labels Are One Less Than Bins
    feature_labels = [
        f"A. > {round(feature_bins[-1], 2)}",
        f"B. {round(feature_bins[-2], 2)} - {round(feature_bins[-1], 2)}",
        f"C. {round(feature_bins[-3], 2)} - {round(feature_bins[-2], 2)}",
        f"D. {round(feature_bins[-4], 2)} - {round(feature_bins[-3], 2)}",
        f"E. {round(feature_bins[0], 2)} - {round(feature_bins[-4], 2)}"
    ]

    feature_ranges[feature] = dict(zip(feature_labels[:-1], feature_bins[:-1]))

    # âœ… Assign Feature Value Buckets Correctly
    Dev_data1[f"{feature}_Bucket"] = pd.cut(Dev_data1[feature], bins=feature_bins, labels=feature_labels[:-1], include_lowest=True)

# âœ… Step 5: Function to Create Tables for Each Feature
def create_feature_table(df, feature):
    table = df.groupby(["Score_Bucket", f"{feature}_Bucket"]).size().reset_index(name="Customer_Count")

    # Calculate percentage
    total_customers = table["Customer_Count"].sum()
    table["Customer_Percentage"] = (table["Customer_Count"] / total_customers) * 100

    # Sort for better readability
    table = table.sort_values(by=["Score_Bucket", f"{feature}_Bucket"])
    
    return table

# âœ… Step 6: Generate 6 Tables from `Dev_data1`
feature_tables = {}
for feature in top_6_features:
    feature_tables[feature] = create_feature_table(Dev_data1, feature)

# âœ… Step 7: Display the Tables with Proper Ranges
print("\nâœ… Score Bucket Ranges Based on Actual Distribution:", score_labels[:-1])
for feature, table in feature_tables.items():
    print(f"\nðŸ”¹ Table for Feature: {feature} (Feature Bucket Ranges)")
    print(f"Feature Ranges: {feature_ranges[feature]}")
    print(table)










import numpy as np
import pandas as pd

# âœ… Step 1: Ensure Unique Score Buckets (Avoid Duplicate Bin Edges)
score_bins = np.percentile(Dev_data1["score"], [0, 20, 40, 60, 80, 100], method="linear")
score_bins = np.unique(score_bins)  # Ensure bin edges are unique

# âœ… Step 2: Define Score Labels Based on Unique Score Ranges
score_labels = [
    f"A. > {round(score_bins[-2], 2)}",
    f"B. {round(score_bins[-3], 2)} - {round(score_bins[-2], 2)}",
    f"C. {round(score_bins[-4], 2)} - {round(score_bins[-3], 2)}",
    f"D. {round(score_bins[-5], 2)} - {round(score_bins[-4], 2)}",
    f"E. < {round(score_bins[1], 2)}"
]

# âœ… Step 3: Assign Score Buckets Using Fixed Score Ranges
Dev_data1["Score_Bucket"] = pd.cut(Dev_data1["score"], bins=score_bins, labels=score_labels, include_lowest=True)

# âœ… Step 4: Identify Top 6 Features
top_6_features = feature_importance.nlargest(6).index.tolist()

# âœ… Step 5: Create Feature Value Buckets Dynamically with Unique Bin Edges
feature_ranges = {}  # Dictionary to store feature-wise range mappings

for feature in top_6_features:
    feature_bins = np.percentile(Dev_data1[feature], [0, 20, 40, 60, 80, 100], method="linear")
    feature_bins = np.unique(feature_bins)  # Ensure unique bin edges

    feature_labels = [
        f"A. > {round(feature_bins[-2], 2)}",
        f"B. {round(feature_bins[-3], 2)} - {round(feature_bins[-2], 2)}",
        f"C. {round(feature_bins[-4], 2)} - {round(feature_bins[-3], 2)}",
        f"D. {round(feature_bins[-5], 2)} - {round(feature_bins[-4], 2)}",
        f"E. < {round(feature_bins[1], 2)}"
    ]

    feature_ranges[feature] = dict(zip(feature_labels, feature_bins))

    # Assign feature value bucket to Dev_data1
    Dev_data1[f"{feature}_Bucket"] = pd.cut(Dev_data1[feature], bins=feature_bins, labels=feature_labels, include_lowest=True)

# âœ… Step 6: Function to Create Tables for Each Feature
def create_feature_table(df, feature):
    table = df.groupby(["Score_Bucket", f"{feature}_Bucket"]).size().reset_index(name="Customer_Count")

    # Calculate percentage
    total_customers = table["Customer_Count"].sum()
    table["Customer_Percentage"] = (table["Customer_Count"] / total_customers) * 100

    # Sort for better readability
    table = table.sort_values(by=["Score_Bucket", f"{feature}_Bucket"])
    
    return table

# âœ… Step 7: Generate 6 Tables from `Dev_data1`
feature_tables = {}
for feature in top_6_features:
    feature_tables[feature] = create_feature_table(Dev_data1, feature)

# âœ… Step 8: Display the Tables with Proper Ranges
print("\nâœ… Score Bucket Ranges Based on Actual Distribution:", score_labels)
for feature, table in feature_tables.items():
    print(f"\nðŸ”¹ Table for Feature: {feature} (Feature Bucket Ranges)")
    print(f"Feature Ranges: {feature_ranges[feature]}")
    print(table)











import numpy as np
import pandas as pd

# âœ… Step 1: Determine Dynamic Score Buckets Based on Actual Distribution
score_bins = np.percentile(Dev_data1["score"], [0, 20, 40, 60, 80, 100])  # 5 equal percentiles
score_labels = [
    f"A. > {round(score_bins[4], 2)}",
    f"B. {round(score_bins[3], 2)} - {round(score_bins[4], 2)}",
    f"C. {round(score_bins[2], 2)} - {round(score_bins[3], 2)}",
    f"D. {round(score_bins[1], 2)} - {round(score_bins[2], 2)}",
    f"E. < {round(score_bins[1], 2)}"
]

# âœ… Step 2: Assign Score Buckets Based on Dynamic Score Ranges
Dev_data1["Score_Bucket"] = pd.cut(Dev_data1["score"], bins=score_bins, labels=score_labels, include_lowest=True)

# âœ… Step 3: Identify Top 6 Features
top_6_features = feature_importance.nlargest(6).index.tolist()

# âœ… Step 4: Create Feature Value Buckets Dynamically
feature_ranges = {}  # Dictionary to store feature-wise range mappings

for feature in top_6_features:
    min_val, max_val = Dev_data1[feature].min(), Dev_data1[feature].max()
    feature_bins = np.percentile(Dev_data1[feature], [0, 20, 40, 60, 80, 100])  # 5 percentiles
    feature_labels = [
        f"A. > {round(feature_bins[4], 2)}",
        f"B. {round(feature_bins[3], 2)} - {round(feature_bins[4], 2)}",
        f"C. {round(feature_bins[2], 2)} - {round(feature_bins[3], 2)}",
        f"D. {round(feature_bins[1], 2)} - {round(feature_bins[2], 2)}",
        f"E. < {round(feature_bins[1], 2)}"
    ]
    
    feature_ranges[feature] = dict(zip(feature_labels, feature_bins))

    # Assign feature value bucket to Dev_data1
    Dev_data1[f"{feature}_Bucket"] = pd.cut(Dev_data1[feature], bins=feature_bins, labels=feature_labels, include_lowest=True)

# âœ… Step 5: Function to Create Tables for Each Feature
def create_feature_table(df, feature):
    table = df.groupby(["Score_Bucket", f"{feature}_Bucket"]).size().reset_index(name="Customer_Count")

    # Calculate percentage
    total_customers = table["Customer_Count"].sum()
    table["Customer_Percentage"] = (table["Customer_Count"] / total_customers) * 100

    # Sort for better readability
    table = table.sort_values(by=["Score_Bucket", f"{feature}_Bucket"])
    
    return table

# âœ… Step 6: Generate 6 Tables from `Dev_data1`
feature_tables = {}
for feature in top_6_features:
    feature_tables[feature] = create_feature_table(Dev_data1, feature)

# âœ… Step 7: Display the Tables with Proper Ranges
print("\nâœ… Score Bucket Ranges Based on Actual Distribution:", score_labels)
for feature, table in feature_tables.items():
    print(f"\nðŸ”¹ Table for Feature: {feature} (Feature Bucket Ranges)")
    print(f"Feature Ranges: {feature_ranges[feature]}")
    print(table)





















import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Load and preprocess the dataset
df = pd.read_csv('main5.csv')
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.weekday

# Basic temporal and customer behavior features
df['Season'] = df['Month'].apply(lambda x: (x%12 + 3)//3)
df['AgeGroup'] = pd.cut(df['AGE_NUM'], bins=[0, 18, 30, 45, 60, 100], labels=False)

# Advanced feature engineering
# Rate of transaction change per customer
df['MonthlyTransCount'] = df.groupby(['ACCT', 'Year', 'Month'])['TRAN_AMT'].transform('count')
df['RateTransChange'] = df.groupby('ACCT')['MonthlyTransCount'].pct_change().fillna(0)

# Average transaction amount change per customer
df['AvgMonthlyTransAmt'] = df.groupby(['ACCT', 'Year', 'Month'])['TRAN_AMT'].transform('mean')
df['RateAmtChange'] = df.groupby('ACCT')['AvgMonthlyTransAmt'].pct_change().fillna(0)

# Sector diversity
df['SectorDiversity'] = df.groupby('ACCT')['Sector'].transform('nunique')

# Prepare the dataset for modeling
le = LabelEncoder()
df['Sector'] = le.fit_transform(df['Sector'])

X = df[['Year', 'Month', 'DayOfWeek', 'Season', 'AgeGroup', 'RateTransChange', 'RateAmtChange', 'SectorDiversity']]
y = df['Sector']  # Assuming the target is now the sector with most engagement

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Handling class imbalance with SMOTE
smote = SMOTE()
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_sm)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the model
model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
model.fit(X_train_scaled, y_train_sm)

# Make predictions and evaluate the model
predictions = model.predict(X_test_scaled)

print(f"Accuracy: {accuracy_score(y_test, predictions):.2f}")
print("\nClassification Report:")
print(classification_report(y_test, predictions))

# Confusion matrix
cm = confusion_matrix(y_test, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap='viridis')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()










import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('main5.csv')
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.weekday
df['AgeGroup'] = pd.cut(df['AGE_NUM'], bins=[0, 18, 30, 45, 60, 100], labels=False)

# Creating transaction count and average amount features per sector
trans_count = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].count().reset_index(name='TransCount')
avg_trans_amt = df.groupby(['ACCT', 'Sector'])['TRAN_AMT'].mean().reset_index(name='AvgTransAmt')
features_df = pd.merge(trans_count, avg_trans_amt, on=['ACCT', 'Sector'])

# Normalize features for merging with the main DataFrame
features_norm = features_df.pivot_table(index='ACCT', columns='Sector', values=['TransCount', 'AvgTransAmt'], fill_value=0)
features_norm.columns = ['_'.join(col).strip() for col in features_norm.columns.values]  # Flatten MultiIndex

# Merge normalized features back to the main DataFrame
df = df.merge(features_norm, on='ACCT')

# Assuming 'Sector' is what we want to predict, label encode the sector
le = LabelEncoder()
df['SectorEncoded'] = le.fit_transform(df['Sector'])

# Define features and target
X = df.drop(['ACCT', 'Date', 'Sector', 'TRAN_AMT', 'AGE_NUM', 'SectorEncoded'], axis=1)
y = df['SectorEncoded']

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train XGBClassifier
model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
model.fit(X_train_scaled, y_train)

# Make predictions
predictions = model.predict(X_test_scaled)

# Model Evaluation
print(f"Accuracy: {accuracy_score(y_test, predictions):.2f}")
print("\nClassification Report:")
print(classification_report(y_test, predictions, target_names=le.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap='viridis', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()











import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Efficient Data Loading
dtypes = {'ACCT': 'category', 'TRAN_AMT': 'float32', 'Sector': 'category', 'AGE_NUM': 'int8'}
parse_dates = ['Date']
df = pd.read_csv('main5.csv', dtype=dtypes, parse_dates=parse_dates)

# Feature Engineering
# Temporal Features
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Weekday'] = df['Date'].dt.weekday

# Customer Activity Features
df['Total_Trans_Amt'] = df.groupby('ACCT')['TRAN_AMT'].transform('sum')
df['Avg_Trans_Amt'] = df.groupby('ACCT')['TRAN_AMT'].transform('mean')

# Encode categorical variables
le_sector = LabelEncoder()
df['Sector_Encoded'] = le_sector.fit_transform(df['Sector'])

# Age Group Binning
df['Age_Group'] = pd.cut(df['AGE_NUM'], bins=[0, 18, 30, 45, 60, 100], labels=False)

# Assuming a binary target variable "Engaged" needs to be created based on business logic
# For demonstration, let's assume engagement is defined by transactions above the user's average transaction amount
df['Engaged'] = (df['TRAN_AMT'] > df['Avg_Trans_Amt']).astype(int)

# Prepare Features and Target Variable
X = df[['Year', 'Month', 'Weekday', 'Total_Trans_Amt', 'Avg_Trans_Amt', 'Sector_Encoded', 'Age_Group']]
y = df['Engaged']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model Training with RandomForest
model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
model.fit(X_train_scaled, y_train)

# Predictions
predictions = model.predict(X_test_scaled)

# Model Evaluation
print(f"Accuracy Score: {accuracy_score(y_test, predictions)}")
print(classification_report(y_test, predictions))








random for
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load and preprocess the data
df = pd.read_csv('main5.csv')
df['Date'] = pd.to_datetime(df['Date'])
df.dropna(inplace=True)

# Feature engineering
# Temporal features
df['Month'] = df['Date'].dt.month
df['DayOfWeek'] = df['Date'].dt.dayofweek

# Age binning
df['AgeGroup'] = pd.cut(df['AGE_NUM'], bins=[0, 18, 30, 45, 60, 100], labels=False)

# Aggregated transaction features
# For simplicity, let's create an example feature: mean transaction amount per account
mean_trans_amt = df.groupby('ACCT')['TRAN_AMT'].transform('mean')
df['MeanTransAmt'] = mean_trans_amt

# Encode categorical variables
le_sector = LabelEncoder()
df['Sector'] = le_sector.fit_transform(df['Sector'])

# Assume engagement is whether a transaction amount is above the mean transaction amount of the user
df['Engaged'] = (df['TRAN_AMT'] > df['MeanTransAmt']).astype(int)

# Splitting dataset into features and target
X = df[['Month', 'DayOfWeek', 'AgeGroup', 'MeanTransAmt', 'Sector']]
y = df['Engaged']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# Predicting
predictions = model.predict(X_test_scaled)

# Evaluating the model
print(f"Accuracy Score: {accuracy_score(y_test, predictions)}")
print(classification_report(y_test, predictions))












import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

def load_and_preprocess_data(filepath):
    df = pd.read_csv(filepath)
    df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')
    df.dropna(inplace=True)
    return df

def generate_interaction_matrices(df, sectors):
    count_matrix = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='count', fill_value=0)
    amount_matrix = df.pivot_table(index='ACCT', columns='Sector', values='TRAN_AMT', aggfunc='sum', fill_value=0)
    # Ensure all sectors are represented
    for sector in sectors:
        if sector not in count_matrix.columns:
            count_matrix[sector] = 0
            amount_matrix[sector] = 0
    return count_matrix[sorted(sectors)], amount_matrix[sorted(sectors)]

def scale_and_combine_matrices(count_matrix, amount_matrix):
    scaler = MinMaxScaler()
    scaled_count_matrix = scaler.fit_transform(count_matrix)
    scaled_amount_matrix = scaler.fit_transform(amount_matrix)
    # Combine by averaging the scaled matrices
    combined_matrix = (scaled_count_matrix + scaled_amount_matrix) / 2
    return pd.DataFrame(combined_matrix, index=count_matrix.index, columns=count_matrix.columns)

def calculate_cosine_similarity(combined_matrix):
    similarity = cosine_similarity(combined_matrix.T)
    return pd.DataFrame(similarity, index=combined_matrix.columns, columns=combined_matrix.columns)

def generate_hot_encoding(df, sectors):
    hot_encoded = pd.get_dummies(df[['ACCT', 'Sector']], columns=['Sector'], prefix='', prefix_sep='').groupby('ACCT').max()
    for sector in sectors:
        if sector not in hot_encoded.columns:
            hot_encoded[sector] = 0
    return hot_encoded[sorted(sectors)]

def get_recommendation_ranks(combined_matrix, sector_similarity_matrix):
    scores = np.dot(combined_matrix, sector_similarity_matrix)
    ranks = pd.DataFrame((-scores).argsort(axis=1), index=combined_matrix.index)
    # Replace numerical indexes with sector names
    sector_names = sector_similarity_matrix.columns
    ranked_sector_names = ranks.applymap(lambda x: sector_names[x])
    return ranked_sector_names.iloc[:, :5]  # Limit to top 5 recommendations

def calculate_hit_rate_enhanced(recommendations, test_transaction_ranks):
    hits = 0
    for acct in recommendations.index:
        top_recommended_sectors = recommendations.loc[acct]
        top_actual_sectors = test_transaction_ranks.loc[acct].nsmallest(len(top_recommended_sectors)).index
        hits += sum(sector in top_actual_sectors for sector in top_recommended_sectors)
    total_possible_hits = len(recommendations) * 5
    hit_rate = hits / total_possible_hits
    return hit_rate

def main():
    df = load_and_preprocess_data('main6.csv')
    split_date = pd.to_datetime("2023-01-01")
    sectors = sorted(df['Sector'].unique())
    train_df, test_df = df[df['DATE'] < split_date], df[df['DATE'] >= split_date]

    train_count_matrix, train_amount_matrix = generate_interaction_matrices(train_df, sectors)
    test_count_matrix, test_amount_matrix = generate_interaction_matrices(test_df, sectors)
    combined_matrix = scale_and_combine_matrices(train_count_matrix, train_amount_matrix)
    test_combined_matrix = scale_and_combine_matrices(test_count_matrix, test_amount_matrix)

    sector_similarity_matrix = calculate_cosine_similarity(combined_matrix)
    
    recommendations = get_recommendation_ranks(test_combined_matrix, sector_similarity_matrix)
    test_transaction_ranks = test_amount_matrix.rank(axis=1, method='max', ascending=True)
    
    hit_rate = calculate_hit_rate_enhanced(recommendations, test_transaction_ranks)
    print(f"Enhanced Hit Rate: {hit_rate:.2%}")

    with pd.ExcelWriter('analysis_outputs.xlsx', engine='openpyxl') as writer:
        train_count_matrix.to_excel(writer, sheet_name='Train Count Matrix')
        train_amount_matrix.to_excel(writer, sheet_name='Train Amount Matrix')
        combined_matrix.to_excel(writer, sheet_name='Combined Interaction Matrix')
        sector_similarity_matrix.to_excel(writer, sheet_name='Sector-Sector Matrix')
        recommendations.to_excel(writer, sheet_name='Recommendations')
        test_transaction_ranks.to_excel(writer, sheet_name='Test Transaction Ranks')

    print("All requested outputs have been exported to 'analysis_outputs.xlsx'.")

if __name__ == "__main__":
    main()
